{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8de633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHONPATH added: /nfs/turbo/coe-chaijy/janeding/regrounding/clip_tl\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "PROJECT_ROOT = '/nfs/turbo/coe-chaijy/janeding/regrounding/clip_tl'\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "print('PYTHONPATH added:', sys.path[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15134d5b",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576b7349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janeding/miniconda3/envs/prisma/lib/python3.13/site-packages/kaleido/__init__.py:14: UserWarning:\n",
      "\n",
      "\n",
      "\n",
      "Warning: You have Plotly version 5.19.0, which is not compatible with this version of Kaleido (1.0.0).\n",
      "\n",
      "This means that static image generation (e.g. `fig.write_image()`) will not work.\n",
      "\n",
      "Please upgrade Plotly to version 6.1.1 or greater, or downgrade Kaleido to version 0.2.1.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vit_prisma.utils.constants import DEVICE\n",
    "from vit_prisma.utils.tutorial_utils import (\n",
    "    plot_image,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f59da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# clear cache\n",
    "torch.cuda.empty_cache()\n",
    "# force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "DEVICE = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16a8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14989b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE from /nfs/turbo/coe-chaijy-unreplicated/pre-trained-weights/prisma-sae/models--Prisma-Multimodal--CLIP-transcoder-topk-768-x64-all_patches_0-mlp-96/snapshots/b9ff9647261516e5c7a882f7fd130db702c8ea25/weights.pt...\n",
      "Loading SAE from /nfs/turbo/coe-chaijy-unreplicated/pre-trained-weights/prisma-sae/models--Prisma-Multimodal--CLIP-transcoder-topk-256-x64-all_patches_1-mlp-94/snapshots/96c293a7299a99fe3d70f1f15498215849f22d96/weights.pt...\n",
      "Loading SAE from /nfs/turbo/coe-chaijy-unreplicated/pre-trained-weights/prisma-sae/models--Prisma-Multimodal--CLIP-transcoder-topk-1024-x64-all_patches_2-mlp-93/snapshots/cfb7ad837b972edfa27c9719de0e9571893dc434/weights.pt...\n",
      "Loading SAE from /nfs/turbo/coe-chaijy-unreplicated/pre-trained-weights/prisma-sae/models--Prisma-Multimodal--CLIP-transcoder-topk-1024-x64-all_patches_3-mlp-90/snapshots/3a169de44e8fa1641a9768aa84f3f99a9edd9828/weights.pt...\n",
      "Loading SAE from /nfs/turbo/coe-chaijy-unreplicated/pre-trained-weights/prisma-sae/models--Prisma-Multimodal--CLIP-transcoder-topk-512-x64-all_patches_4-mlp-76/snapshots/01926c5c792d963b51953bb5922674a484c215a5/weights.pt...\n",
      "Loading SAE from /nfs/turbo/coe-chaijy-unreplicated/pre-trained-weights/prisma-sae/models--Prisma-Multimodal--CLIP-transcoder-topk-1024-x64-all_patches_5-mlp-91/snapshots/12ebdd11e4e404bb95bbcab991eee795fb8cf66c/weights.pt...\n",
      "Loading SAE from /nfs/turbo/coe-chaijy-unreplicated/pre-trained-weights/prisma-sae/models--Prisma-Multimodal--CLIP-transcoder-topk-1024-x64-all_patches_6-mlp-94/snapshots/d0d722d64076e3911bff4b56d92a370543f1ca15/weights.pt...\n",
      "Loading SAE from /nfs/turbo/coe-chaijy-unreplicated/pre-trained-weights/prisma-sae/models--Prisma-Multimodal--CLIP-transcoder-topk-1024-x64-all_patches_7-mlp-97/snapshots/010d526da64705bae8b61f1c6b66617a2ecb69f8/weights.pt...\n",
      "Loading SAE from /nfs/turbo/coe-chaijy-unreplicated/pre-trained-weights/prisma-sae/models--Prisma-Multimodal--CLIP-transcoder-topk-1024-x64-all_patches_8-mlp-98/snapshots/43d0de77b2a27e664be61a74548d30a826db26fe/weights.pt...\n",
      "Loading SAE from /nfs/turbo/coe-chaijy-unreplicated/pre-trained-weights/prisma-sae/models--Prisma-Multimodal--CLIP-transcoder-topk-1024-x64-all_patches_9-mlp-98/snapshots/6f57a523ab849e1b5544b94572d539da7c30cbdb/weights.pt...\n",
      "Loading SAE from /nfs/turbo/coe-chaijy-unreplicated/pre-trained-weights/prisma-sae/models--Prisma-Multimodal--CLIP-transcoder-topk-1024-x64-all_patches_10-mlp-97/snapshots/a083216cc058dad1ea74a5b375c6c152b76ca6ad/weights.pt...\n"
     ]
    }
   ],
   "source": [
    "# transcoder list\n",
    "from src.analysis.utils import *\n",
    "\n",
    "tc_list = load_all_tc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "721360c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HookedViT\n",
    "from vit_prisma.models.model_loader import load_hooked_model\n",
    "from vit_prisma.utils.enums import ModelType\n",
    "\n",
    "model_name = tc_list[0].cfg.model_name\n",
    "hookedvit = load_hooked_model(model_name, model_type=ModelType.VISION)\n",
    "hookedvit = hookedvit.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa7f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check: predict label to be 'chemistry'\n",
    "# load the labels of a big word list for the check, but not the later ablation\n",
    "import pandas as pd\n",
    "\n",
    "file_path = '/nfs/turbo/coe-chaijy/janeding/regrounding/MyTC_bert/data/concrete_visual_vocabulary.csv'\n",
    "labels = pd.read_csv(file_path)\n",
    "\n",
    "labels = labels.dropna(subset=['word'])\n",
    "labels['word'] = labels['word'].astype(str).str.lower().str.strip()\n",
    "labels = labels[labels['word'].ne('')]\n",
    "labels = labels[labels['word'].ne('nan')]\n",
    "\n",
    "# turn into dict\n",
    "label_dict = {\n",
    "    row['word']: {\n",
    "        'concreteness': row['concreteness'],\n",
    "        'imageability': row['imageability'],\n",
    "    }\n",
    "    for _, row in labels.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ea0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CLIP text transformer encoder model\n",
    "\n",
    "import open_clip\n",
    "import torch\n",
    "from open_clip.model import text_global_pool\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def name_from_hf(model_name):\n",
    "    \"\"\"return model name from huggingface\"\"\"\n",
    "    return model_name.replace('open-clip:', 'hf-hub:')\n",
    "\n",
    "\n",
    "class OpenCLIPTextOnly(nn.Module):\n",
    "    def __init__(self, clip):\n",
    "        super().__init__()\n",
    "        # only keep the text part of the model\n",
    "        self.vocab_size = clip.vocab_size\n",
    "        self.token_embedding = clip.token_embedding\n",
    "        self.positional_embedding = clip.positional_embedding\n",
    "        self.transformer = clip.transformer\n",
    "        self.ln_final = clip.ln_final\n",
    "        self.text_projection = clip.text_projection\n",
    "        self.attn_mask = clip.attn_mask\n",
    "        self.text_pool_type = clip.text_pool_type\n",
    "        self.text_eos_id = getattr(clip, 'text_eos_id', None)\n",
    "        # self.register_buffer('attn_mask', self.attn_mask, persistent=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_text(self, text, normalize=False):\n",
    "        cast_dtype = self.transformer.get_cast_dtype()\n",
    "        x = self.token_embedding(text).to(cast_dtype)  # [B, L, D]\n",
    "        x = x + self.positional_embedding.to(cast_dtype)\n",
    "        x = self.transformer(\n",
    "            x, attn_mask=self.attn_mask\n",
    "        )  # transformer(width=D)\n",
    "        x = self.ln_final(x)\n",
    "        x = text_global_pool(x, text, self.text_pool_type)\n",
    "        if self.text_projection is not None:\n",
    "            x = (\n",
    "                self.text_projection(x)\n",
    "                if isinstance(self.text_projection, nn.Linear)\n",
    "                else x @ self.text_projection\n",
    "            )\n",
    "        return nn.functional.normalize(x, dim=-1) if normalize else x\n",
    "\n",
    "    def forward(self, text):\n",
    "        return self.encode_text(text, normalize=True)\n",
    "\n",
    "\n",
    "tmp_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    name_from_hf(tc_list[0].cfg.model_name),\n",
    ")\n",
    "tmp_model.to(DEVICE)\n",
    "tmp_model.eval()\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer(name_from_hf(tc_list[0].cfg.model_name))\n",
    "\n",
    "# free up visual tower memory (optional)\n",
    "if hasattr(tmp_model, 'visual'):\n",
    "    delattr(tmp_model, 'visual')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "text_model = OpenCLIPTextOnly(tmp_model).eval().to(DEVICE)\n",
    "# text_model.forward(input_ids) or text_model.encode_text(input_ids, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e007a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\tTask(Task-2) matplotlib.image:image.py:_normalize_image_array()- Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.7922626..2.145897].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# image\n",
    "\n",
    "from PIL import Image\n",
    "from vit_prisma.transforms import get_clip_val_transforms\n",
    "\n",
    "# img_path = '/nfs/turbo/coe-chaijy/janeding/example_images/smiling-face.png'\n",
    "img_path = '/nfs/turbo/coe-chaijy/janeding/regrounding/clip_tl/example_images/flask.png'\n",
    "gt_label = 'chemistry'\n",
    "img = Image.open(img_path).convert()  # Ensure it's 3 channels\n",
    "transforms = get_clip_val_transforms()\n",
    "img_tensor = transforms(img)\n",
    "plot_image(img_tensor.detach().cpu(), unstandardise=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fda556a6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import open_clip\n",
    "\n",
    "CHUNK_SIZE = 512\n",
    "\n",
    "labels = list(label_dict.keys())\n",
    "prompts = [f'a photo of a {l}' for l in labels]\n",
    "text_tokens = open_clip.tokenize(prompts)  # [N, 77] on CPU\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_input = img_tensor.unsqueeze(0).to(DEVICE)\n",
    "    vis_out, cache = hookedvit.run_with_cache(image_input)\n",
    "    image_features = vis_out.to(DEVICE)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_feats_out = []\n",
    "    for i in range(0, len(text_tokens), CHUNK_SIZE):\n",
    "        chunk = text_tokens[i : i + CHUNK_SIZE].to(DEVICE, non_blocking=True)\n",
    "        feats = text_model.encode_text(chunk)\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        text_feats_out.append(feats.cpu())\n",
    "    text_features = torch.cat(text_feats_out).to(DEVICE)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    top_probs, top_idx = text_probs.squeeze().topk(50)\n",
    "\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_idx)):\n",
    "    print(f'{i + 1}. label: {labels[idx]:<20} | prob: {prob.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f28a09de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['much',\n",
       " 'weed',\n",
       " 'test tube',\n",
       " 'reconnaissance',\n",
       " 'backlight',\n",
       " 'laboratory bottle',\n",
       " 'refrigerator',\n",
       " 'experiment',\n",
       " 'funk',\n",
       " 'enema',\n",
       " 'disqualify',\n",
       " 'lab bench',\n",
       " 'testing',\n",
       " 'governing',\n",
       " 'maidenly',\n",
       " 'hockey stick',\n",
       " 'tearfulness',\n",
       " 'discovery',\n",
       " 'cavity',\n",
       " 'orbiter',\n",
       " 'acid',\n",
       " 'unspoiled',\n",
       " 'unventilated',\n",
       " 'disorganize',\n",
       " 'graduated tube',\n",
       " 'inclusiveness',\n",
       " 'misappropriation',\n",
       " 'spreader',\n",
       " 'satisfied',\n",
       " 'ascend',\n",
       " 'indulgency',\n",
       " 'pipette',\n",
       " 'analysis',\n",
       " 'chairwoman',\n",
       " 'provided',\n",
       " 'sewing kit',\n",
       " 'transducer',\n",
       " 'nightcaps',\n",
       " 'lapse',\n",
       " 'moneyman',\n",
       " 'science',\n",
       " 'homogenous',\n",
       " 'solvent',\n",
       " 'opalescent',\n",
       " 'education',\n",
       " 'urinalysis',\n",
       " 'hotshot',\n",
       " 'violent',\n",
       " 'illuminate',\n",
       " 'outgun',\n",
       " 'look back',\n",
       " 'promiscuousness',\n",
       " 'cosigner',\n",
       " 'unreasoningly',\n",
       " 'placidness',\n",
       " 'sailed',\n",
       " 'observation',\n",
       " 'chemistry',\n",
       " 'unpremeditatedly',\n",
       " 'safety goggles',\n",
       " 'grievance',\n",
       " 'clasping',\n",
       " 'deflower',\n",
       " 'soilless',\n",
       " 'pinecone',\n",
       " 'garmentless',\n",
       " 'world',\n",
       " 'accounting',\n",
       " 'lab coat',\n",
       " 'damnably',\n",
       " 'featherweight',\n",
       " 'inconstantly',\n",
       " 'intimidating',\n",
       " 'applause',\n",
       " 'depressurize',\n",
       " 'aboard',\n",
       " 'corkscrew',\n",
       " 'laboratory',\n",
       " 'sound',\n",
       " 'relapse',\n",
       " 'umbilical cord',\n",
       " 'blast',\n",
       " 'bank building',\n",
       " 'handsomely',\n",
       " 'lapboard',\n",
       " 'autocratic',\n",
       " 'windmill',\n",
       " 'instrumentation',\n",
       " 'regenerate',\n",
       " 'mainsheet',\n",
       " 'candidness',\n",
       " 'cumulativeness',\n",
       " 'cuff',\n",
       " 'houseboat',\n",
       " 'matriculation',\n",
       " 'visceral',\n",
       " 'glassware',\n",
       " 'liquid',\n",
       " 'knelt',\n",
       " 'associational',\n",
       " 'hang back',\n",
       " 'conglomerate',\n",
       " 'inadvertence',\n",
       " 'sporting',\n",
       " 'backcourt',\n",
       " 'depersonalization',\n",
       " 'wooden spoon',\n",
       " 'trachea',\n",
       " 'understatement',\n",
       " 'larvae',\n",
       " 'concert',\n",
       " 'red light',\n",
       " 'urgency',\n",
       " 'heliport',\n",
       " 'mixing',\n",
       " 'drum kit',\n",
       " 'experimentation',\n",
       " 'overprice',\n",
       " 'propeller',\n",
       " 'antisocial',\n",
       " 'immorality',\n",
       " 'cylinder',\n",
       " 'scientist',\n",
       " 'biologist',\n",
       " 'jetted',\n",
       " 'inadequate',\n",
       " 'shanty',\n",
       " 'albacore',\n",
       " 'camelback',\n",
       " 'technician',\n",
       " 'consultancy',\n",
       " 'epicenter',\n",
       " 'worming',\n",
       " 'thyself',\n",
       " 'uptempo',\n",
       " 'buffet',\n",
       " 'guesthouse',\n",
       " 'graphic',\n",
       " 'hippopotamus',\n",
       " 'unaffectionate',\n",
       " 'holler',\n",
       " 'advancing',\n",
       " 'lordship',\n",
       " 'premonition',\n",
       " 'chemical',\n",
       " 'heating',\n",
       " 'expletive',\n",
       " 'twisty',\n",
       " 'engagingness',\n",
       " 'inventor',\n",
       " 'unseasoned',\n",
       " 'container',\n",
       " 'electrostatics',\n",
       " 'Swiss chard',\n",
       " 'stranding',\n",
       " 'whacker',\n",
       " 'terabyte',\n",
       " 'sauerkraut',\n",
       " 'pintsize',\n",
       " 'proofreading',\n",
       " 'outgoingness',\n",
       " 'which',\n",
       " 'mull over',\n",
       " 'saxophone',\n",
       " 'corrupt',\n",
       " 'distillation',\n",
       " 'blockheaded',\n",
       " 'flask',\n",
       " 'doc',\n",
       " 'interrupt',\n",
       " 'apparatus',\n",
       " 'vial',\n",
       " 'investable',\n",
       " 'solution',\n",
       " 'standpoint',\n",
       " 'trustless',\n",
       " 'untraded',\n",
       " 'mixture',\n",
       " 'edged',\n",
       " 'perfect',\n",
       " 'motorsport',\n",
       " 'base',\n",
       " 'irreproachability',\n",
       " 'research tools',\n",
       " 'compound',\n",
       " 'commercial',\n",
       " 'souring',\n",
       " 'emptiness',\n",
       " 'irrelevantly',\n",
       " 'summerlike',\n",
       " 'science equipment',\n",
       " 'research',\n",
       " 'plexiglass',\n",
       " 'crouched',\n",
       " 'enthrallment',\n",
       " 'technology',\n",
       " 'equivalence',\n",
       " 'admonishingly',\n",
       " 'secret',\n",
       " 'wipe off',\n",
       " 'caricaturist',\n",
       " 'grayfish',\n",
       " 'spring fever',\n",
       " 'hardhead',\n",
       " 'classes',\n",
       " 'sample',\n",
       " 'titration',\n",
       " 'determinable',\n",
       " 'reagent',\n",
       " 'craps',\n",
       " 'chancellorship',\n",
       " 'proficiently',\n",
       " 'pureness',\n",
       " 'scamper',\n",
       " 'isle',\n",
       " 'snip',\n",
       " 'bruiser',\n",
       " 'prehistory',\n",
       " 'reaction',\n",
       " 'unobjectionably',\n",
       " 'hardware',\n",
       " 'clergyman',\n",
       " 'railway station',\n",
       " 'hearing',\n",
       " 'forgotten',\n",
       " 'beaker',\n",
       " 'knowledge',\n",
       " 'handbrake',\n",
       " 'funneled',\n",
       " 'potbelly',\n",
       " 'confinable',\n",
       " 'cellar',\n",
       " 'cultivatable',\n",
       " 'nightingale',\n",
       " 'twit',\n",
       " 'long haul',\n",
       " 'measurement',\n",
       " 'hangout',\n",
       " 'phooey',\n",
       " 'shouldering',\n",
       " 'inundate',\n",
       " 'columbine',\n",
       " 'wasting',\n",
       " 'disreputability',\n",
       " 'barbed wire',\n",
       " 'ethicality',\n",
       " 'innovation',\n",
       " 'master class',\n",
       " 'brewery']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now try a smaller label set\n",
    "from custom_labels import final_labels\n",
    "\n",
    "# final_labels.append('cat')\n",
    "smaller_text_tokens = open_clip.tokenize(final_labels)\n",
    "\n",
    "final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c328bbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. label: test tube            | prob: 0.4590\n",
      "2. label: science equipment    | prob: 0.0981\n",
      "3. label: chemistry            | prob: 0.0883\n",
      "4. label: chemical             | prob: 0.0811\n",
      "5. label: laboratory bottle    | prob: 0.0781\n",
      "6. label: reagent              | prob: 0.0348\n",
      "7. label: beaker               | prob: 0.0329\n",
      "8. label: research tools       | prob: 0.0326\n",
      "9. label: laboratory           | prob: 0.0241\n",
      "10. label: titration            | prob: 0.0214\n",
      "11. label: science              | prob: 0.0162\n",
      "12. label: graduated tube       | prob: 0.0132\n",
      "13. label: distillation         | prob: 0.0066\n",
      "14. label: experiment           | prob: 0.0032\n",
      "15. label: vial                 | prob: 0.0020\n",
      "16. label: solvent              | prob: 0.0010\n",
      "17. label: flask                | prob: 0.0010\n",
      "18. label: pipette              | prob: 0.0009\n",
      "19. label: liquid               | prob: 0.0008\n",
      "20. label: scientist            | prob: 0.0006\n",
      "21. label: biologist            | prob: 0.0005\n",
      "22. label: experimentation      | prob: 0.0004\n",
      "23. label: lab bench            | prob: 0.0003\n",
      "24. label: instrumentation      | prob: 0.0003\n",
      "25. label: compound             | prob: 0.0003\n",
      "26. label: testing              | prob: 0.0002\n",
      "27. label: research             | prob: 0.0002\n",
      "28. label: chancellorship       | prob: 0.0001\n",
      "29. label: urinalysis           | prob: 0.0001\n",
      "30. label: pintsize             | prob: 0.0001\n",
      "31. label: glassware            | prob: 0.0001\n",
      "32. label: depressurize         | prob: 0.0001\n",
      "33. label: ethicality           | prob: 0.0001\n",
      "34. label: classes              | prob: 0.0001\n",
      "35. label: cylinder             | prob: 0.0001\n",
      "36. label: solution             | prob: 0.0001\n",
      "37. label: acid                 | prob: 0.0000\n",
      "38. label: electrostatics       | prob: 0.0000\n",
      "39. label: apparatus            | prob: 0.0000\n",
      "40. label: enema                | prob: 0.0000\n",
      "41. label: consultancy          | prob: 0.0000\n",
      "42. label: homogenous           | prob: 0.0000\n",
      "43. label: cosigner             | prob: 0.0000\n",
      "44. label: engagingness         | prob: 0.0000\n",
      "45. label: reaction             | prob: 0.0000\n",
      "46. label: graphic              | prob: 0.0000\n",
      "47. label: measurement          | prob: 0.0000\n",
      "48. label: disorganize          | prob: 0.0000\n",
      "49. label: standpoint           | prob: 0.0000\n",
      "50. label: irreproachability    | prob: 0.0000\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZE = 512\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_input = img_tensor.unsqueeze(0).to(DEVICE)\n",
    "    vis_out, cache = hookedvit.run_with_cache(image_input)\n",
    "    image_features = vis_out.to(DEVICE)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_feats_out = []\n",
    "    for i in range(0, len(smaller_text_tokens), CHUNK_SIZE):\n",
    "        chunk = smaller_text_tokens[i : i + CHUNK_SIZE].to(\n",
    "            DEVICE, non_blocking=True\n",
    "        )\n",
    "        feats = text_model.encode_text(chunk)\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        text_feats_out.append(feats.cpu())\n",
    "    text_features = torch.cat(text_feats_out).to(DEVICE)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    top_probs, top_idx = text_probs.squeeze().topk(50)\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_idx)):\n",
    "    print(f'{i + 1}. label: {final_labels[idx]:<20} | prob: {prob.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c09abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_features(text_model, labels, chunk_size=1024, device=DEVICE):\n",
    "    prompts = [f'a photo of a {l}' for l in labels]\n",
    "    label_tokens = open_clip.tokenize(prompts)  # [N, 77] on CPU\n",
    "\n",
    "    text_feats_out = []\n",
    "    for i in range(0, len(label_tokens), chunk_size):\n",
    "        chunk = label_tokens[i : i + chunk_size].to(device, non_blocking=True)\n",
    "        feats = text_model.encode_text(chunk)\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        text_feats_out.append(feats.cpu())\n",
    "    text_features = torch.cat(text_feats_out).to(device)\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "936c417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = get_text_features(\n",
    "    text_model, final_labels, chunk_size=1024, device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fadcaa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionModelSAERunnerConfig(model_class_name='HookedViT',\n",
      "                           model_name='open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K',\n",
      "                           vit_model_cfg=None,\n",
      "                           model_path=None,\n",
      "                           hook_point_layer=0,\n",
      "                           layer_subtype='ln2.hook_normalized',\n",
      "                           hook_point_head_index=None,\n",
      "                           context_size=50,\n",
      "                           use_cached_activations=False,\n",
      "                           use_patches_only=False,\n",
      "                           cached_activations_path='activations/_network_scratch_s_sonia.joseph_datasets_kaggle_datasets/open-clip:laion_CLIP-ViT-B-32-DataComp.XL-s13B-b90K/blocks.9.ln2.hook_normalized',\n",
      "                           image_size=224,\n",
      "                           architecture='standard',\n",
      "                           b_dec_init_method='geometric_median',\n",
      "                           expansion_factor=64,\n",
      "                           from_pretrained_path=None,\n",
      "                           is_transcoder=True,\n",
      "                           transcoder_with_skip_connection=True,\n",
      "                           out_hook_point_layer=0,\n",
      "                           layer_out_subtype='hook_mlp_out',\n",
      "                           d_out=768,\n",
      "                           _device='cuda',\n",
      "                           seed=42,\n",
      "                           _dtype='float32',\n",
      "                           d_in=768,\n",
      "                           activation_fn_str='topk',\n",
      "                           activation_fn_kwargs={'k': 768},\n",
      "                           cls_token_only=False,\n",
      "                           max_grad_norm=1.0,\n",
      "                           initialization_method='independent',\n",
      "                           normalize_activations='layer_norm',\n",
      "                           n_batches_in_buffer=20,\n",
      "                           store_batch_size=32,\n",
      "                           num_workers=16,\n",
      "                           num_epochs=1,\n",
      "                           verbose=False,\n",
      "                           l1_coefficient=0.0002,\n",
      "                           lp_norm=1,\n",
      "                           lr=0.0009762258997107048,\n",
      "                           lr_scheduler_name='cosineannealingwarmup',\n",
      "                           lr_warm_up_steps=500,\n",
      "                           train_batch_size=4096,\n",
      "                           dataset_name='imagenet1k',\n",
      "                           dataset_path='/network/scratch/s/sonia.joseph/datasets/kaggle_datasets',\n",
      "                           dataset_train_path='/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/train',\n",
      "                           dataset_val_path='/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/val',\n",
      "                           use_ghost_grads=False,\n",
      "                           feature_sampling_window=1000,\n",
      "                           dead_feature_window=5000,\n",
      "                           dead_feature_threshold=1e-08,\n",
      "                           log_to_wandb=True,\n",
      "                           wandb_project='openclip-transcoders',\n",
      "                           wandb_entity=None,\n",
      "                           wandb_log_frequency=10,\n",
      "                           n_validation_runs=0,\n",
      "                           n_checkpoints=10,\n",
      "                           checkpoint_path='/network/scratch/p/praneet.suresh/openclip-transcoder-checkpoints/496b0e61-openclip-transcoders')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(tc_list[0].cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "531ea40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedViTConfig(n_layers=12,\n",
      "                d_model=768,\n",
      "                d_head=64,\n",
      "                d_mlp=3072,\n",
      "                model_name='open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K',\n",
      "                use_cls_token=True,\n",
      "                n_heads=12,\n",
      "                activation_name='gelu',\n",
      "                d_vocab=-1,\n",
      "                eps=1e-05,\n",
      "                use_attn_result=False,\n",
      "                use_attn_scale=True,\n",
      "                use_split_qkv_input=False,\n",
      "                use_hook_mlp_in=False,\n",
      "                use_attn_in=False,\n",
      "                use_local_attn=False,\n",
      "                original_architecture=None,\n",
      "                from_checkpoint=False,\n",
      "                checkpoint_index=None,\n",
      "                checkpoint_label_type=None,\n",
      "                checkpoint_value=None,\n",
      "                tokenizer_name=None,\n",
      "                window_size=None,\n",
      "                attn_types=None,\n",
      "                init_mode='gpt2',\n",
      "                normalization_type='LN',\n",
      "                normalize_output=True,\n",
      "                device='cpu',\n",
      "                n_devices=1,\n",
      "                attention_dir='bidirectional',\n",
      "                attn_only=False,\n",
      "                seed=None,\n",
      "                initializer_range=-1.0,\n",
      "                init_weights=True,\n",
      "                scale_attn_by_inverse_layer_idx=False,\n",
      "                positional_embedding_type='standard',\n",
      "                final_rms=False,\n",
      "                d_vocab_out=-1,\n",
      "                parallel_attn_mlp=False,\n",
      "                rotary_dim=None,\n",
      "                n_params=None,\n",
      "                use_hook_tokens=False,\n",
      "                gated_mlp=False,\n",
      "                default_prepend_bos=True,\n",
      "                dtype=torch.float32,\n",
      "                tokenizer_prepends_bos=None,\n",
      "                n_key_value_heads=None,\n",
      "                post_embedding_ln=False,\n",
      "                rotary_base=10000,\n",
      "                trust_remote_code=False,\n",
      "                rotary_adjacent_pairs=False,\n",
      "                layer_norm_pre=True,\n",
      "                use_bert_block=False,\n",
      "                weight_type='he',\n",
      "                cls_std=1e-06,\n",
      "                pos_std=0.02,\n",
      "                n_channels=3,\n",
      "                patch_size=32,\n",
      "                image_size=224,\n",
      "                classification_type='cls',\n",
      "                n_classes=512,\n",
      "                return_type='class_logits',\n",
      "                log_dir='logs',\n",
      "                use_wandb=True,\n",
      "                wandb_team_name='perceptual-alignment',\n",
      "                wandb_project_name=None,\n",
      "                log_frequency=1,\n",
      "                print_every=0,\n",
      "                optimizer_name='AdamW',\n",
      "                lr=0.0003,\n",
      "                weight_decay=0.01,\n",
      "                loss_fn_name='CrossEntropy',\n",
      "                batch_size=512,\n",
      "                warmup_steps=10,\n",
      "                scheduler_step=200,\n",
      "                scheduler_gamma=0.8,\n",
      "                scheduler_type='WarmupThenStep',\n",
      "                early_stopping=False,\n",
      "                early_stopping_patience=2,\n",
      "                num_epochs=50,\n",
      "                attn_dropout_rate=0.0,\n",
      "                mlp_dropout_rate=0.0,\n",
      "                parent_dir='',\n",
      "                save_dir='Checkpoints',\n",
      "                save_checkpoints=True,\n",
      "                save_cp_frequency=5,\n",
      "                is_video_transformer=False,\n",
      "                video_tubelet_depth=None,\n",
      "                video_num_frames=None)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(hookedvit.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffb2a9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original hookedvit object is now of type: <class 'vit_prisma.models.base_vit.HookedTranscoderViT'>\n",
      "hookedsaevit object is of type: <class 'vit_prisma.models.base_vit.HookedTranscoderViT'>\n"
     ]
    }
   ],
   "source": [
    "# load hookedsaevit\n",
    "from vit_prisma.models.base_vit import HookedTranscoderViT\n",
    "\n",
    "hookedvit.__class__ = HookedTranscoderViT\n",
    "hookedvit.mlp_to_transcoders = {}\n",
    "hookedvit._original_mlps = {}\n",
    "\n",
    "hookedsaevit = hookedvit\n",
    "\n",
    "print(f'Original hookedvit object is now of type: {type(hookedvit)}')\n",
    "print(f'hookedsaevit object is of type: {type(hookedsaevit)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "606ed450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionModelSAERunnerConfig(model_class_name='HookedViT',\n",
      "                           model_name='open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K',\n",
      "                           vit_model_cfg=None,\n",
      "                           model_path=None,\n",
      "                           hook_point_layer=10,\n",
      "                           layer_subtype='ln2.hook_normalized',\n",
      "                           hook_point_head_index=None,\n",
      "                           context_size=50,\n",
      "                           use_cached_activations=False,\n",
      "                           use_patches_only=False,\n",
      "                           cached_activations_path='activations/_network_scratch_s_sonia.joseph_datasets_kaggle_datasets/open-clip:laion_CLIP-ViT-B-32-DataComp.XL-s13B-b90K/blocks.9.ln2.hook_normalized',\n",
      "                           image_size=224,\n",
      "                           architecture='standard',\n",
      "                           b_dec_init_method='geometric_median',\n",
      "                           expansion_factor=64,\n",
      "                           from_pretrained_path=None,\n",
      "                           is_transcoder=True,\n",
      "                           transcoder_with_skip_connection=True,\n",
      "                           out_hook_point_layer=10,\n",
      "                           layer_out_subtype='hook_mlp_out',\n",
      "                           d_out=768,\n",
      "                           _device='cuda',\n",
      "                           seed=42,\n",
      "                           _dtype='float32',\n",
      "                           d_in=768,\n",
      "                           activation_fn_str='topk',\n",
      "                           activation_fn_kwargs={'k': 1024},\n",
      "                           cls_token_only=False,\n",
      "                           max_grad_norm=1.0,\n",
      "                           initialization_method='independent',\n",
      "                           normalize_activations='layer_norm',\n",
      "                           n_batches_in_buffer=20,\n",
      "                           store_batch_size=32,\n",
      "                           num_workers=16,\n",
      "                           num_epochs=1,\n",
      "                           verbose=False,\n",
      "                           l1_coefficient=0.0002,\n",
      "                           lp_norm=1,\n",
      "                           lr=0.0010281538518292835,\n",
      "                           lr_scheduler_name='cosineannealingwarmup',\n",
      "                           lr_warm_up_steps=500,\n",
      "                           train_batch_size=4096,\n",
      "                           dataset_name='imagenet1k',\n",
      "                           dataset_path='/network/scratch/s/sonia.joseph/datasets/kaggle_datasets',\n",
      "                           dataset_train_path='/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/train',\n",
      "                           dataset_val_path='/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/val',\n",
      "                           use_ghost_grads=False,\n",
      "                           feature_sampling_window=1000,\n",
      "                           dead_feature_window=5000,\n",
      "                           dead_feature_threshold=1e-08,\n",
      "                           log_to_wandb=True,\n",
      "                           wandb_project='openclip-transcoders',\n",
      "                           wandb_entity=None,\n",
      "                           wandb_log_frequency=10,\n",
      "                           n_validation_runs=0,\n",
      "                           n_checkpoints=10,\n",
      "                           checkpoint_path='/network/scratch/p/praneet.suresh/openclip-transcoder-checkpoints/4941547c-openclip-transcoders')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(tc_list[10].cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64883f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_img_input(\n",
    "    hooked_vis_model: HookedTranscoderViT,\n",
    "    transcoders: list[SparseAutoencoder],\n",
    "    fwd_hooks,\n",
    "    img_tensor: torch.Tensor,\n",
    "    text_features: torch.Tensor,\n",
    "    labels: list[str],\n",
    "    device=DEVICE,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        img_tensor = img_tensor.unsqueeze(0).to(DEVICE)\n",
    "        vis_out = hooked_vis_model.run_with_hooks_with_transcoders(\n",
    "            img_tensor,\n",
    "            transcoders=transcoders,\n",
    "            fwd_hooks=fwd_hooks,\n",
    "            bwd_hooks=[],\n",
    "        )\n",
    "        image_features = vis_out.to(device)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        top_probs, top_idx = text_probs.squeeze().topk(20)\n",
    "\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_idx)):\n",
    "        print(f'{i + 1}. label: {labels[idx]:<20} | prob: {prob.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "754f6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_activations(\n",
    "    hooked_vis_model: HookedTranscoderViT,\n",
    "    transcoder: SparseAutoencoder,\n",
    "    img_tensor: torch.Tensor,\n",
    "    text_features: torch.Tensor,\n",
    "    labels: list[str],\n",
    "):\n",
    "    top_act_ids: dict[int, list[int]] = {}\n",
    "    hook_layer = transcoder.cfg.hook_point_layer\n",
    "    hook_point = f'blocks.{hook_layer}.mlp.hook_hidden_post'\n",
    "\n",
    "    def get_top_activations_hook(\n",
    "        feature_activations: torch.Tensor,\n",
    "        hook,\n",
    "    ):\n",
    "        nonzero_indices = torch.nonzero(feature_activations, as_tuple=False)\n",
    "        if nonzero_indices.numel() > 0:\n",
    "            feature_ids = nonzero_indices[:, -1]\n",
    "            sorted_feature_ids = sorted(feature_ids.detach().unique().tolist())\n",
    "            top_act_ids[hook_layer] = sorted_feature_ids\n",
    "        else:\n",
    "            top_act_ids[hook_layer] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_tensor = img_tensor.unsqueeze(0).to(DEVICE)\n",
    "        vis_out = hooked_vis_model.run_with_hooks_with_transcoders(\n",
    "            img_tensor,\n",
    "            transcoders=[transcoder],\n",
    "            fwd_hooks=[(hook_point, get_top_activations_hook)],\n",
    "            bwd_hooks=[],\n",
    "            reset_hooks_end=True,\n",
    "        )\n",
    "        image_features = vis_out.to(img_tensor.device)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        top_probs, top_idx = text_probs.squeeze().topk(20)\n",
    "\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_idx)):\n",
    "        print(f'{i + 1}. label: {labels[idx]:<20} | prob: {prob.item():.4f}')\n",
    "\n",
    "    return top_act_ids[hook_layer], top_probs, top_idx"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ff92736",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def pick_noise_level(\n",
    "    model: nn.Module,\n",
    "    embedding_layer: str,\n",
    "    std_multiplier: float = 3,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Pick a noise level to corrupt the input text with, such that the\n",
    "    noise is a multiplier of the stdev of the token embeddings.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        embedding_weights = cast(\n",
    "            torch.Tensor, get_module(model, embedding_layer).weight\n",
    "        )\n",
    "        noise_level_std = embedding_weights.std().item()\n",
    "    return std_multiplier * noise_level_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f89aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def _get_consistent_noise(\n",
    "    self, sample_num: int, subj_len: int, layer_size: int\n",
    ") -> np.typing.NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Returns a numpy array of noise for a given sample number and shape, which is consistent for each combination of inputs\n",
    "    \"\"\"\n",
    "    cache_key = (sample_num, subj_len, layer_size)\n",
    "    if cache_key not in self._noise_cache:\n",
    "        prng = np.random.RandomState(self.random_seed)\n",
    "        self._noise_cache[cache_key] = prng.randn(\n",
    "            self.samples_per_patch, subj_len, layer_size\n",
    "        )[sample_num]\n",
    "    return self._noise_cache[cache_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac90d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def zero_ablate_feature_hook(\n",
    "    feature_activations, hook, feature_ids, position=None, batch_mode=True\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Zero ablate the given activations.\n",
    "\n",
    "    If batch_mode is True, assert the feature activations are batch input.\n",
    "    Otherwise will ablate all the feature_ids in one input.\n",
    "    \"\"\"\n",
    "\n",
    "    if batch_mode is True:\n",
    "        batch_size = feature_activations.shape[0]\n",
    "        assert batch_size == len(feature_activations)\n",
    "\n",
    "        batch_indices = torch.arange(\n",
    "            batch_size, device=feature_activations.device\n",
    "        )\n",
    "        feature_ids_tensor = torch.tensor(\n",
    "            feature_ids, device=feature_activations.device, dtype=torch.long\n",
    "        )\n",
    "\n",
    "        if position is None:\n",
    "            feature_activations[batch_indices, :, feature_ids_tensor] = 0\n",
    "        else:\n",
    "            feature_activations[batch_indices, position, feature_ids_tensor] = 0\n",
    "\n",
    "    elif position is None:\n",
    "        feature_activations[:, :, feature_ids] = 0\n",
    "    else:\n",
    "        feature_activations[:, position, feature_ids] = 0\n",
    "\n",
    "    return feature_activations\n",
    "\n",
    "\n",
    "def gaussian_ablate_feature_hook(\n",
    "    feature_activations,\n",
    "    hook,\n",
    "    feature_ids,\n",
    "    position=None,\n",
    "    sigma=3.0,\n",
    "    # n_repeats=10,\n",
    "    mode='add',\n",
    "    match_scale=True,\n",
    "    eps=1e-6,\n",
    "    seed=42,\n",
    "    batch_mode=True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Add gaussian noise to the feature activations.\n",
    "\n",
    "    Args:\n",
    "        - feature_activations: The feature activations after the activation\n",
    "        function in transcoder.\n",
    "        - hook: The hook to ablate, should be after the activation function in\n",
    "        transcoder.\n",
    "        - feature_ids: The feature ids to ablate. If None, all features are\n",
    "        ablated.\n",
    "        - position: The token position to ablate. If None, all positions are\n",
    "        ablated.\n",
    "        - sigma: The standard deviation of the gaussian noise. In the ROME\n",
    "        paper, they use 3.0, and point out that the noise level should be large\n",
    "        enough to make an effect.\n",
    "        - mode: The mode to add the gaussian noise. Can be 'add' or 'replace'.\n",
    "        - match_scale: Whether to match the scale of the feature activations.\n",
    "            Reccomended to be True since the feature activations can vary.\n",
    "        - eps: The epsilon to avoid division by zero.\n",
    "        - seed: The seed to generate the gaussian noise.\n",
    "        - batch_mode: If True, assert the feature activations are batch input.\n",
    "            Ablate one feature in one input in the batch.\n",
    "            Otherwise will ablate all the feature_ids in one input.\n",
    "\n",
    "    Returns:\n",
    "        The feature activations after the gaussian noise is added.\n",
    "    \"\"\"\n",
    "\n",
    "    current_rng_state = torch.get_rng_state()\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if batch_mode is True:\n",
    "        batch_size = feature_activations.shape[0]\n",
    "        assert batch_size == len(feature_activations)\n",
    "\n",
    "        feature_ids_tensor = torch.tensor(\n",
    "            feature_ids, device=feature_activations.device, dtype=torch.long\n",
    "        )\n",
    "\n",
    "        if position is None:\n",
    "            feature_ids_for_gather = feature_ids_tensor.view(\n",
    "                batch_size, 1, 1\n",
    "            ).expand(-1, feature_activations.shape[1], -1)\n",
    "            target = torch.gather(\n",
    "                feature_activations, 2, feature_ids_for_gather\n",
    "            )\n",
    "\n",
    "            noise_std = sigma\n",
    "            if match_scale:\n",
    "                std = (\n",
    "                    target.detach()\n",
    "                    .float()\n",
    "                    .std(dim=1, keepdim=True, unbiased=False)\n",
    "                    .clamp_min(eps)\n",
    "                )\n",
    "                noise_std *= std\n",
    "\n",
    "            noise = torch.randn_like(target).mul_(noise_std)\n",
    "            ablated_values = target + noise if mode == 'add' else noise\n",
    "\n",
    "            feature_activations.scatter_(\n",
    "                2, feature_ids_for_gather, ablated_values\n",
    "            )\n",
    "    else:\n",
    "        if position is None:\n",
    "            target_slice = (slice(None), slice(None), feature_ids)\n",
    "        else:\n",
    "            target_slice = (slice(None), position, feature_ids)\n",
    "\n",
    "        target = feature_activations[target_slice]\n",
    "\n",
    "        # print(f'target shape: {target.shape}')\n",
    "        # print(f'target sum: {target.sum().item()}')\n",
    "        # print(f'target max: {target.max().item()}')\n",
    "        # print(f'target min: {target.min().item()}')\n",
    "        # print(f'feature_activations shape: {feature_activations.shape}')\n",
    "        # print(f'feature_activations sum: {feature_activations.sum().item()}')\n",
    "        # print(f'feature_activations max: {feature_activations.max().item()}')\n",
    "        # print(f'feature_ids range: {min(feature_ids)} to {max(feature_ids)}')\n",
    "\n",
    "        if match_scale:\n",
    "            dims = (0, 1) if target.dim() == 3 else (0,)\n",
    "            # dims = tuple(range(target.dim()))\n",
    "            std = (\n",
    "                target.detach()\n",
    "                # feature_activations.detach()\n",
    "                .float()\n",
    "                .std(dim=dims, keepdim=True, unbiased=False)\n",
    "                .clamp_min(eps)\n",
    "                .to(target.device)\n",
    "            )\n",
    "            noise_std = sigma * std\n",
    "        else:\n",
    "            noise_std = sigma\n",
    "\n",
    "        # print(f'noise_std: {noise_std}')\n",
    "        # shape_full = (n_repeats, *target.shape)\n",
    "\n",
    "        # set seed for reproducibility\n",
    "        noise = torch.randn_like(target).mul_(noise_std)  # type: ignore\n",
    "\n",
    "        # noise = (\n",
    "        #     torch.randn(shape_full, device=target.device, dtype=target.dtype)\n",
    "        #     .mul_(noise_std)\n",
    "        #     .sum(dim=0)\n",
    "        # )\n",
    "\n",
    "        # see how much element in target are nonzero\n",
    "        # print(f'{target.nonzero().numel()} / {target.numel()} elements are nonzero')\n",
    "\n",
    "        if mode == 'add':\n",
    "            target.add_(noise)\n",
    "\n",
    "        elif mode == 'replace':\n",
    "            target.copy_(noise)\n",
    "\n",
    "        feature_activations[target_slice] = target\n",
    "\n",
    "    torch.set_rng_state(current_rng_state)\n",
    "    return feature_activations\n",
    "\n",
    "\n",
    "def _test_with_ablation(\n",
    "    model,\n",
    "    transcoder,\n",
    "    type,\n",
    "    img_tensor,\n",
    "    text_features,\n",
    "    ablation_features,\n",
    "    labels,\n",
    "    **ablation_kwargs,\n",
    "):\n",
    "    hook_layer = transcoder.cfg.hook_point_layer\n",
    "    hook_point = f'blocks.{hook_layer}.mlp.hook_hidden_post'\n",
    "    if type == 'zero':\n",
    "        ablation_hook = partial(\n",
    "            zero_ablate_feature_hook,\n",
    "            feature_ids=ablation_features,\n",
    "            **ablation_kwargs,\n",
    "        )\n",
    "    elif type == 'gaussian':\n",
    "        ablation_hook = partial(\n",
    "            gaussian_ablate_feature_hook,\n",
    "            feature_ids=ablation_features,\n",
    "            **ablation_kwargs,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Invalid ablation type: {type}')\n",
    "\n",
    "    test_img_input(\n",
    "        model,\n",
    "        transcoder,\n",
    "        [(hook_point, ablation_hook)],\n",
    "        img_tensor,\n",
    "        text_features,\n",
    "        labels,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09bc7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder frozen ablate hook\n",
    "\n",
    "\n",
    "def make_frozen_decoder_ablate_hook(self, tc, feature_ids):\n",
    "    W_dec = self._get_decoder_weight(tc)  # [F, d]\n",
    "    feat_ids = torch.tensor(feature_ids, device=self.device, dtype=torch.long)\n",
    "\n",
    "    def hook_fn(activations, hook):\n",
    "        # activations: X  [B,T,d] at blocks.{L}.mlp.hook_hidden_post\n",
    "        with torch.no_grad():\n",
    "            X = activations\n",
    "            Z = self._encode_latents(tc, X)  # [B,T,F]\n",
    "            # only keep the features to be ablated\n",
    "            if Z.dim() == 3:\n",
    "                Z_sel = torch.zeros_like(Z)\n",
    "                Z_sel[..., feat_ids] = Z[..., feat_ids]  # [B,T,F]\n",
    "            else:\n",
    "                raise RuntimeError('Unexpected latent shape.')\n",
    "            # calculate the contribution: sum_f (a_f * d_f)  -> [B,T,d]\n",
    "            # equivalent to einsum('btf,fd->btd', Z_sel, W_dec)\n",
    "            delta = torch.matmul(Z_sel, W_dec)  # [B,T,d]\n",
    "            return X - delta  # subtract the contribution from the residual\n",
    "\n",
    "    return hook_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d431d328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. label: chemistry            | prob: 0.1975\n",
      "2. label: laboratory bottle    | prob: 0.1142\n",
      "3. label: reagent              | prob: 0.1133\n",
      "4. label: chemical             | prob: 0.1005\n",
      "5. label: science              | prob: 0.0871\n",
      "6. label: research tools       | prob: 0.0841\n",
      "7. label: reaction             | prob: 0.0326\n",
      "8. label: liquid               | prob: 0.0288\n",
      "9. label: titration            | prob: 0.0270\n",
      "10. label: test tube            | prob: 0.0253\n",
      "11. label: experiment           | prob: 0.0253\n",
      "12. label: solvent              | prob: 0.0232\n",
      "13. label: science equipment    | prob: 0.0218\n",
      "14. label: experimentation      | prob: 0.0163\n",
      "15. label: research             | prob: 0.0136\n",
      "16. label: solution             | prob: 0.0123\n",
      "17. label: flask                | prob: 0.0108\n",
      "18. label: ethicality           | prob: 0.0100\n",
      "19. label: beaker               | prob: 0.0079\n",
      "20. label: vial                 | prob: 0.0059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[23,\n",
       " 88,\n",
       " 163,\n",
       " 225,\n",
       " 323,\n",
       " 343,\n",
       " 366,\n",
       " 419,\n",
       " 435,\n",
       " 450,\n",
       " 465,\n",
       " 476,\n",
       " 485,\n",
       " 492,\n",
       " 506,\n",
       " 554,\n",
       " 576,\n",
       " 598,\n",
       " 759,\n",
       " 807,\n",
       " 874,\n",
       " 889,\n",
       " 993,\n",
       " 998,\n",
       " 1011,\n",
       " 1023,\n",
       " 1048,\n",
       " 1109,\n",
       " 1174,\n",
       " 1221,\n",
       " 1237,\n",
       " 1262,\n",
       " 1271,\n",
       " 1300,\n",
       " 1301,\n",
       " 1310,\n",
       " 1358,\n",
       " 1379,\n",
       " 1426,\n",
       " 1464,\n",
       " 1498,\n",
       " 1625,\n",
       " 1639,\n",
       " 1723,\n",
       " 1745,\n",
       " 1780,\n",
       " 1793,\n",
       " 1796,\n",
       " 1882,\n",
       " 1899,\n",
       " 2088,\n",
       " 2130,\n",
       " 2159,\n",
       " 2166,\n",
       " 2197,\n",
       " 2216,\n",
       " 2217,\n",
       " 2238,\n",
       " 2245,\n",
       " 2250,\n",
       " 2259,\n",
       " 2261,\n",
       " 2282,\n",
       " 2312,\n",
       " 2360,\n",
       " 2391,\n",
       " 2405,\n",
       " 2413,\n",
       " 2415,\n",
       " 2434,\n",
       " 2436,\n",
       " 2541,\n",
       " 2575,\n",
       " 2618,\n",
       " 2629,\n",
       " 2716,\n",
       " 2725,\n",
       " 2735,\n",
       " 2745,\n",
       " 2768,\n",
       " 2790,\n",
       " 2818,\n",
       " 2822,\n",
       " 2952,\n",
       " 2959,\n",
       " 3017,\n",
       " 3037,\n",
       " 3053,\n",
       " 3062,\n",
       " 3069,\n",
       " 3100,\n",
       " 3133,\n",
       " 3143,\n",
       " 3146,\n",
       " 3149,\n",
       " 3190,\n",
       " 3194,\n",
       " 3198,\n",
       " 3201,\n",
       " 3324,\n",
       " 3399,\n",
       " 3430,\n",
       " 3458,\n",
       " 3547,\n",
       " 3572,\n",
       " 3625,\n",
       " 3695,\n",
       " 3699,\n",
       " 3759,\n",
       " 3789,\n",
       " 3853,\n",
       " 3872,\n",
       " 3887,\n",
       " 3910,\n",
       " 3966,\n",
       " 3978,\n",
       " 4032,\n",
       " 4056,\n",
       " 4080,\n",
       " 4113,\n",
       " 4130,\n",
       " 4133,\n",
       " 4214,\n",
       " 4223,\n",
       " 4228,\n",
       " 4287,\n",
       " 4317,\n",
       " 4330,\n",
       " 4331,\n",
       " 4376,\n",
       " 4377,\n",
       " 4380,\n",
       " 4409,\n",
       " 4410,\n",
       " 4416,\n",
       " 4423,\n",
       " 4487,\n",
       " 4506,\n",
       " 4573,\n",
       " 4580,\n",
       " 4595,\n",
       " 4656,\n",
       " 4669,\n",
       " 4715,\n",
       " 4748,\n",
       " 4782,\n",
       " 4793,\n",
       " 4798,\n",
       " 4806,\n",
       " 4892,\n",
       " 4923,\n",
       " 4964,\n",
       " 5025,\n",
       " 5041,\n",
       " 5043,\n",
       " 5060,\n",
       " 5063,\n",
       " 5124,\n",
       " 5152,\n",
       " 5160,\n",
       " 5206,\n",
       " 5216,\n",
       " 5224,\n",
       " 5259,\n",
       " 5288,\n",
       " 5292,\n",
       " 5322,\n",
       " 5325,\n",
       " 5423,\n",
       " 5446,\n",
       " 5455,\n",
       " 5515,\n",
       " 5589,\n",
       " 5722,\n",
       " 5785,\n",
       " 5808,\n",
       " 5834,\n",
       " 5857,\n",
       " 5891,\n",
       " 5898,\n",
       " 5981,\n",
       " 6043,\n",
       " 6131,\n",
       " 6184,\n",
       " 6315,\n",
       " 6349,\n",
       " 6438,\n",
       " 6501,\n",
       " 6529,\n",
       " 6581,\n",
       " 6614,\n",
       " 6646,\n",
       " 6705,\n",
       " 6710,\n",
       " 6781,\n",
       " 6821,\n",
       " 6839,\n",
       " 6851,\n",
       " 6880,\n",
       " 6896,\n",
       " 6932,\n",
       " 6961,\n",
       " 6986,\n",
       " 6988,\n",
       " 7002,\n",
       " 7007,\n",
       " 7027,\n",
       " 7073,\n",
       " 7074,\n",
       " 7170,\n",
       " 7177,\n",
       " 7202,\n",
       " 7233,\n",
       " 7260,\n",
       " 7344,\n",
       " 7388,\n",
       " 7400,\n",
       " 7405,\n",
       " 7425,\n",
       " 7447,\n",
       " 7508,\n",
       " 7510,\n",
       " 7531,\n",
       " 7548,\n",
       " 7555,\n",
       " 7561,\n",
       " 7583,\n",
       " 7584,\n",
       " 7601,\n",
       " 7608,\n",
       " 7610,\n",
       " 7633,\n",
       " 7677,\n",
       " 7740,\n",
       " 7743,\n",
       " 7757,\n",
       " 7787,\n",
       " 7799,\n",
       " 7804,\n",
       " 7856,\n",
       " 7878,\n",
       " 7882,\n",
       " 7931,\n",
       " 8029,\n",
       " 8031,\n",
       " 8057,\n",
       " 8083,\n",
       " 8167,\n",
       " 8290,\n",
       " 8310,\n",
       " 8458,\n",
       " 8547,\n",
       " 8554,\n",
       " 8582,\n",
       " 8583,\n",
       " 8592,\n",
       " 8636,\n",
       " 8683,\n",
       " 8700,\n",
       " 8717,\n",
       " 8787,\n",
       " 8866,\n",
       " 8880,\n",
       " 8885,\n",
       " 8908,\n",
       " 8916,\n",
       " 8935,\n",
       " 8943,\n",
       " 8957,\n",
       " 9071,\n",
       " 9241,\n",
       " 9249,\n",
       " 9269,\n",
       " 9326,\n",
       " 9337,\n",
       " 9414,\n",
       " 9465,\n",
       " 9484,\n",
       " 9499,\n",
       " 9530,\n",
       " 9592,\n",
       " 9593,\n",
       " 9622,\n",
       " 9630,\n",
       " 9659,\n",
       " 9675,\n",
       " 9743,\n",
       " 9838,\n",
       " 9915,\n",
       " 9945,\n",
       " 10005,\n",
       " 10030,\n",
       " 10095,\n",
       " 10195,\n",
       " 10332,\n",
       " 10338,\n",
       " 10343,\n",
       " 10349,\n",
       " 10442,\n",
       " 10460,\n",
       " 10480,\n",
       " 10644,\n",
       " 10679,\n",
       " 10705,\n",
       " 10720,\n",
       " 10731,\n",
       " 10742,\n",
       " 10746,\n",
       " 10811,\n",
       " 10869,\n",
       " 10879,\n",
       " 11005,\n",
       " 11089,\n",
       " 11183,\n",
       " 11233,\n",
       " 11300,\n",
       " 11545,\n",
       " 11566,\n",
       " 11628,\n",
       " 11634,\n",
       " 11690,\n",
       " 11697,\n",
       " 11712,\n",
       " 11725,\n",
       " 11739,\n",
       " 11802,\n",
       " 11815,\n",
       " 11881,\n",
       " 11893,\n",
       " 11898,\n",
       " 11899,\n",
       " 11907,\n",
       " 11996,\n",
       " 12027,\n",
       " 12030,\n",
       " 12065,\n",
       " 12101,\n",
       " 12105,\n",
       " 12110,\n",
       " 12147,\n",
       " 12160,\n",
       " 12222,\n",
       " 12247,\n",
       " 12310,\n",
       " 12318,\n",
       " 12518,\n",
       " 12532,\n",
       " 12550,\n",
       " 12574,\n",
       " 12579,\n",
       " 12683,\n",
       " 12691,\n",
       " 12696,\n",
       " 12728,\n",
       " 12741,\n",
       " 12754,\n",
       " 12781,\n",
       " 12800,\n",
       " 12815,\n",
       " 12836,\n",
       " 12871,\n",
       " 12873,\n",
       " 12950,\n",
       " 12967,\n",
       " 13048,\n",
       " 13098,\n",
       " 13200,\n",
       " 13202,\n",
       " 13231,\n",
       " 13233,\n",
       " 13312,\n",
       " 13388,\n",
       " 13399,\n",
       " 13409,\n",
       " 13412,\n",
       " 13451,\n",
       " 13452,\n",
       " 13455,\n",
       " 13474,\n",
       " 13525,\n",
       " 13585,\n",
       " 13611,\n",
       " 13638,\n",
       " 13652,\n",
       " 13721,\n",
       " 13736,\n",
       " 13834,\n",
       " 13837,\n",
       " 13846,\n",
       " 13896,\n",
       " 13933,\n",
       " 13955,\n",
       " 13983,\n",
       " 14016,\n",
       " 14028,\n",
       " 14054,\n",
       " 14060,\n",
       " 14085,\n",
       " 14135,\n",
       " 14150,\n",
       " 14151,\n",
       " 14184,\n",
       " 14233,\n",
       " 14403,\n",
       " 14425,\n",
       " 14483,\n",
       " 14503,\n",
       " 14509,\n",
       " 14534,\n",
       " 14621,\n",
       " 14708,\n",
       " 14733,\n",
       " 14755,\n",
       " 14762,\n",
       " 14767,\n",
       " 14783,\n",
       " 14793,\n",
       " 14810,\n",
       " 14832,\n",
       " 14880,\n",
       " 14882,\n",
       " 15018,\n",
       " 15019,\n",
       " 15034,\n",
       " 15057,\n",
       " 15077,\n",
       " 15138,\n",
       " 15208,\n",
       " 15229,\n",
       " 15265,\n",
       " 15321,\n",
       " 15344,\n",
       " 15358,\n",
       " 15380,\n",
       " 15405,\n",
       " 15426,\n",
       " 15435,\n",
       " 15439,\n",
       " 15440,\n",
       " 15464,\n",
       " 15486,\n",
       " 15497,\n",
       " 15557,\n",
       " 15586,\n",
       " 15620,\n",
       " 15654,\n",
       " 15677,\n",
       " 15684,\n",
       " 15687,\n",
       " 15704,\n",
       " 15709,\n",
       " 15733,\n",
       " 15734,\n",
       " 15747,\n",
       " 15781,\n",
       " 15808,\n",
       " 15827,\n",
       " 15882,\n",
       " 15959,\n",
       " 16023,\n",
       " 16067,\n",
       " 16172,\n",
       " 16223,\n",
       " 16252,\n",
       " 16267,\n",
       " 16268,\n",
       " 16300,\n",
       " 16348,\n",
       " 16371,\n",
       " 16400,\n",
       " 16442,\n",
       " 16445,\n",
       " 16531,\n",
       " 16535,\n",
       " 16569,\n",
       " 16576,\n",
       " 16582,\n",
       " 16606,\n",
       " 16644,\n",
       " 16687,\n",
       " 16727,\n",
       " 16747,\n",
       " 16768,\n",
       " 16808,\n",
       " 16822,\n",
       " 16845,\n",
       " 16851,\n",
       " 16958,\n",
       " 16965,\n",
       " 16985,\n",
       " 16999,\n",
       " 17020,\n",
       " 17035,\n",
       " 17046,\n",
       " 17047,\n",
       " 17050,\n",
       " 17066,\n",
       " 17073,\n",
       " 17097,\n",
       " 17121,\n",
       " 17126,\n",
       " 17164,\n",
       " 17180,\n",
       " 17182,\n",
       " 17216,\n",
       " 17278,\n",
       " 17289,\n",
       " 17326,\n",
       " 17362,\n",
       " 17366,\n",
       " 17372,\n",
       " 17402,\n",
       " 17403,\n",
       " 17404,\n",
       " 17423,\n",
       " 17447,\n",
       " 17453,\n",
       " 17456,\n",
       " 17517,\n",
       " 17523,\n",
       " 17535,\n",
       " 17656,\n",
       " 17680,\n",
       " 17728,\n",
       " 17758,\n",
       " 17790,\n",
       " 17791,\n",
       " 17821,\n",
       " 17844,\n",
       " 17894,\n",
       " 17908,\n",
       " 18078,\n",
       " 18219,\n",
       " 18314,\n",
       " 18369,\n",
       " 18420,\n",
       " 18440,\n",
       " 18539,\n",
       " 18546,\n",
       " 18568,\n",
       " 18647,\n",
       " 18688,\n",
       " 18690,\n",
       " 18817,\n",
       " 18820,\n",
       " 18868,\n",
       " 18880,\n",
       " 19039,\n",
       " 19046,\n",
       " 19065,\n",
       " 19119,\n",
       " 19130,\n",
       " 19140,\n",
       " 19290,\n",
       " 19341,\n",
       " 19345,\n",
       " 19370,\n",
       " 19396,\n",
       " 19429,\n",
       " 19449,\n",
       " 19454,\n",
       " 19522,\n",
       " 19549,\n",
       " 19794,\n",
       " 19921,\n",
       " 19931,\n",
       " 19958,\n",
       " 19968,\n",
       " 19987,\n",
       " 19990,\n",
       " 20031,\n",
       " 20035,\n",
       " 20059,\n",
       " 20120,\n",
       " 20143,\n",
       " 20221,\n",
       " 20223,\n",
       " 20237,\n",
       " 20243,\n",
       " 20371,\n",
       " 20437,\n",
       " 20495,\n",
       " 20497,\n",
       " 20634,\n",
       " 20638,\n",
       " 20787,\n",
       " 20818,\n",
       " 20906,\n",
       " 20916,\n",
       " 20927,\n",
       " 20951,\n",
       " 20954,\n",
       " 21027,\n",
       " 21057,\n",
       " 21062,\n",
       " 21069,\n",
       " 21092,\n",
       " 21095,\n",
       " 21109,\n",
       " 21114,\n",
       " 21127,\n",
       " 21150,\n",
       " 21223,\n",
       " 21365,\n",
       " 21371,\n",
       " 21430,\n",
       " 21435,\n",
       " 21467,\n",
       " 21477,\n",
       " 21597,\n",
       " 21629,\n",
       " 21637,\n",
       " 21653,\n",
       " 21666,\n",
       " 21675,\n",
       " 21680,\n",
       " 21838,\n",
       " 21840,\n",
       " 21899,\n",
       " 21906,\n",
       " 21942,\n",
       " 21957,\n",
       " 22004,\n",
       " 22025,\n",
       " 22035,\n",
       " 22053,\n",
       " 22124,\n",
       " 22180,\n",
       " 22217,\n",
       " 22271,\n",
       " 22308,\n",
       " 22326,\n",
       " 22341,\n",
       " 22559,\n",
       " 22563,\n",
       " 22571,\n",
       " 22602,\n",
       " 22616,\n",
       " 22620,\n",
       " 22634,\n",
       " 22670,\n",
       " 22687,\n",
       " 22747,\n",
       " 22753,\n",
       " 22794,\n",
       " 22829,\n",
       " 22831,\n",
       " 22839,\n",
       " 22840,\n",
       " 22961,\n",
       " 22966,\n",
       " 23021,\n",
       " 23024,\n",
       " 23057,\n",
       " 23060,\n",
       " 23084,\n",
       " 23130,\n",
       " 23151,\n",
       " 23174,\n",
       " 23186,\n",
       " 23223,\n",
       " 23224,\n",
       " 23274,\n",
       " 23331,\n",
       " 23353,\n",
       " 23371,\n",
       " 23455,\n",
       " 23474,\n",
       " 23509,\n",
       " 23536,\n",
       " 23574,\n",
       " 23680,\n",
       " 23697,\n",
       " 23772,\n",
       " 23806,\n",
       " 23821,\n",
       " 23900,\n",
       " 23932,\n",
       " 23946,\n",
       " 23970,\n",
       " 23992,\n",
       " 24019,\n",
       " 24057,\n",
       " 24058,\n",
       " 24115,\n",
       " 24116,\n",
       " 24177,\n",
       " 24204,\n",
       " 24246,\n",
       " 24304,\n",
       " 24325,\n",
       " 24378,\n",
       " 24426,\n",
       " 24430,\n",
       " 24434,\n",
       " 24583,\n",
       " 24604,\n",
       " 24632,\n",
       " 24651,\n",
       " 24710,\n",
       " 24777,\n",
       " 24806,\n",
       " 24834,\n",
       " 24843,\n",
       " 24886,\n",
       " 24942,\n",
       " 24970,\n",
       " 25001,\n",
       " 25047,\n",
       " 25075,\n",
       " 25079,\n",
       " 25090,\n",
       " 25142,\n",
       " 25146,\n",
       " 25179,\n",
       " 25223,\n",
       " 25229,\n",
       " 25236,\n",
       " 25269,\n",
       " 25276,\n",
       " 25306,\n",
       " 25343,\n",
       " 25355,\n",
       " 25422,\n",
       " 25437,\n",
       " 25474,\n",
       " 25484,\n",
       " 25520,\n",
       " 25534,\n",
       " 25586,\n",
       " 25619,\n",
       " 25664,\n",
       " 25670,\n",
       " 25688,\n",
       " 25716,\n",
       " 25737,\n",
       " 25738,\n",
       " 25741,\n",
       " 25755,\n",
       " 25767,\n",
       " 25817,\n",
       " 25862,\n",
       " 25892,\n",
       " 26032,\n",
       " 26037,\n",
       " 26173,\n",
       " 26177,\n",
       " 26212,\n",
       " 26222,\n",
       " 26229,\n",
       " 26253,\n",
       " 26300,\n",
       " 26320,\n",
       " 26393,\n",
       " 26456,\n",
       " 26512,\n",
       " 26537,\n",
       " 26561,\n",
       " 26568,\n",
       " 26633,\n",
       " 26808,\n",
       " 26824,\n",
       " 26887,\n",
       " 26896,\n",
       " 26915,\n",
       " 26920,\n",
       " 26921,\n",
       " 26934,\n",
       " 27021,\n",
       " 27070,\n",
       " 27071,\n",
       " 27128,\n",
       " 27146,\n",
       " 27149,\n",
       " 27187,\n",
       " 27248,\n",
       " 27384,\n",
       " 27405,\n",
       " 27410,\n",
       " 27416,\n",
       " 27425,\n",
       " 27460,\n",
       " 27498,\n",
       " 27510,\n",
       " 27558,\n",
       " 27628,\n",
       " 27652,\n",
       " 27704,\n",
       " 27716,\n",
       " 27730,\n",
       " 27828,\n",
       " 27860,\n",
       " 27913,\n",
       " 27920,\n",
       " 27953,\n",
       " 27954,\n",
       " 27962,\n",
       " 28011,\n",
       " 28062,\n",
       " 28086,\n",
       " 28174,\n",
       " 28207,\n",
       " 28231,\n",
       " 28250,\n",
       " 28333,\n",
       " 28380,\n",
       " 28384,\n",
       " 28434,\n",
       " 28439,\n",
       " 28507,\n",
       " 28509,\n",
       " 28516,\n",
       " 28519,\n",
       " 28533,\n",
       " 28553,\n",
       " 28567,\n",
       " 28576,\n",
       " 28578,\n",
       " 28584,\n",
       " 28597,\n",
       " 28646,\n",
       " 28684,\n",
       " 28725,\n",
       " 28768,\n",
       " 28900,\n",
       " 28934,\n",
       " 28980,\n",
       " 29122,\n",
       " 29141,\n",
       " 29151,\n",
       " 29157,\n",
       " 29167,\n",
       " 29201,\n",
       " 29269,\n",
       " 29293,\n",
       " 29300,\n",
       " 29301,\n",
       " 29324,\n",
       " 29356,\n",
       " 29424,\n",
       " 29451,\n",
       " 29471,\n",
       " 29534,\n",
       " 29554,\n",
       " 29567,\n",
       " 29629,\n",
       " 29630,\n",
       " 29677,\n",
       " 29687,\n",
       " 29690,\n",
       " 29747,\n",
       " 29801,\n",
       " 29809,\n",
       " 29880,\n",
       " 29881,\n",
       " 29893,\n",
       " 29898,\n",
       " 29916,\n",
       " 29942,\n",
       " 29944,\n",
       " 29951,\n",
       " 29968,\n",
       " 30034,\n",
       " 30141,\n",
       " 30150,\n",
       " 30154,\n",
       " 30175,\n",
       " 30193,\n",
       " 30202,\n",
       " 30330,\n",
       " 30346,\n",
       " 30357,\n",
       " 30394,\n",
       " 30447,\n",
       " 30457,\n",
       " 30479,\n",
       " 30612,\n",
       " 30661,\n",
       " 30671,\n",
       " 30702,\n",
       " 30745,\n",
       " 30793,\n",
       " 30876,\n",
       " 30889,\n",
       " 30919,\n",
       " 30986,\n",
       " 31083,\n",
       " 31105,\n",
       " 31124,\n",
       " 31167,\n",
       " 31189,\n",
       " 31201,\n",
       " 31232,\n",
       " 31248,\n",
       " 31249,\n",
       " 31257,\n",
       " 31285,\n",
       " 31287,\n",
       " 31303,\n",
       " 31305,\n",
       " 31322,\n",
       " 31332,\n",
       " 31349,\n",
       " 31376,\n",
       " 31434,\n",
       " 31540,\n",
       " 31623,\n",
       " 31658,\n",
       " 31673,\n",
       " 31684,\n",
       " 31713,\n",
       " 31721,\n",
       " 31752,\n",
       " 31778,\n",
       " 31854,\n",
       " 31855,\n",
       " 31879,\n",
       " 31921,\n",
       " 31936,\n",
       " 31965,\n",
       " 31979,\n",
       " 31989,\n",
       " 32006,\n",
       " 32032,\n",
       " 32070,\n",
       " 32121,\n",
       " 32150,\n",
       " 32167,\n",
       " 32198,\n",
       " 32253,\n",
       " 32254,\n",
       " 32280,\n",
       " 32343,\n",
       " 32378,\n",
       " 32484,\n",
       " 32518,\n",
       " 32555,\n",
       " 32751,\n",
       " 32868,\n",
       " 32877,\n",
       " 32889,\n",
       " 32921,\n",
       " 32986,\n",
       " 33028,\n",
       " 33050,\n",
       " 33177,\n",
       " 33180,\n",
       " 33199,\n",
       " 33238,\n",
       " 33256,\n",
       " 33342,\n",
       " 33361,\n",
       " 33365,\n",
       " 33392,\n",
       " 33412,\n",
       " 33475,\n",
       " 33492,\n",
       " 33525,\n",
       " 33530,\n",
       " 33620,\n",
       " 33624,\n",
       " 33628,\n",
       " 33675,\n",
       " 33681,\n",
       " 33695,\n",
       " 33726,\n",
       " 33729,\n",
       " 33781,\n",
       " 33824,\n",
       " 33842,\n",
       " 33968,\n",
       " 34001,\n",
       " 34013,\n",
       " 34018,\n",
       " 34156,\n",
       " 34201,\n",
       " 34330,\n",
       " 34331,\n",
       " 34360,\n",
       " 34371,\n",
       " 34404,\n",
       " 34417,\n",
       " 34484,\n",
       " 34501,\n",
       " 34578,\n",
       " 34596,\n",
       " 34624,\n",
       " 34656,\n",
       " 34688,\n",
       " 34790,\n",
       " 34828,\n",
       " 34837,\n",
       " 34851,\n",
       " 34871,\n",
       " 34921,\n",
       " 34985,\n",
       " 34999,\n",
       " 35147,\n",
       " 35243,\n",
       " 35281,\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcoder = tc_list[0]\n",
    "\n",
    "transcoder.eval()\n",
    "hookedsaevit.eval()\n",
    "top_act_ids, top_probs, top_idx = get_top_activations(\n",
    "    hookedsaevit,\n",
    "    transcoder,\n",
    "    img_tensor,  # type: ignore\n",
    "    text_features,\n",
    "    final_labels,\n",
    ")\n",
    "\n",
    "top_act_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5685df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1419"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_act_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "960b1cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. label: graphic              | prob: 0.0890\n",
      "2. label: uptempo              | prob: 0.0434\n",
      "3. label: ethicality           | prob: 0.0328\n",
      "4. label: immorality           | prob: 0.0275\n",
      "5. label: solution             | prob: 0.0270\n",
      "6. label: misappropriation     | prob: 0.0268\n",
      "7. label: chemistry            | prob: 0.0265\n",
      "8. label: experimentation      | prob: 0.0253\n",
      "9. label: disreputability      | prob: 0.0247\n",
      "10. label: which                | prob: 0.0219\n",
      "11. label: research             | prob: 0.0194\n",
      "12. label: reaction             | prob: 0.0188\n",
      "13. label: inclusiveness        | prob: 0.0188\n",
      "14. label: autocratic           | prob: 0.0181\n",
      "15. label: science              | prob: 0.0165\n",
      "16. label: compound             | prob: 0.0159\n",
      "17. label: research tools       | prob: 0.0153\n",
      "18. label: unpremeditatedly     | prob: 0.0148\n",
      "19. label: antisocial           | prob: 0.0143\n",
      "20. label: equivalence          | prob: 0.0134\n"
     ]
    }
   ],
   "source": [
    "# ablation_features = list(range(20000, 40000))\n",
    "ablation_features = top_act_ids\n",
    "_test_with_ablation(\n",
    "    hookedsaevit,\n",
    "    transcoder,\n",
    "    'zero',\n",
    "    img_tensor,\n",
    "    text_features,\n",
    "    ablation_features,\n",
    "    final_labels,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bae18fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    import gc\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6907d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AblationExperimentRunner:\n",
    "    \"\"\"\n",
    "    Encapsulates the logic for running a suite of ablation experiments\n",
    "    on a HookedTranscoderViT model, ensuring comparability between different\n",
    "    ablation methods by using a feature injection technique. Will run 4 types of\n",
    "    forward passes:\n",
    "    - Original CLIP baseline\n",
    "    - Transcoder baseline\n",
    "    - Zero Ablation\n",
    "    - Gaussian Noise Ablation (with n_gn_samples samples)\n",
    "\n",
    "    The feature activations from the transcoder baseline are used to inject\n",
    "    zero and Gaussian noise into the model for apple-to-apple comparison.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: 'HookedTranscoderViT',\n",
    "        transcoders: list[SparseAutoencoder],\n",
    "        img_tensor: torch.Tensor,\n",
    "        labels: list[str],\n",
    "        text_features: torch.Tensor,\n",
    "        device='cuda',\n",
    "        activated_layers: Optional[list[int]] = None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.transcoders = {tc.cfg.hook_point_layer: tc for tc in transcoders}\n",
    "        self.img_tensor = img_tensor.to(device)\n",
    "        self.labels = labels\n",
    "        self.text_features = text_features.to(device)\n",
    "        self.device = device\n",
    "        self.gn_generator = torch.Generator(device=device)\n",
    "        self.results: dict[str, Any] = {}\n",
    "        self.activated_layers = activated_layers\n",
    "\n",
    "    @staticmethod\n",
    "    def set_seeds(seed: int):\n",
    "        \"\"\"Sets random seeds for reproducibility.\"\"\"\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def _format_text_probs(self, probs, idxs) -> dict[str, float]:\n",
    "        \"\"\"Formats text probabilities into a dictionary.\"\"\"\n",
    "        return {\n",
    "            self.labels[idx.item()]: prob.item()\n",
    "            for prob, idx in zip(probs, idxs)\n",
    "        }\n",
    "\n",
    "    def _run_forward_pass(\n",
    "        self,\n",
    "        transcoders_to_use: list[SparseAutoencoder],\n",
    "        fwd_hooks,\n",
    "        batch_size=1,\n",
    "        capture_config: Optional[dict[str, str]] = None,\n",
    "    ) -> dict[str, Any]:\n",
    "        \"\"\"Helper to run the model and get logits and text probabilities.\"\"\"\n",
    "        all_fwd_hooks = list(fwd_hooks)\n",
    "        captured_data = {}\n",
    "\n",
    "        if capture_config is not None:\n",
    "            for name, hook_point in capture_config.items():\n",
    "                captured_data[name] = None\n",
    "\n",
    "                def make_capture_hook(capture_name: str):\n",
    "                    def _capture_hook(activations, hook):\n",
    "                        nonlocal captured_data\n",
    "                        captured_data[capture_name] = (\n",
    "                            activations.clone().detach()\n",
    "                        )\n",
    "\n",
    "                    return _capture_hook\n",
    "\n",
    "                all_fwd_hooks.append((hook_point, make_capture_hook(name)))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_input = self.img_tensor.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
    "            img_input = img_input.to(self.device)\n",
    "\n",
    "            vis_out = self.model.run_with_hooks_with_transcoders(\n",
    "                img_input,\n",
    "                transcoders=transcoders_to_use,\n",
    "                fwd_hooks=all_fwd_hooks,\n",
    "                bwd_hooks=[],\n",
    "                reset_hooks_end=True,\n",
    "            )\n",
    "            image_features = vis_out.to(self.device)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            logits = 100.0 * image_features @ self.text_features.T\n",
    "            text_probs = logits.softmax(dim=-1)\n",
    "\n",
    "        # clear the gpu memory\n",
    "        del img_input, vis_out, image_features\n",
    "        clear_gpu_memory()\n",
    "\n",
    "        results = {\n",
    "            'text_probs': text_probs,\n",
    "            'logits': logits,\n",
    "        }\n",
    "        if capture_config is not None:\n",
    "            results['captured_data'] = captured_data\n",
    "\n",
    "        return results\n",
    "\n",
    "    def run_original_clip_baseline(self, seed: int):\n",
    "        \"\"\"Runs the baseline CLIP model without any transcoders.\"\"\"\n",
    "        print(f'--- [Seed {seed}] Running Original CLIP Baseline ---')\n",
    "        self.set_seeds(seed)\n",
    "        self.model.eval()\n",
    "        # This resets all transcoders, restoring the original MLP layers\n",
    "        # self.model.reset_transcoders()\n",
    "        results = self._run_forward_pass(\n",
    "            transcoders_to_use=[], fwd_hooks=[]\n",
    "        )  # shape: (batch_size=1, num_text_features)\n",
    "        top_probs, top_idx = results['text_probs'].squeeze(dim=0).topk(20)\n",
    "        self.results[f'original_clip_seed{seed}'] = {\n",
    "            'text_probs': self._format_text_probs(top_probs, top_idx),\n",
    "            'logits': results['logits'].squeeze(dim=0),\n",
    "        }\n",
    "        print('Original CLIP Baseline Done.')\n",
    "        clear_gpu_memory()\n",
    "\n",
    "    def set_activated_layers(self, layers: Optional[list[int]]):\n",
    "        if layers is None:\n",
    "            self.activated_layers = None\n",
    "            return\n",
    "        layers = sorted(set(layers))\n",
    "        for l in layers:\n",
    "            if l not in self.transcoders:\n",
    "                raise ValueError(f'Layer {l} not found in the transcoders.')\n",
    "        self.activated_layers = layers\n",
    "\n",
    "    def _ctx(self, layers: Optional[list[int]]):\n",
    "        return 'all' if layers is None else '_'.join(map(str, layers))\n",
    "\n",
    "    def _transcoder_layers(self, activated_layers: Optional[list[int]]):\n",
    "        if activated_layers is None:\n",
    "            activated_layers = self.activated_layers\n",
    "        if activated_layers is None:\n",
    "            return [\n",
    "                self.transcoders[l] for l in sorted(self.transcoders.keys())\n",
    "            ]\n",
    "        for l in activated_layers:\n",
    "            if l not in self.transcoders.keys():\n",
    "                raise ValueError(f'Layer {l} not found in the transcoders.')\n",
    "        return [self.transcoders[l] for l in activated_layers]\n",
    "\n",
    "    def _set_activated_layers(\n",
    "        self,\n",
    "        layer_idx: Union[int, list[int]],\n",
    "        activated_layers: Optional[list[int]],\n",
    "    ):\n",
    "        if isinstance(layer_idx, int):\n",
    "            layers_to_add = [layer_idx]\n",
    "        else:\n",
    "            layers_to_add = list(layer_idx)\n",
    "        if activated_layers is None:\n",
    "            activated_layers = self.activated_layers\n",
    "        if activated_layers is None:\n",
    "            activated_layers = layers_to_add\n",
    "        else:\n",
    "            activated_layers = sorted(set([*activated_layers, *layers_to_add]))\n",
    "        return activated_layers\n",
    "\n",
    "    def _get_activated_layers(self, layer_idx: Union[int, list[int]]):\n",
    "        \"\"\"Get the activated layers for replacing the transcoder layers.\"\"\"\n",
    "        if self.activated_layers is None:\n",
    "            return layer_idx\n",
    "        return self.activated_layers\n",
    "\n",
    "    def agg_gn_results(self, key: str):\n",
    "        \"\"\"Aggregate the Gaussian noise results for a given key.\"\"\"\n",
    "        if key not in self.results:\n",
    "            raise ValueError(f'Key {key} not found in results.')\n",
    "        gn_results = self.results[key]\n",
    "\n",
    "        if (\n",
    "            'all_features' in key\n",
    "        ):  # Handles single run with all features ablated\n",
    "            logits = [sample_res['all']['logits'] for sample_res in gn_results]\n",
    "            mean_logits = torch.stack(logits).mean(dim=0)\n",
    "            mean_text_probs = mean_logits.softmax(dim=-1)\n",
    "            top_probs, top_idx = mean_text_probs.topk(20)\n",
    "            return self._format_text_probs(top_probs, top_idx), mean_logits\n",
    "\n",
    "        elif (\n",
    "            'each_feature' in key\n",
    "        ):  # Handles batch run with one feature ablated per item\n",
    "            logits_per_feature = {}\n",
    "            for sample_res in gn_results:\n",
    "                for feat_id, fea_data in sample_res.items():\n",
    "                    logits_per_feature.setdefault(feat_id, []).append(\n",
    "                        fea_data['logits']\n",
    "                    )\n",
    "\n",
    "            aggregated_results = {}\n",
    "            for feat_id, logits_list in logits_per_feature.items():\n",
    "                mean_logits = torch.stack(logits_list).mean(dim=0)\n",
    "                mean_text_probs = mean_logits.softmax(dim=-1)\n",
    "                top_probs, top_idx = mean_text_probs.topk(20)\n",
    "                aggregated_results[feat_id] = {\n",
    "                    'text_probs': self._format_text_probs(top_probs, top_idx),\n",
    "                    'mean_logits': mean_logits,\n",
    "                }\n",
    "            return aggregated_results\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Invalid key format: {key}. Key must contain \"all_features\" or \"each_feature\".'\n",
    "            )\n",
    "\n",
    "    def get_inverse_ids(\n",
    "        self, features_to_preserve: list[int], total_features: int\n",
    "    ) -> list[int]:\n",
    "        total_features_set = set(range(total_features))\n",
    "        features_to_preserve_set = set(features_to_preserve)\n",
    "        return sorted(list(total_features_set - features_to_preserve_set))\n",
    "\n",
    "    def run_transcoder_baseline(\n",
    "        self,\n",
    "        layer_idx: Union[int, list[int]],\n",
    "        seed: int,\n",
    "        activated_layers: Optional[list[int]] = None,\n",
    "    ):\n",
    "        \"\"\"Runs the model with transcoders replacing the original MLP layers.\n",
    "\n",
    "        Returns:\n",
    "            captured_activations: The activations from the transcoder baseline.\n",
    "            top_act_ids: The top activation feature ids for each layer.\n",
    "\n",
    "        Args:\n",
    "            layer_idx: The layer index (one layer) to capture activations on.\n",
    "            activated_layers: The layer indices to replace the transcoder layers\n",
    "            for.\n",
    "        \"\"\"\n",
    "        # set the activated layers\n",
    "        activated_layers = self._set_activated_layers(\n",
    "            layer_idx, activated_layers\n",
    "        )\n",
    "        ctx = self._ctx(activated_layers)\n",
    "\n",
    "        print(f'Replacing the transcoder layers for L{ctx}.')\n",
    "        print(\n",
    "            f'--- [Seed {seed}] Running Transcoder Baseline for L{layer_idx} ---'\n",
    "        )\n",
    "        self.set_seeds(seed)\n",
    "        self.model.eval()\n",
    "\n",
    "        transcoders = self._transcoder_layers(activated_layers)\n",
    "\n",
    "        captured_activations = dict()\n",
    "        top_act_ids: dict[int, list[int]] = {}\n",
    "\n",
    "        layers = [layer_idx] if isinstance(layer_idx, int) else list(layer_idx)\n",
    "        layer_ctx = (\n",
    "            str(layer_idx)\n",
    "            if isinstance(layer_idx, int)\n",
    "            else '_'.join(map(str, layers))\n",
    "        )\n",
    "\n",
    "        fwd_hooks = []\n",
    "        for l in layers:\n",
    "            hook_point = f'blocks.{l}.mlp.hook_hidden_post'\n",
    "            captured_activations[l] = None\n",
    "            top_act_ids[l] = []\n",
    "\n",
    "            def make_capture_hook(layer: int):\n",
    "                def capture_hook(activations, hook):\n",
    "                    nonlocal captured_activations, top_act_ids\n",
    "                    captured_activations[layer] = (\n",
    "                        activations[0:1].clone().detach()\n",
    "                    )\n",
    "                    nonzero_indices = torch.nonzero(activations, as_tuple=False)\n",
    "                    if nonzero_indices.numel() > 0:\n",
    "                        feature_ids = nonzero_indices[:, -1]\n",
    "                        sorted_feature_ids = sorted(\n",
    "                            feature_ids.detach().unique().tolist()\n",
    "                        )\n",
    "                        top_act_ids[layer] = sorted_feature_ids\n",
    "                    else:\n",
    "                        top_act_ids[layer] = []\n",
    "\n",
    "                return capture_hook\n",
    "\n",
    "            fwd_hooks.append((hook_point, make_capture_hook(l)))\n",
    "\n",
    "        results = self._run_forward_pass(\n",
    "            transcoders_to_use=transcoders,\n",
    "            fwd_hooks=fwd_hooks,\n",
    "        )\n",
    "\n",
    "        if any(v is None for v in captured_activations.values()):\n",
    "            raise RuntimeError('Failed to capture activations.')\n",
    "\n",
    "        top_probs, top_idx = results['text_probs'].squeeze(dim=0).topk(20)\n",
    "        logits = results['logits'].squeeze(dim=0)\n",
    "\n",
    "        self.results[f'transcoder_L{layer_ctx}_TC{ctx}_seed{seed}'] = {\n",
    "            'text_probs': self._format_text_probs(top_probs, top_idx),\n",
    "            'logits': logits,\n",
    "            'captured_activations': captured_activations,\n",
    "            'top_act_ids': top_act_ids,\n",
    "        }\n",
    "\n",
    "        return captured_activations, top_act_ids\n",
    "\n",
    "    def _run_ablation_suite_batch(\n",
    "        self,\n",
    "        ablation_feature_ids,\n",
    "        runtime_batch_size,\n",
    "        transcoders,\n",
    "        zero_hook_builder,\n",
    "        gn_hook_builder,\n",
    "        seed,\n",
    "        n_gn_samples,\n",
    "        layer_idx,\n",
    "        ctx,\n",
    "        capture_config: Optional[dict[str, str]] = None,\n",
    "        output_prefix: str = '',\n",
    "    ):\n",
    "        all_zero_ablation_results = {}\n",
    "        all_gn_ablation_results = [{} for _ in range(n_gn_samples)]\n",
    "\n",
    "        runtime_batch_size = min(runtime_batch_size, len(ablation_feature_ids))\n",
    "        for i in range(0, len(ablation_feature_ids), runtime_batch_size):\n",
    "            chunk_feature_ids = ablation_feature_ids[i : i + runtime_batch_size]\n",
    "            chunk_size = len(chunk_feature_ids)\n",
    "\n",
    "            chunk_num = i // runtime_batch_size + 1\n",
    "            total_chunks = (\n",
    "                len(ablation_feature_ids) - 1\n",
    "            ) // runtime_batch_size + 1\n",
    "            print(\n",
    "                f'Running zero ablation for chunk {chunk_num} of '\n",
    "                f'{total_chunks}...'\n",
    "            )\n",
    "\n",
    "            zero_hooks = zero_hook_builder(chunk_feature_ids)\n",
    "            zero_results = self._run_forward_pass(\n",
    "                transcoders_to_use=transcoders,\n",
    "                fwd_hooks=zero_hooks,\n",
    "                batch_size=chunk_size,\n",
    "                capture_config=capture_config,\n",
    "            )\n",
    "            # process the results\n",
    "            for j, feature_id in enumerate(chunk_feature_ids):\n",
    "                top_probs, top_idx = zero_results['text_probs'][j].topk(20)\n",
    "                all_zero_ablation_results[feature_id] = {\n",
    "                    'text_probs': self._format_text_probs(top_probs, top_idx),\n",
    "                    'logits': zero_results['logits'][j],\n",
    "                }\n",
    "                if capture_config is not None:\n",
    "                    assert 'captured_data' in zero_results, (\n",
    "                        'captured_data not found in results'\n",
    "                    )\n",
    "                    all_zero_ablation_results[feature_id]['captured_data'] = {\n",
    "                        k: v[j]\n",
    "                        for k, v in zero_results['captured_data'].items()\n",
    "                    }\n",
    "\n",
    "            print('Zero Ablation Done.')\n",
    "\n",
    "            # 2b. Gaussian Noise Ablation\n",
    "            print(\n",
    "                f'--- [Seed {seed}] Running Gaussian Noise Ablation for '\n",
    "                f'L{layer_idx} ({n_gn_samples} samples) ---'\n",
    "            )\n",
    "            for sample_idx in range(n_gn_samples):\n",
    "                gn_seed = seed * 100 + sample_idx\n",
    "\n",
    "                # def inject_hook_gn_ablation(activations, hook):\n",
    "                #     modified_activations = captured_activations.repeat(\n",
    "                #         activations.shape[0], 1, 1\n",
    "                #     )\n",
    "                #     return gaussian_ablate_feature_hook(\n",
    "                #         modified_activations,\n",
    "                #         hook,\n",
    "                #         feature_ids=chunk_feature_ids,\n",
    "                #         sigma=sigma,\n",
    "                #         match_scale=True,\n",
    "                #         seed=gn_seed,\n",
    "                #         batch_mode=True,\n",
    "                #     )\n",
    "                gn_hooks = gn_hook_builder(chunk_feature_ids, gn_seed)\n",
    "                gn_results = self._run_forward_pass(\n",
    "                    transcoders_to_use=transcoders,\n",
    "                    fwd_hooks=gn_hooks,\n",
    "                    batch_size=chunk_size,\n",
    "                    capture_config=capture_config,\n",
    "                )\n",
    "                # process the results\n",
    "                for j, feature_id in enumerate(chunk_feature_ids):\n",
    "                    top_probs, top_idx = gn_results['text_probs'][j].topk(20)\n",
    "                    all_gn_ablation_results[sample_idx][feature_id] = {\n",
    "                        'text_probs': self._format_text_probs(\n",
    "                            top_probs, top_idx\n",
    "                        ),\n",
    "                        'logits': gn_results['logits'][j],\n",
    "                    }\n",
    "                    if capture_config is not None:\n",
    "                        assert 'captured_data' in gn_results, (\n",
    "                            'captured_data not found in results'\n",
    "                        )\n",
    "                        all_gn_ablation_results[sample_idx][feature_id][\n",
    "                            'captured_data'\n",
    "                        ] = {\n",
    "                            k: v[j]\n",
    "                            for k, v in gn_results['captured_data'].items()\n",
    "                        }\n",
    "\n",
    "            print('GN Ablation Done.')\n",
    "\n",
    "        self.results[\n",
    "            f'{output_prefix}zero_abl_L{layer_idx}_TC{ctx}_seed{seed}_each_feature'\n",
    "        ] = all_zero_ablation_results\n",
    "        self.results[\n",
    "            f'{output_prefix}gn_abl_L{layer_idx}_TC{ctx}_seed{seed}_each_feature'\n",
    "        ] = all_gn_ablation_results\n",
    "\n",
    "    def _get_zero_abl_hook(\n",
    "        self, captured_activations, hook_point, feature_ids, batch_mode=True\n",
    "    ):\n",
    "        def inject_hook_zero_ablation(activations, hook):\n",
    "            return zero_ablate_feature_hook(\n",
    "                captured_activations.clone()\n",
    "                .detach()\n",
    "                .repeat(activations.shape[0], 1, 1),\n",
    "                hook,\n",
    "                feature_ids=feature_ids,\n",
    "                batch_mode=batch_mode,\n",
    "            )\n",
    "\n",
    "        return (hook_point, inject_hook_zero_ablation)\n",
    "\n",
    "    def _get_gn_abl_hook(\n",
    "        self,\n",
    "        captured_activations,\n",
    "        hook_point,\n",
    "        feature_ids,\n",
    "        sigma,\n",
    "        seed,\n",
    "        batch_mode=True,\n",
    "    ):\n",
    "        def inject_hook_gn_ablation(activations, hook):\n",
    "            return gaussian_ablate_feature_hook(\n",
    "                captured_activations.clone()\n",
    "                .detach()\n",
    "                .repeat(activations.shape[0], 1, 1),\n",
    "                hook,\n",
    "                feature_ids=feature_ids,\n",
    "                sigma=sigma,\n",
    "                match_scale=True,\n",
    "                seed=seed,\n",
    "                batch_mode=batch_mode,\n",
    "            )\n",
    "\n",
    "        return (hook_point, inject_hook_gn_ablation)\n",
    "\n",
    "    def _build_zero_hooks_builder(\n",
    "        self, captured_activations, hook_point, batch_mode: bool\n",
    "    ):\n",
    "        def builder(feature_ids):\n",
    "            return [\n",
    "                self._get_zero_abl_hook(\n",
    "                    captured_activations,\n",
    "                    hook_point,\n",
    "                    feature_ids,\n",
    "                    batch_mode=batch_mode,\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        return builder\n",
    "\n",
    "    def _build_gn_hooks_builder(\n",
    "        self, captured_activations, hook_point, sigma: float, batch_mode: bool\n",
    "    ):\n",
    "        def builder(feature_ids, gn_seed: int):\n",
    "            return [\n",
    "                self._get_gn_abl_hook(\n",
    "                    captured_activations,\n",
    "                    hook_point,\n",
    "                    feature_ids,\n",
    "                    sigma,\n",
    "                    gn_seed,\n",
    "                    batch_mode=batch_mode,\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        return builder\n",
    "\n",
    "    def _run_ablation_suite_single(\n",
    "        self,\n",
    "        # ablation_feature_ids,\n",
    "        # captured_activations,\n",
    "        transcoders,\n",
    "        zero_hooks,\n",
    "        gn_hook_builder,\n",
    "        # hook_point,\n",
    "        seed,\n",
    "        n_gn_samples,\n",
    "        # sigma,\n",
    "        layer_idx,\n",
    "        ctx,\n",
    "        capture_config: Optional[dict[str, str]] = None,\n",
    "        output_prefix: str = '',\n",
    "    ):\n",
    "        all_zero_ablation_results = {'all': {}}\n",
    "        all_gn_ablation_results = [{} for _ in range(n_gn_samples)]\n",
    "\n",
    "        zero_results = self._run_forward_pass(\n",
    "            transcoders_to_use=transcoders,\n",
    "            fwd_hooks=zero_hooks,\n",
    "            capture_config=capture_config,\n",
    "        )\n",
    "        # process the results\n",
    "        top_probs, top_idx = zero_results['text_probs'].squeeze(dim=0).topk(20)\n",
    "        all_zero_ablation_results['all'] = {\n",
    "            'text_probs': self._format_text_probs(top_probs, top_idx),\n",
    "            'logits': zero_results['logits'].squeeze(dim=0),\n",
    "        }\n",
    "        if capture_config is not None:\n",
    "            assert 'captured_data' in zero_results, (\n",
    "                'captured_data not found in results'\n",
    "            )\n",
    "            all_zero_ablation_results['all']['captured_data'] = {\n",
    "                k: v.squeeze(dim=0)\n",
    "                for k, v in zero_results['captured_data'].items()\n",
    "            }\n",
    "        print('Zero Ablation Done.')\n",
    "\n",
    "        print(\n",
    "            f'--- [Seed {seed}] Running Gaussian Noise Ablation for '\n",
    "            f'L{layer_idx} ({n_gn_samples} samples) ---'\n",
    "        )\n",
    "        for sample_idx in range(n_gn_samples):\n",
    "            gn_seed = seed * 100 + sample_idx\n",
    "            gn_hooks = (\n",
    "                gn_hook_builder(gn_seed)\n",
    "                if callable(gn_hook_builder)\n",
    "                else gn_hook_builder[sample_idx]\n",
    "            )\n",
    "            gn_results = self._run_forward_pass(\n",
    "                transcoders_to_use=transcoders,\n",
    "                fwd_hooks=gn_hooks,\n",
    "                capture_config=capture_config,\n",
    "            )\n",
    "            top_probs, top_idx = (\n",
    "                gn_results['text_probs'].squeeze(dim=0).topk(20)\n",
    "            )\n",
    "            all_gn_ablation_results[sample_idx]['all'] = {\n",
    "                'text_probs': self._format_text_probs(top_probs, top_idx),\n",
    "                'logits': gn_results['logits'].squeeze(dim=0),\n",
    "            }\n",
    "            if capture_config is not None:\n",
    "                assert 'captured_data' in gn_results, (\n",
    "                    'captured_data not found in results'\n",
    "                )\n",
    "                all_gn_ablation_results[sample_idx]['all']['captured_data'] = {\n",
    "                    k: v.squeeze(dim=0)\n",
    "                    for k, v in gn_results['captured_data'].items()\n",
    "                }\n",
    "        print('GN Ablation Done.')\n",
    "        self.results[\n",
    "            f'{output_prefix}zero_abl_L{layer_idx}_TC{ctx}_seed{seed}_all_features'\n",
    "        ] = all_zero_ablation_results\n",
    "        self.results[\n",
    "            f'{output_prefix}gn_abl_L{layer_idx}_TC{ctx}_seed{seed}_all_features'\n",
    "        ] = all_gn_ablation_results\n",
    "\n",
    "    def run_ablation_suite(\n",
    "        self,\n",
    "        layer_idx: int,\n",
    "        seed: int = 42,\n",
    "        n_gn_samples: int = 10,\n",
    "        sigma: float = 1.0,\n",
    "        batch_mode: bool = True,\n",
    "        activated_layers: Optional[list[int]] = None,\n",
    "        runtime_batch_size: int = 512,\n",
    "        ablation_feature_ids: Optional[list[int]] = None,\n",
    "        capture_config: Optional[dict[str, str]] = None,\n",
    "        output_prefix: str = '',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Runs the full suite of experiments for a single layer and seed.\n",
    "        1. Captures baseline activations with the transcoder if not already run.\n",
    "        2. Runs zero and Gaussian noise ablations by injecting modified versions\n",
    "           of the captured activations.\n",
    "\n",
    "        Args:\n",
    "            layer_idx: The layer index to run the experiments on.\n",
    "            n_gn_samples: The number of Gaussian noise samples to run.\n",
    "            sigma: The standard deviation of the Gaussian noise.\n",
    "            batch_mode: Whether to run the ablation in batch mode.\n",
    "            activated_layers: The layer indices to replace the transcoder layers\n",
    "            for.\n",
    "            runtime_batch_size: The batch size to use for the runtime.\n",
    "            ablation_feature_ids: The feature ids to ablate. If None, will use\n",
    "                the top activation feature ids for each layer.\n",
    "        \"\"\"\n",
    "        # set the activated layers\n",
    "        activated_layers = self._set_activated_layers(\n",
    "            layer_idx, activated_layers\n",
    "        )\n",
    "        ctx = self._ctx(activated_layers)\n",
    "\n",
    "        self.set_seeds(seed)\n",
    "        self.model.eval()\n",
    "\n",
    "        # --- Step 1: Run the transcoder baseline if not already run ---\n",
    "        layer_ctx = str(layer_idx)\n",
    "        if f'transcoder_L{layer_ctx}_TC{ctx}_seed{seed}' not in self.results:\n",
    "            captured_activations, top_act_ids = self.run_transcoder_baseline(\n",
    "                layer_idx, seed, activated_layers\n",
    "            )\n",
    "        else:\n",
    "            captured_activations = self.results[\n",
    "                f'transcoder_L{layer_ctx}_TC{ctx}_seed{seed}'\n",
    "            ]['captured_activations']\n",
    "            top_act_ids = self.results[\n",
    "                f'transcoder_L{layer_ctx}_TC{ctx}_seed{seed}'\n",
    "            ]['top_act_ids']\n",
    "\n",
    "        if ablation_feature_ids is None:\n",
    "            ablation_feature_ids = top_act_ids[layer_idx]\n",
    "            assert ablation_feature_ids is not None, (\n",
    "                'No top activation feature ids found.'\n",
    "            )\n",
    "\n",
    "        print(f'Top activation features number: {len(top_act_ids[layer_idx])}')\n",
    "\n",
    "        # --- Step 2: Run ablation experiments using injection ---\n",
    "        hook_point = f'blocks.{layer_idx}.mlp.hook_hidden_post'\n",
    "        transcoders = self._transcoder_layers(activated_layers)\n",
    "\n",
    "        print(f'--- [Seed {seed}] Running Zero Ablation for L{layer_idx} ---')\n",
    "        zero_hooks_builder = self._build_zero_hooks_builder(\n",
    "            captured_activations[layer_idx], hook_point, batch_mode\n",
    "        )\n",
    "        gn_hooks_builder = self._build_gn_hooks_builder(\n",
    "            captured_activations[layer_idx], hook_point, sigma, batch_mode\n",
    "        )\n",
    "\n",
    "        if batch_mode:\n",
    "            self._run_ablation_suite_batch(\n",
    "                ablation_feature_ids,\n",
    "                runtime_batch_size,\n",
    "                transcoders,\n",
    "                zero_hooks_builder,\n",
    "                gn_hooks_builder,\n",
    "                seed,\n",
    "                n_gn_samples,\n",
    "                layer_idx,\n",
    "                ctx,\n",
    "            )\n",
    "\n",
    "        # ablate all the features in one forward pass\n",
    "        else:\n",
    "            print(\n",
    "                f'--- [Seed {seed}] Running Zero Ablation for L{layer_idx} ---'\n",
    "            )\n",
    "\n",
    "            zero_hooks = zero_hooks_builder(ablation_feature_ids)\n",
    "            gn_hooks = [\n",
    "                gn_hooks_builder(ablation_feature_ids, seed * 100 + sample_idx)\n",
    "                for sample_idx in range(n_gn_samples)\n",
    "            ]\n",
    "\n",
    "            self._run_ablation_suite_single(\n",
    "                transcoders,\n",
    "                zero_hooks,\n",
    "                gn_hooks,\n",
    "                seed,\n",
    "                n_gn_samples,\n",
    "                layer_idx,\n",
    "                ctx,\n",
    "                capture_config=capture_config,\n",
    "                output_prefix=output_prefix,\n",
    "            )\n",
    "\n",
    "        print('Ablation Suite Done.')\n",
    "        clear_gpu_memory()\n",
    "\n",
    "    def get_fea_act_summary(\n",
    "        self, base_activations, post_activations, monitored_ids\n",
    "    ):\n",
    "        \"\"\"Sum over batch and patch dimensions\"\"\"\n",
    "        base_sum = base_activations.sum(\n",
    "            dim=tuple(range(base_activations.dim() - 1))\n",
    "        )\n",
    "        post_sum = post_activations.sum(\n",
    "            dim=tuple(range(post_activations.dim() - 1))\n",
    "        )\n",
    "        summary = {}\n",
    "        # if the monitored_ids is not provided, use all the features\n",
    "        if monitored_ids is None:\n",
    "            monitored_ids = range(base_sum.shape[0])\n",
    "\n",
    "        # for all features\n",
    "        base_mean = base_sum.mean(dim=0)\n",
    "        post_mean = post_sum.mean(dim=0)\n",
    "        direction = (\n",
    "            'up'\n",
    "            if post_mean > base_mean\n",
    "            else 'down'\n",
    "            if post_mean < base_mean\n",
    "            else 'same'\n",
    "        )\n",
    "        relative_act = (base_mean - post_mean) / (base_mean + 1e-10)\n",
    "        summary['all_features'] = {\n",
    "            'baseline_mean_activation': base_mean.item(),\n",
    "            'ablated_mean_activation': post_mean.item(),\n",
    "            'relative_act': relative_act.item(),\n",
    "            'direction': direction,\n",
    "        }\n",
    "\n",
    "        # for other features that are not monitored\n",
    "        other_ids = set(range(base_sum.shape[0])) - set(monitored_ids)\n",
    "        # check if there are other features\n",
    "        if len(other_ids) > 0:\n",
    "            other_ids = sorted(list(other_ids))\n",
    "            other_base_sum = base_sum[other_ids]\n",
    "            other_post_sum = post_sum[other_ids]\n",
    "            other_base_mean = other_base_sum.mean(dim=0)\n",
    "            other_post_mean = other_post_sum.mean(dim=0)\n",
    "            other_direction = (\n",
    "                'up'\n",
    "                if other_post_mean > other_base_mean\n",
    "                else 'down'\n",
    "                if other_post_mean < other_base_mean\n",
    "                else 'same'\n",
    "            )\n",
    "            other_relative_act = (other_base_mean - other_post_mean) / (\n",
    "                other_base_mean + 1e-10\n",
    "            )\n",
    "            summary['other_features'] = {\n",
    "                'baseline_mean_activation': other_base_mean.item(),\n",
    "                'ablated_mean_activation': other_post_mean.item(),\n",
    "                'relative_act': other_relative_act.item(),\n",
    "                'direction': other_direction,\n",
    "            }\n",
    "\n",
    "        # for all monitored features\n",
    "        mnt_base_mean = base_sum[monitored_ids].mean(dim=0)\n",
    "        mnt_post_mean = post_sum[monitored_ids].mean(dim=0)\n",
    "        mnt_direction = (\n",
    "            'up'\n",
    "            if mnt_post_mean > mnt_base_mean\n",
    "            else 'down'\n",
    "            if mnt_post_mean < mnt_base_mean\n",
    "            else 'same'\n",
    "        )\n",
    "        mnt_relative_act = (mnt_base_mean - mnt_post_mean) / (\n",
    "            mnt_base_mean + 1e-10\n",
    "        )\n",
    "        summary['mnt_features'] = {\n",
    "            'baseline_mean_activation': mnt_base_mean.item(),\n",
    "            'ablated_mean_activation': mnt_post_mean.item(),\n",
    "            'relative_act': mnt_relative_act.item(),\n",
    "            'direction': mnt_direction,\n",
    "        }\n",
    "\n",
    "        # for each monitored feature\n",
    "        summary['per_mnt_feature'] = {}\n",
    "        for mon_f_id in monitored_ids:\n",
    "            base_val = base_sum[mon_f_id].item()\n",
    "            post_val = post_sum[mon_f_id].item()\n",
    "            direction = (\n",
    "                'up'\n",
    "                if post_val > base_val\n",
    "                else 'down'\n",
    "                if post_val < base_val\n",
    "                else 'same'\n",
    "            )\n",
    "            relative_act = (base_val - post_val) / (base_val + 1e-10)\n",
    "            summary['per_mnt_feature'][mon_f_id] = {\n",
    "                'baseline_sum_activation': base_val,\n",
    "                'ablated_sum_activation': post_val,\n",
    "                'relative_act': relative_act,\n",
    "                'direction': direction,\n",
    "            }\n",
    "        return summary\n",
    "\n",
    "    def monitor_abl_activations(\n",
    "        self,\n",
    "        abl_layer_idx: int,\n",
    "        monitor_layer_idx: int,\n",
    "        seed: int = 42,\n",
    "        n_gn_samples: int = 10,\n",
    "        sigma: float = 1.0,\n",
    "        batch_mode: bool = True,\n",
    "        activated_layers: Optional[list[int]] = None,\n",
    "        runtime_batch_size: int = 512,\n",
    "        ablation_feature_ids: Optional[list[int]] = None,\n",
    "        monitor_feature_ids: Optional[list[int]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Runs the full suite of experiments for a single layer and seed.\n",
    "        1. Captures baseline activations with the transcoder if not already run.\n",
    "        2. Runs zero and Gaussian noise ablations by injecting modified versions\n",
    "           of the captured activations.\n",
    "\n",
    "        Args:\n",
    "            layer_idx: The layer index to run the experiments on.\n",
    "            n_gn_samples: The number of Gaussian noise samples to run.\n",
    "            sigma: The standard deviation of the Gaussian noise.\n",
    "            batch_mode: Whether to run the ablation in batch mode.\n",
    "            activated_layers: The layer indices to replace the transcoder layers\n",
    "            for.\n",
    "            runtime_batch_size: The batch size to use for the runtime.\n",
    "            ablation_feature_ids: The feature ids to ablate. If None, will use\n",
    "                the top activation feature ids for each layer.\n",
    "        \"\"\"\n",
    "        # set the activated layers\n",
    "        activated_layers = self._set_activated_layers(\n",
    "            abl_layer_idx, activated_layers\n",
    "        )\n",
    "        ctx = self._ctx(activated_layers)\n",
    "\n",
    "        self.set_seeds(seed)\n",
    "        self.model.eval()\n",
    "\n",
    "        # --- Step 1: Run the transcoder baseline if not already run ---\n",
    "        layers_to_cap = [abl_layer_idx, monitor_layer_idx]\n",
    "        layer_ctx = '_'.join(map(str, sorted(layers_to_cap)))\n",
    "        baseline_key = f'transcoder_L{layer_ctx}_TC{ctx}_seed{seed}'\n",
    "\n",
    "        if baseline_key not in self.results:\n",
    "            self.run_transcoder_baseline(layers_to_cap, seed, activated_layers)\n",
    "\n",
    "        baseline_results = self.results[baseline_key]\n",
    "        captured_activations = baseline_results['captured_activations'][\n",
    "            abl_layer_idx\n",
    "        ]\n",
    "        top_act_ids = baseline_results['top_act_ids']\n",
    "        baseline_mnt_activations = baseline_results['captured_activations'][\n",
    "            monitor_layer_idx\n",
    "        ]\n",
    "\n",
    "        # --- Step 2: Run ablation experiments using injection ---\n",
    "        abl_hook_point = f'blocks.{abl_layer_idx}.mlp.hook_hidden_post'\n",
    "        monitor_hook_point = f'blocks.{monitor_layer_idx}.mlp.hook_hidden_post'\n",
    "        transcoders = self._transcoder_layers(activated_layers)\n",
    "\n",
    "        zero_hooks_builder = self._build_zero_hooks_builder(\n",
    "            captured_activations, abl_hook_point, batch_mode\n",
    "        )\n",
    "        gn_hooks_builder = self._build_gn_hooks_builder(\n",
    "            captured_activations, abl_hook_point, sigma, batch_mode\n",
    "        )\n",
    "        capture_config = {\n",
    "            'ablation': abl_hook_point,\n",
    "            'monitor': monitor_hook_point,\n",
    "        }\n",
    "        output_prefix = f'mnt_L{monitor_layer_idx}_abl{abl_layer_idx}'\n",
    "\n",
    "        print(\n",
    "            f'--- [Seed {seed}] Running Zero Ablation for L{abl_layer_idx} ---'\n",
    "        )\n",
    "\n",
    "        if batch_mode:\n",
    "            self._run_ablation_suite_batch(\n",
    "                ablation_feature_ids,\n",
    "                runtime_batch_size,\n",
    "                transcoders,\n",
    "                zero_hooks_builder,\n",
    "                gn_hooks_builder,\n",
    "                seed,\n",
    "                n_gn_samples,\n",
    "                abl_layer_idx,\n",
    "                ctx,\n",
    "                capture_config=capture_config,\n",
    "                output_prefix=output_prefix,\n",
    "            )\n",
    "\n",
    "        # ablate all the features in one forward pass\n",
    "        else:\n",
    "            print(\n",
    "                f'--- [Seed {seed}] Running Zero Ablation for L{abl_layer_idx} ---'\n",
    "            )\n",
    "            zero_hooks = zero_hooks_builder(ablation_feature_ids)\n",
    "\n",
    "            def _provider(sample_idx: int):\n",
    "                gn_seed = seed * 100 + sample_idx\n",
    "                return gn_hooks_builder(ablation_feature_ids, gn_seed)\n",
    "\n",
    "            gn_hooks = _provider\n",
    "            self._run_ablation_suite_single(\n",
    "                transcoders,\n",
    "                zero_hooks,\n",
    "                gn_hooks,\n",
    "                seed,\n",
    "                n_gn_samples,\n",
    "                abl_layer_idx,\n",
    "                ctx,\n",
    "                capture_config=capture_config,\n",
    "                output_prefix=output_prefix,\n",
    "            )\n",
    "\n",
    "        print('Ablation Suite Done.')\n",
    "\n",
    "        # --- Step 3: Analyze the changes in the abl / mnt features' activations ---\n",
    "        def _analyze_fea_act(baseline_act, feature_ids, type='mnt'):\n",
    "            analysis_results = {}\n",
    "            analysis_key = f'analysis_{type}_{output_prefix}TC{ctx}_seed{seed}'\n",
    "\n",
    "            base_act_full = baseline_act\n",
    "            base_act_sliced = base_act_full\n",
    "\n",
    "            if feature_ids is not None:\n",
    "                fea_tensor = torch.tensor(\n",
    "                    feature_ids, device=base_act_full.device\n",
    "                )\n",
    "                base_act_sliced = base_act_sliced.index_select(-1, fea_tensor)\n",
    "\n",
    "            # zero ablation analysis\n",
    "            zero_results_key = f'{output_prefix}zero_abl_L{abl_layer_idx}_TC{ctx}_seed{seed}_each_feature'\n",
    "            if zero_results_key in self.results:\n",
    "                for abl_fid, data in self.results[zero_results_key].items():\n",
    "                    if type == 'mnt':\n",
    "                        abl_act_full = data['captured_data']['monitor']\n",
    "                    elif type == 'abl':\n",
    "                        abl_act_full = data['captured_data']['ablation']\n",
    "                    else:\n",
    "                        raise ValueError(f'Invalid type: {type}')\n",
    "                    abl_act_sliced = abl_act_full\n",
    "                    if feature_ids is not None:\n",
    "                        abl_act_sliced = abl_act_full.index_select(\n",
    "                            -1, fea_tensor\n",
    "                        )\n",
    "\n",
    "                    delta = abl_act_sliced - base_act_sliced\n",
    "                    fea_summary = self.get_fea_act_summary(\n",
    "                        base_act_full, abl_act_full, feature_ids\n",
    "                    )\n",
    "                    # cos sim\n",
    "                    cos_sim = F.cosine_similarity(\n",
    "                        base_act_sliced.flatten(),\n",
    "                        abl_act_sliced.flatten(),\n",
    "                        dim=0,\n",
    "                    ).item()\n",
    "                    analysis_results.setdefault(abl_fid, {}).update(\n",
    "                        {\n",
    "                            'baseline_activation': base_act_sliced.cpu(),\n",
    "                            'zero_ablated_activation': abl_act_sliced.cpu(),\n",
    "                            'zero_delta': delta.cpu(),\n",
    "                            'zero_delta_norm': torch.norm(delta, p=2).item(),\n",
    "                            'zero_cos_sim': cos_sim,\n",
    "                            'zero_fea_summary': fea_summary,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            # gn ablation analysis\n",
    "            gn_results_key = f'{output_prefix}gn_abl_L{abl_layer_idx}_TC{ctx}_seed{seed}_each_feature'\n",
    "            if gn_results_key in self.results and n_gn_samples > 0:\n",
    "                gn_results_per_sample = self.results[gn_results_key]\n",
    "                agg_gn_results = {}\n",
    "                # aggregate the gn results per feature\n",
    "                for sample_res in gn_results_per_sample:\n",
    "                    for abl_fid, data in sample_res.items():\n",
    "                        if type == 'mnt':\n",
    "                            abl_act_full = data['captured_data']['monitor']\n",
    "                        elif type == 'abl':\n",
    "                            abl_act_full = data['captured_data']['ablation']\n",
    "                        else:\n",
    "                            raise ValueError(f'Invalid type: {type}')\n",
    "                        agg_gn_results.setdefault(abl_fid, []).append(\n",
    "                            abl_act_full\n",
    "                        )\n",
    "\n",
    "                for abl_fid, act_list in agg_gn_results.items():\n",
    "                    abl_act_full = torch.stack(act_list).mean(dim=0)\n",
    "                    abl_act_sliced = abl_act_full\n",
    "                    if feature_ids is not None:\n",
    "                        abl_act_sliced = abl_act_full.index_select(\n",
    "                            -1, fea_tensor\n",
    "                        )\n",
    "\n",
    "                    delta = abl_act_sliced - base_act_sliced\n",
    "                    fea_summary = self.get_fea_act_summary(\n",
    "                        base_act_full, abl_act_full, feature_ids\n",
    "                    )\n",
    "                    cos_sim = F.cosine_similarity(\n",
    "                        base_act_sliced.flatten(),\n",
    "                        abl_act_sliced.flatten(),\n",
    "                        dim=0,\n",
    "                    ).item()\n",
    "                    analysis_results.setdefault(abl_fid, {}).update(\n",
    "                        {\n",
    "                            'gn_mean_ablated_activation': abl_act_sliced.cpu(),\n",
    "                            'gn_delta': delta.cpu(),\n",
    "                            'gn_delta_norm': torch.norm(delta, p=2).item(),\n",
    "                            'gn_cos_sim': cos_sim,\n",
    "                            'gn_fea_summary': fea_summary,\n",
    "                        }\n",
    "                    )\n",
    "                self.results[analysis_key] = analysis_results\n",
    "\n",
    "        print(\n",
    "            f\"--- [Seed {seed}] Analyzing the changes in the mnt features' activations ---\"\n",
    "        )\n",
    "        _analyze_fea_act(\n",
    "            baseline_mnt_activations, monitor_feature_ids, type='mnt'\n",
    "        )\n",
    "        print(\n",
    "            f\"--- [Seed {seed}] Analyzing the changes in the abl features' activations ---\"\n",
    "        )\n",
    "        _analyze_fea_act(captured_activations, ablation_feature_ids, type='abl')\n",
    "\n",
    "        print('Analysis Done.')\n",
    "\n",
    "    def analyze_feature_impact(\n",
    "        self,\n",
    "        layer_idx,\n",
    "        seed,\n",
    "        top_k_features,\n",
    "        top_k_words,\n",
    "        activated_layers: Optional[list[int]] = None,\n",
    "    ):\n",
    "        \"\"\"Find the impact of the top k features and corresponding top k words.\"\"\"\n",
    "        # set the activated layers\n",
    "        activated_layers = self._set_activated_layers(\n",
    "            layer_idx, activated_layers\n",
    "        )\n",
    "        ctx = self._ctx(activated_layers)\n",
    "\n",
    "        transcoder_baseline_results = self.results[\n",
    "            f'transcoder_L{layer_idx}_TC{ctx}_seed{seed}'\n",
    "        ]\n",
    "        zero_ablation_results = self.results[\n",
    "            f'zero_abl_L{layer_idx}_TC{ctx}_seed{seed}_each_feature'\n",
    "        ]\n",
    "        gn_ablation_results = self.results[\n",
    "            f'gn_abl_L{layer_idx}_TC{ctx}_seed{seed}_each_feature'\n",
    "        ]\n",
    "\n",
    "        base_logits = transcoder_baseline_results['logits']\n",
    "\n",
    "        abl_feat_ids = sorted(list(zero_ablation_results.keys()))\n",
    "        # stack the result of each feature to run in batch\n",
    "        try:\n",
    "            abl_feat_logits = torch.stack(\n",
    "                [\n",
    "                    zero_ablation_results[feat_id]['logits']\n",
    "                    for feat_id in abl_feat_ids\n",
    "                ]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f'Error stacking logits: {e}')\n",
    "            raise e\n",
    "\n",
    "        # base_logits will broadcast automatically and the shape will be\n",
    "        # (batch_size, num_text_features)\n",
    "        logit_diff = abl_feat_logits - base_logits\n",
    "\n",
    "        impact_scores = torch.norm(logit_diff, p=2, dim=-1)\n",
    "        sort_idx = torch.argsort(impact_scores, descending=True)\n",
    "        top_idx = sort_idx[:top_k_features]\n",
    "\n",
    "        print(f'Top {top_k_features} features with highest impact:')\n",
    "        # for each top feature, find the largest change in text probabilities\n",
    "        for f_idx in top_idx:\n",
    "            feat_id = abl_feat_ids[f_idx]\n",
    "            feat_logit_diff = logit_diff[f_idx]\n",
    "            impact_score = impact_scores[f_idx]\n",
    "            # top logit diff\n",
    "            top_neg_logit_diff, top_neg_idx = torch.topk(\n",
    "                feat_logit_diff, k=top_k_words, largest=False\n",
    "            )\n",
    "            top_pos_logit_diff, top_pos_idx = torch.topk(\n",
    "                feat_logit_diff, k=top_k_words, largest=True\n",
    "            )\n",
    "            top_neg_words_logit_diff = [self.labels[idx] for idx in top_neg_idx]\n",
    "            top_pos_words_logit_diff = [self.labels[idx] for idx in top_pos_idx]\n",
    "\n",
    "            neg_results = {\n",
    "                word: f'{diff.item():.4f}'\n",
    "                for word, diff in zip(\n",
    "                    top_neg_words_logit_diff, top_neg_logit_diff\n",
    "                )\n",
    "            }\n",
    "            pos_results = {\n",
    "                word: f'{diff.item():.4f}'\n",
    "                for word, diff in zip(\n",
    "                    top_pos_words_logit_diff, top_pos_logit_diff\n",
    "                )\n",
    "            }\n",
    "\n",
    "            print(\n",
    "                f'Feature {feat_id} has the largest change in text '\n",
    "                f'probabilities: {impact_score}'\n",
    "            )\n",
    "            print(f'Top {top_k_words} negative words: {neg_results}')\n",
    "            print(f'Top {top_k_words} positive words: {pos_results}')\n",
    "\n",
    "    def analyze_single_concept(\n",
    "        self,\n",
    "        layer_idx,\n",
    "        seed,\n",
    "        concept_name,\n",
    "        top_k_features=20,\n",
    "        mode='zero',\n",
    "        activated_layers: Optional[list[int]] = None,\n",
    "    ):\n",
    "        \"\"\"Analyze the impact of a single concept on the model.\"\"\"\n",
    "        # set the activated layers\n",
    "        activated_layers = self._set_activated_layers(\n",
    "            layer_idx, activated_layers\n",
    "        )\n",
    "        ctx = self._ctx(activated_layers)\n",
    "\n",
    "        transcoder_baseline_results = self.results[\n",
    "            f'transcoder_L{layer_idx}_TC{ctx}_seed{seed}'\n",
    "        ]\n",
    "        if mode == 'zero':\n",
    "            abl_results = self.results[\n",
    "                f'zero_abl_L{layer_idx}_TC{ctx}_seed{seed}_each_feature'\n",
    "            ]\n",
    "        elif mode == 'gn':\n",
    "            # need to get the mean of the gn samples\n",
    "            gn_results = self.results[\n",
    "                f'gn_abl_L{layer_idx}_TC{ctx}_seed{seed}_each_feature'\n",
    "            ]\n",
    "            abl_results = {}\n",
    "            for i in range(len(gn_results)):\n",
    "                for feat_id in gn_results[i].keys():\n",
    "                    if feat_id not in abl_results:\n",
    "                        abl_results[feat_id] = {\n",
    "                            'logits': gn_results[i][feat_id]['logits']\n",
    "                            .clone()\n",
    "                            .detach()\n",
    "                        }\n",
    "                    else:\n",
    "                        abl_results[feat_id]['logits'] += (\n",
    "                            gn_results[i][feat_id]['logits'].clone().detach()\n",
    "                        )\n",
    "            for feat_id in abl_results.keys():\n",
    "                abl_results[feat_id]['logits'] /= len(gn_results)\n",
    "        else:\n",
    "            raise ValueError(f'Invalid mode: {mode}')\n",
    "\n",
    "        base_logits = transcoder_baseline_results['logits']\n",
    "\n",
    "        abl_feat_ids = sorted(list(abl_results.keys()))\n",
    "        # stack the result of each feature to run in batch\n",
    "        try:\n",
    "            abl_feat_logits = torch.stack(\n",
    "                [abl_results[feat_id]['logits'] for feat_id in abl_feat_ids]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f'Error stacking logits: {e}')\n",
    "            raise e\n",
    "\n",
    "        # get the logit for the concept\n",
    "        try:\n",
    "            concept_id = self.labels.index(concept_name)\n",
    "        except ValueError:\n",
    "            raise ValueError(f'Concept {concept_name} not found in the labels')\n",
    "\n",
    "        base_concept_logits = base_logits[concept_id]  # scalar\n",
    "        concept_logits = abl_feat_logits[:, concept_id]\n",
    "        logit_diff = (\n",
    "            base_concept_logits - concept_logits\n",
    "        )  # > 0 means the concept is suppressed\n",
    "\n",
    "        # find which feature has the largest change in logit for the concept\n",
    "        if top_k_features == -1:\n",
    "            k = logit_diff.numel()\n",
    "        else:\n",
    "            k = min(top_k_features, logit_diff.numel())\n",
    "\n",
    "        top_k_logit_diff, top_k_feat_idx = torch.topk(\n",
    "            logit_diff, k=k, largest=True\n",
    "        )\n",
    "        top_k_feat_idx = top_k_feat_idx.tolist()\n",
    "        top_k_feat_ids = [abl_feat_ids[idx] for idx in top_k_feat_idx]\n",
    "\n",
    "        print(f'Base logit for the concept: {base_concept_logits.item()}')\n",
    "        print(\n",
    "            f'Top {top_k_features} features with largest logit difference for {concept_name} in {mode} ablation:'\n",
    "        )\n",
    "        for feat_id, logit_diff in zip(top_k_feat_ids, top_k_logit_diff):\n",
    "            feat_id = int(feat_id)\n",
    "            print(\n",
    "                f'Feature {feat_id} has the largest logit difference: {logit_diff}'\n",
    "            )\n",
    "            print(\n",
    "                f'Logit for the concept: {abl_results[feat_id][\"logits\"][concept_id].item()}'\n",
    "            )\n",
    "\n",
    "        return top_k_logit_diff, top_k_feat_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe65fea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\tTask(Task-2) matplotlib.image:image.py:_normalize_image_array()- Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.7922626..2.145897].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_path = (\n",
    "    '/nfs/turbo/coe-chaijy/janeding/regrounding/clip_tl/test_img/flask.png'\n",
    ")\n",
    "# gt_label = 'chemistry'\n",
    "img = Image.open(img_path).convert()  # Ensure it's 3 channels\n",
    "transforms = get_clip_val_transforms()\n",
    "img_tensor = transforms(img)\n",
    "plot_image(img_tensor.detach().cpu(), unstandardise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600aab4d",
   "metadata": {},
   "source": [
    "# Ablation for TC on layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93efeb0c",
   "metadata": {},
   "source": [
    "results = {}\n",
    "FIX_L = 1\n",
    "for i in range(FIX_L + 1, 11):\n",
    "    runner = AblationExperimentRunner(\n",
    "        model=hookedsaevit,\n",
    "        transcoders=tc_list,\n",
    "        img_tensor=img_tensor,\n",
    "        text_features=text_features,\n",
    "        labels=final_labels,\n",
    "        device='cuda',\n",
    "        activated_layers=[FIX_L, i],\n",
    "    )\n",
    "    runner.run_transcoder_baseline(layer_idx=FIX_L, seed=42)\n",
    "    results[i] = runner.results[f'transcoder_L{FIX_L}_TC{FIX_L}_{i}_seed42'][\n",
    "        'text_probs'\n",
    "    ]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61433d0",
   "metadata": {},
   "source": [
    "## Try L1 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85d44778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing the transcoder layers for L1_5.\n",
      "--- [Seed 42] Running Transcoder Baseline for L1 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chemistry': 0.32059502601623535,\n",
       " 'titration': 0.1483946144580841,\n",
       " 'experimentation': 0.1313956379890442,\n",
       " 'science': 0.07199804484844208,\n",
       " 'science equipment': 0.047188661992549896,\n",
       " 'technology': 0.0370088592171669,\n",
       " 'research tools': 0.02706180326640606,\n",
       " 'graphic': 0.02584078349173069,\n",
       " 'experiment': 0.023069527000188828,\n",
       " 'chemical': 0.019389508292078972,\n",
       " 'test tube': 0.018862372264266014,\n",
       " 'electrostatics': 0.018215486779808998,\n",
       " 'solution': 0.017842872068285942,\n",
       " 'ethicality': 0.007043260149657726,\n",
       " 'reagent': 0.0059230271726846695,\n",
       " 'research': 0.004928987938910723,\n",
       " 'testing': 0.004735670052468777,\n",
       " 'laboratory bottle': 0.00420342106372118,\n",
       " 'laboratory': 0.004067030269652605,\n",
       " 'hardware': 0.003388571087270975}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner_L1_5 = AblationExperimentRunner(\n",
    "    model=hookedsaevit,\n",
    "    transcoders=tc_list,\n",
    "    img_tensor=img_tensor,\n",
    "    text_features=text_features,\n",
    "    labels=final_labels,\n",
    "    device='cuda',\n",
    "    activated_layers=[1, 5],\n",
    ")\n",
    "runner_L1_5.run_transcoder_baseline(layer_idx=1, seed=42)\n",
    "runner_L1_5.results['transcoder_L1_TC1_5_seed42']['text_probs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c2b9c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top activation features number: 2639\n",
      "--- [Seed 42] Running Zero Ablation for L1 ---\n",
      "Running zero ablation for chunk 1 of 3...\n",
      "Zero Ablation Done.\n",
      "--- [Seed 42] Running Gaussian Noise Ablation for L1 (10 samples) ---\n",
      "GN Ablation Done.\n",
      "Running zero ablation for chunk 2 of 3...\n",
      "Zero Ablation Done.\n",
      "--- [Seed 42] Running Gaussian Noise Ablation for L1 (10 samples) ---\n",
      "GN Ablation Done.\n",
      "Running zero ablation for chunk 3 of 3...\n",
      "Zero Ablation Done.\n",
      "--- [Seed 42] Running Gaussian Noise Ablation for L1 (10 samples) ---\n",
      "GN Ablation Done.\n",
      "Ablation Suite Done.\n"
     ]
    }
   ],
   "source": [
    "runner_L1_5.run_ablation_suite(\n",
    "    layer_idx=1,\n",
    "    seed=42,\n",
    "    n_gn_samples=10,\n",
    "    sigma=2.0,\n",
    "    batch_mode=True,\n",
    "    runtime_batch_size=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "340808a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['transcoder_L1_TC1_5_seed42', 'zero_abl_L1_TC1_5_seed42_each_feature', 'gn_abl_L1_TC1_5_seed42_each_feature'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner_L1_5.results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71a63f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base logit for the concept: 20.085433959960938\n",
      "Top 10 features with largest logit difference for science equipment in gn ablation:\n",
      "Feature 48184 has the largest logit difference: 2.0168895721435547\n",
      "Logit for the concept: 18.068544387817383\n",
      "Feature 44755 has the largest logit difference: 1.532297134399414\n",
      "Logit for the concept: 18.553136825561523\n",
      "Feature 13262 has the largest logit difference: 1.130392074584961\n",
      "Logit for the concept: 18.955041885375977\n",
      "Feature 5682 has the largest logit difference: 1.043527603149414\n",
      "Logit for the concept: 19.041906356811523\n",
      "Feature 29628 has the largest logit difference: 1.023386001586914\n",
      "Logit for the concept: 19.062047958374023\n",
      "Feature 33425 has the largest logit difference: 0.9995594024658203\n",
      "Logit for the concept: 19.085874557495117\n",
      "Feature 13989 has the largest logit difference: 0.9304656982421875\n",
      "Logit for the concept: 19.15496826171875\n",
      "Feature 48397 has the largest logit difference: 0.9158267974853516\n",
      "Logit for the concept: 19.169607162475586\n",
      "Feature 7317 has the largest logit difference: 0.8500843048095703\n",
      "Logit for the concept: 19.235349655151367\n",
      "Feature 1645 has the largest logit difference: 0.7532939910888672\n",
      "Logit for the concept: 19.33213996887207\n",
      "Top activation features number: 2639\n",
      "--- [Seed 42] Running Zero Ablation for L1 ---\n",
      "--- [Seed 42] Running Zero Ablation for L1 ---\n",
      "Zero Ablation Done.\n",
      "--- [Seed 42] Running Gaussian Noise Ablation for L1 (10 samples) ---\n",
      "GN Ablation Done.\n",
      "Ablation Suite Done.\n"
     ]
    }
   ],
   "source": [
    "# find the top 20 logit difference for 100\n",
    "# runner.analyze_feature_impact(1, 42, 1, 20)\n",
    "shallow_logit_diff, shallow_feat_ids = runner_L1_5.analyze_single_concept(\n",
    "    layer_idx=1,\n",
    "    seed=42,\n",
    "    concept_name='science equipment',\n",
    "    top_k_features=10,\n",
    "    mode='gn',\n",
    ")\n",
    "\n",
    "# ablate the top k features to see if the concept is still there\n",
    "runner_L1_5.run_ablation_suite(\n",
    "    layer_idx=1,\n",
    "    ablation_feature_ids=shallow_feat_ids,\n",
    "    seed=42,\n",
    "    n_gn_samples=10,\n",
    "    sigma=2.0,\n",
    "    batch_mode=False,\n",
    "    runtime_batch_size=1024,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f47a8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['transcoder_L1_TC1_5_seed42', 'zero_abl_L1_TC1_5_seed42_each_feature', 'gn_abl_L1_TC1_5_seed42_each_feature', 'zero_abl_L1_TC1_5_seed42_all_features', 'gn_abl_L1_TC1_5_seed42_all_features'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner_L1_5.results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b44c93c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'graphic': 0.2705092430114746,\n",
       "  'technology': 0.08460656553506851,\n",
       "  'experimentation': 0.07529199123382568,\n",
       "  'misappropriation': 0.0370749831199646,\n",
       "  'uptempo': 0.028375782072544098,\n",
       "  'inconstantly': 0.02692423202097416,\n",
       "  'sailed': 0.02678210288286209,\n",
       "  'funk': 0.02480977401137352,\n",
       "  'disorganize': 0.023675141856074333,\n",
       "  'solution': 0.023593997582793236,\n",
       "  'electrostatics': 0.019907059147953987,\n",
       "  'ethicality': 0.01783432997763157,\n",
       "  'determinable': 0.01318801287561655,\n",
       "  'regenerate': 0.012460512109100819,\n",
       "  'candidness': 0.012133711948990822,\n",
       "  'secret': 0.011565906926989555,\n",
       "  'hardware': 0.011233455501496792,\n",
       "  'antisocial': 0.011219858191907406,\n",
       "  'world': 0.010434068739414215,\n",
       "  'funneled': 0.010227318853139877},\n",
       " tensor([17.1520,  7.9416, 11.0976, 12.8049, 15.4398, 10.7742, 16.1657, 16.0722,\n",
       "         18.3749, 10.9389, 14.4008, 11.5758, 15.3599, 13.8779, 13.6183, 10.8560,\n",
       "         16.5774, 14.6575, 12.6678, 14.1437, 17.0960, 12.9325, 12.5892, 18.3280,\n",
       "         10.4524, 14.8932, 18.7766, 12.3714, 13.3147, 14.8654, 15.4534, 10.8013,\n",
       "         13.8235, 12.4039, 14.3574, 12.1828, 13.4363, 14.1990, 15.1674, 10.3781,\n",
       "         17.0149, 15.0426, 14.6360, 14.7412, 15.2009,  6.6726, 13.9204, 13.4627,\n",
       "         15.4551, 12.8895, 16.9842, 13.9281, 14.4154, 15.9160, 16.8447, 18.4513,\n",
       "         14.4905, 16.4940, 15.2298, 11.1218, 13.8370, 10.1192, 15.3677, 10.3024,\n",
       "          6.2857, 17.0504, 17.5087, 12.2244,  9.0532, 14.5774, 14.1456, 18.4566,\n",
       "         13.2266, 11.7502, 12.6078, 13.0702, 10.5582, 12.6206, 17.4875, 16.4287,\n",
       "          5.7326, 13.4065, 10.1623, 14.8730, 10.4299, 16.8945,  9.6631, 14.5290,\n",
       "         17.6862, 12.4626, 17.6596, 12.7359, 12.3621, 11.6872, 12.8349, 11.2722,\n",
       "         14.2142, 15.9077, 14.9761, 16.7284, 12.9205, 12.3465, 16.5357, 13.1152,\n",
       "         11.4058, 16.8481,  8.8073, 10.4181, 15.3918, 10.1527, 12.5171, 10.6262,\n",
       "         14.5006, 11.6543, 15.4756, 14.8860, 19.4850, 13.4374, 13.0023, 17.5813,\n",
       "         16.6843, 13.7728, 10.9520, 10.1108, 15.5010, 15.7572, 10.7655,  8.4633,\n",
       "          8.7372, 12.0753, 13.3940, 15.1882, 12.2264, 16.4622, 18.5091, 12.0184,\n",
       "          5.7730, 20.7639,  8.0442, 12.8148, 14.1523, 12.8236, 11.0311, 16.0539,\n",
       "         15.6886, 14.3212, 14.7247, 14.4164, 16.0173, 13.5014, 12.7298, 12.0420,\n",
       "         18.1547,  6.7895, 14.1346, 11.2977, 17.4073,  7.5333, 15.4212, 12.8897,\n",
       "         15.9675, 17.4218, 10.5349, 12.3746, 15.7281, 12.9300, 16.0098, 12.7204,\n",
       "         13.2307, 15.4282, 14.0374, 10.2565, 13.0713, 18.3246, 15.7670, 17.3650,\n",
       "         16.5710, 16.7254, 16.4066, 16.1497, 15.4635, 14.4100, 14.5830, 15.6028,\n",
       "         16.0960, 13.7554, 12.4674, 15.5468, 15.0590, 13.2560, 14.6323, 15.4727,\n",
       "         15.0022, 13.2588, 15.9547, 19.6016, 17.3276, 14.2014, 17.6117, 14.4598,\n",
       "          9.3840,  7.5933, 15.4323, 11.6804, 15.2119, 15.3269, 12.9116, 17.7429,\n",
       "         13.6111, 12.2611, 14.0741, 14.1358, 16.8285, 14.1248, 14.7031, 12.8927,\n",
       "         10.6302, 12.7172, 15.9139, 15.2692, 17.5825, 10.3589,  9.9397, 11.7173,\n",
       "         13.8261, 10.5998, 16.4848, 13.3861, 17.4887,  7.5883, 12.9483, 11.6044,\n",
       "         13.2071,  8.4138, 16.6123, 12.3828, 13.3668, 14.2503,  7.8374, 11.6139,\n",
       "         15.9359,  3.8209, 14.8100, 14.9956,  4.3435, 18.0447, 15.4745, 12.9626,\n",
       "         11.5087], device='cuda:0'))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner_L1_5.agg_gn_results('gn_abl_L1_TC1_5_seed42_all_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c23feddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top activation features number: 2409\n",
      "--- [Seed 42] Running Zero Ablation for L5 ---\n",
      "Running zero ablation for chunk 1 of 3...\n",
      "Zero Ablation Done.\n",
      "--- [Seed 42] Running Gaussian Noise Ablation for L5 (10 samples) ---\n",
      "GN Ablation Done.\n",
      "Running zero ablation for chunk 2 of 3...\n",
      "Zero Ablation Done.\n",
      "--- [Seed 42] Running Gaussian Noise Ablation for L5 (10 samples) ---\n",
      "GN Ablation Done.\n",
      "Running zero ablation for chunk 3 of 3...\n",
      "Zero Ablation Done.\n",
      "--- [Seed 42] Running Gaussian Noise Ablation for L5 (10 samples) ---\n",
      "GN Ablation Done.\n",
      "Ablation Suite Done.\n",
      "Base logit for the concept: 22.0014591217041\n",
      "Top 10 features with largest logit difference for chemistry in gn ablation:\n",
      "Feature 19105 has the largest logit difference: 0.243988037109375\n",
      "Logit for the concept: 21.757471084594727\n",
      "Feature 7581 has the largest logit difference: 0.171875\n",
      "Logit for the concept: 21.8295841217041\n",
      "Feature 606 has the largest logit difference: 0.16771888732910156\n",
      "Logit for the concept: 21.833740234375\n",
      "Feature 39754 has the largest logit difference: 0.1658611297607422\n",
      "Logit for the concept: 21.83559799194336\n",
      "Feature 9588 has the largest logit difference: 0.14691162109375\n",
      "Logit for the concept: 21.85454750061035\n",
      "Feature 8851 has the largest logit difference: 0.14615249633789062\n",
      "Logit for the concept: 21.85530662536621\n",
      "Feature 1617 has the largest logit difference: 0.14583396911621094\n",
      "Logit for the concept: 21.85562515258789\n",
      "Feature 22460 has the largest logit difference: 0.14501190185546875\n",
      "Logit for the concept: 21.856447219848633\n",
      "Feature 23011 has the largest logit difference: 0.143341064453125\n",
      "Logit for the concept: 21.858118057250977\n",
      "Feature 19619 has the largest logit difference: 0.14168548583984375\n",
      "Logit for the concept: 21.859773635864258\n"
     ]
    }
   ],
   "source": [
    "# find the chemistry concept on layer 5\n",
    "runner_L1_5.run_ablation_suite(\n",
    "    layer_idx=5,\n",
    "    seed=42,\n",
    "    n_gn_samples=10,\n",
    "    sigma=2.0,\n",
    "    batch_mode=True,\n",
    "    runtime_batch_size=1024,\n",
    ")\n",
    "\n",
    "deep_logit_diff, deep_feat_ids = runner_L1_5.analyze_single_concept(\n",
    "    layer_idx=5,\n",
    "    seed=42,\n",
    "    concept_name='chemistry',\n",
    "    top_k_features=10,\n",
    "    mode='gn',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8c1153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top activation features number: 2409\n",
      "--- [Seed 42] Running Zero Ablation for L5 ---\n",
      "--- [Seed 42] Running Zero Ablation for L5 ---\n",
      "Zero Ablation Done.\n",
      "--- [Seed 42] Running Gaussian Noise Ablation for L5 (10 samples) ---\n",
      "GN Ablation Done.\n",
      "Ablation Suite Done.\n"
     ]
    }
   ],
   "source": [
    "# ablate the top k features to see if the concept is still there\n",
    "runner_L1_5.run_ablation_suite(\n",
    "    layer_idx=5,\n",
    "    ablation_feature_ids=deep_feat_ids,\n",
    "    seed=42,\n",
    "    n_gn_samples=10,\n",
    "    sigma=2.0,\n",
    "    batch_mode=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e5c1414d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'chemistry': 0.2532220184803009,\n",
       "  'experimentation': 0.20039917528629303,\n",
       "  'titration': 0.0791768729686737,\n",
       "  'science': 0.07186168432235718,\n",
       "  'technology': 0.0633515790104866,\n",
       "  'graphic': 0.04899157956242561,\n",
       "  'science equipment': 0.04598945751786232,\n",
       "  'electrostatics': 0.025219760835170746,\n",
       "  'experiment': 0.023979321122169495,\n",
       "  'research tools': 0.02183775044977665,\n",
       "  'solution': 0.02153300680220127,\n",
       "  'chemical': 0.017357058823108673,\n",
       "  'test tube': 0.011825112625956535,\n",
       "  'ethicality': 0.008844159543514252,\n",
       "  'testing': 0.005384991876780987,\n",
       "  'hardware': 0.00511134834960103,\n",
       "  'reagent': 0.004384201020002365,\n",
       "  'acid': 0.0042983125895261765,\n",
       "  'research': 0.004141317214816809,\n",
       "  'uptempo': 0.0036199218593537807},\n",
       " tensor([12.7087,  4.7725, 18.2655, 12.0577, 13.8909, 16.9477, 12.5032, 18.9725,\n",
       "         16.2016, 13.2847, 13.3014, 15.1545, 17.4789, 15.6734, 11.2597, 11.0368,\n",
       "         14.5132, 13.7428, 12.5279, 12.4446, 17.2535,  8.0529, 11.2422, 14.5855,\n",
       "         15.9467, 15.5547, 16.0304, 14.3568, 10.9808, 12.7454, 12.7873, 16.1080,\n",
       "         13.4693,  9.9027, 11.7918, 12.5608, 13.7295, 12.6027, 12.1997,  9.3441,\n",
       "         20.0700, 13.8013, 16.6198, 13.0937, 16.0030, 11.7818, 14.3338, 12.8861,\n",
       "         15.0063, 12.8307, 14.0431, 11.4888, 14.4573, 13.3114, 15.5290, 15.9352,\n",
       "         13.9188, 21.3295, 12.9446, 10.5007, 13.9222,  9.3308, 12.9625,  9.5677,\n",
       "          1.6891, 15.2851, 12.8344, 12.2339,  9.6222, 12.9502, 11.6547, 14.9177,\n",
       "         11.9531, 10.1713, 13.5864, 11.5522, 12.6114, 17.0630, 15.9009, 13.7336,\n",
       "          6.6112, 11.3292,  8.0542, 14.0345, 10.3680, 16.0734,  8.5893, 16.9018,\n",
       "         16.4787, 11.2974, 15.8599, 10.4548, 10.9705,  9.3954, 12.7131, 11.0864,\n",
       "         14.5182, 16.9337, 12.4206, 15.4733, 11.9869,  8.1160, 14.3865, 11.7219,\n",
       "         10.3601, 14.8176,  9.7633, 11.7120, 13.5825,  8.8328, 13.3256, 12.2621,\n",
       "         14.6734, 10.4870, 15.6842, 15.0782, 21.0956, 11.4736, 13.2969, 16.0602,\n",
       "         16.0245, 15.2842, 14.4645, 11.8215, 14.9173, 11.7689,  6.5750,  4.9615,\n",
       "          7.6852, 11.8505, 14.6336, 13.4043, 10.1750, 15.2590, 17.0817, 10.4854,\n",
       "          1.9082, 19.6869,  8.6199, 10.0032, 11.5424, 12.2307,  8.7898, 15.4897,\n",
       "         18.6493, 15.2011, 13.0281, 11.1915, 15.4530, 15.2404,  8.9416, 11.7917,\n",
       "         19.0229,  4.8582,  9.9050, 11.1794, 16.3585,  5.3977, 14.5880,  9.9253,\n",
       "         14.2772, 14.8675,  7.4572, 12.7394, 13.2052, 16.7253, 14.8930, 16.6988,\n",
       "         12.7823, 13.6202, 15.8564, 13.5205, 11.3564, 18.8648, 14.9000, 14.1860,\n",
       "         12.0349, 15.1669, 12.5358, 13.6176, 15.6726, 14.6271, 13.3604, 18.8789,\n",
       "         15.3479, 12.0651, 10.3484, 12.9870, 13.2909, 10.1053, 19.6237, 17.2163,\n",
       "         13.0108,  8.4659, 15.3771, 19.9440, 16.5229, 12.7526, 15.2799, 10.7900,\n",
       "          9.8948,  4.3098, 12.1892, 10.1095, 14.6440, 14.2186, 20.1669, 16.1040,\n",
       "         17.2733, 11.1006, 15.4935, 15.1128, 13.4217, 10.4877, 11.9547,  9.9850,\n",
       "          7.4954, 12.8902, 16.8564, 14.3496, 17.4267, 10.1605,  8.9422, 10.8496,\n",
       "          8.9508, 15.5835, 16.2622, 12.3878, 16.2440,  5.6783, 11.2925, 10.1474,\n",
       "         10.5334,  3.8761, 13.8422, 10.1405, 15.2254, 11.5229,  8.2674, 11.5407,\n",
       "         13.6299,  0.3660, 11.2382, 16.4016,  1.0084, 17.9750, 16.1869, 12.9480,\n",
       "         11.8211], device='cuda:0'))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner_L1_5.agg_gn_results('gn_abl_L5_TC1_5_seed42_all_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ca29eaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Seed 42] Running Zero Ablation for L1 ---\n",
      "--- [Seed 42] Running Zero Ablation for L1 ---\n",
      "Zero Ablation Done.\n",
      "--- [Seed 42] Running Gaussian Noise Ablation for L1 (10 samples) ---\n",
      "GN Ablation Done.\n",
      "Ablation Suite Done.\n",
      "--- [Seed 42] Analyzing the changes in the mnt features' activations ---\n",
      "--- [Seed 42] Analyzing the changes in the abl features' activations ---\n",
      "Analysis Done.\n"
     ]
    }
   ],
   "source": [
    "# ablate the shallow layer feature and see the change of the act of deep layer\n",
    "runner_L1_5.monitor_abl_activations(\n",
    "    1,\n",
    "    5,\n",
    "    42,\n",
    "    10,\n",
    "    batch_mode=False,\n",
    "    runtime_batch_size=1024,\n",
    "    ablation_feature_ids=shallow_feat_ids,\n",
    "    monitor_feature_ids=deep_feat_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1ca70509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['transcoder_L1_TC1_5_seed42', 'zero_abl_L1_TC1_5_seed42_each_feature', 'gn_abl_L1_TC1_5_seed42_each_feature', 'zero_abl_L1_TC1_5_seed42_all_features', 'gn_abl_L1_TC1_5_seed42_all_features', 'transcoder_L5_TC1_5_seed42', 'zero_abl_L5_TC1_5_seed42_each_feature', 'gn_abl_L5_TC1_5_seed42_each_feature', 'zero_abl_L5_TC1_5_seed42_all_features', 'gn_abl_L5_TC1_5_seed42_all_features', 'transcoder_L1_5_TC1_5_seed42', 'mnt_L5_abl1zero_abl_L1_TC1_5_seed42_each_feature', 'mnt_L5_abl1gn_abl_L1_TC1_5_seed42_each_feature', 'analysis_mnt_mnt_L5_abl1TC1_5_seed42', 'analysis_abl_mnt_L5_abl1TC1_5_seed42', 'mnt_L5_abl1zero_abl_L1_TC1_5_seed42_all_features', 'mnt_L5_abl1gn_abl_L1_TC1_5_seed42_all_features'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner_L1_5.results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1e4cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abl Feature ID: 48184\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.25020647048950195,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.07353560626506805},\n",
      " 'mnt_features': {'ablated_mean_activation': 14.250128746032715,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.24940413236618042},\n",
      " 'other_features': {'ablated_mean_activation': 0.2473575919866562,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'down',\n",
      "                    'relative_act': 0.07098386436700821},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 16.361976623535156,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'down',\n",
      "                           'relative_act': 0.33990154681990753},\n",
      "                     1617: {'ablated_sum_activation': 8.702807426452637,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.142355359691125},\n",
      "                     7581: {'ablated_sum_activation': 6.547603130340576,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.24002120385639927},\n",
      "                     8851: {'ablated_sum_activation': 11.012500762939453,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.094442764936285},\n",
      "                     9588: {'ablated_sum_activation': 18.01408576965332,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.09399886190656585},\n",
      "                     19105: {'ablated_sum_activation': 1.2937999963760376,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.8866836093676126},\n",
      "                     19619: {'ablated_sum_activation': 26.464576721191406,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.18509939395092617},\n",
      "                     22460: {'ablated_sum_activation': 5.874489784240723,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.2933125421363254},\n",
      "                     23011: {'ablated_sum_activation': 12.410551071166992,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.5877033680836531},\n",
      "                     39754: {'ablated_sum_activation': 35.81889724731445,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.052758545411559096}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.26953014731407166,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0019839087035506964},\n",
      " 'mnt_features': {'ablated_mean_activation': 18.91192054748535,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.003853961592540145},\n",
      " 'other_features': {'ablated_mean_activation': 0.26573657989501953,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'down',\n",
      "                    'relative_act': 0.001956767635419965},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 24.749189376831055,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'down',\n",
      "                           'relative_act': 0.001532516458040179},\n",
      "                     1617: {'ablated_sum_activation': 7.479479789733887,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.018222119962186176},\n",
      "                     7581: {'ablated_sum_activation': 5.346264362335205,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.012505040206275051},\n",
      "                     8851: {'ablated_sum_activation': 11.915975570678711,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.02015008914442656},\n",
      "                     9588: {'ablated_sum_activation': 19.70793914794922,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.008808133489127703},\n",
      "                     19105: {'ablated_sum_activation': 15.194221496582031,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.3307731822249456},\n",
      "                     19619: {'ablated_sum_activation': 31.712059020996094,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.023518252813598352},\n",
      "                     22460: {'ablated_sum_activation': 7.939701080322266,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.04487242659308235},\n",
      "                     23011: {'ablated_sum_activation': 25.978723526000977,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.13694886312538915},\n",
      "                     39754: {'ablated_sum_activation': 39.09564971923828,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.03389615410059425}}}\n",
      "Abl Feature ID: 44755\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.27052006125450134,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.0016815441194921732},\n",
      " 'mnt_features': {'ablated_mean_activation': 19.05933380126953,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.00391072453930974},\n",
      " 'other_features': {'ablated_mean_activation': 0.2666966915130615,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'up',\n",
      "                    'relative_act': -0.0016491827555000782},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 24.32732391357422,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'down',\n",
      "                           'relative_act': 0.018552021261859657},\n",
      "                     1617: {'ablated_sum_activation': 7.400044918060303,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.02864896809924079},\n",
      "                     7581: {'ablated_sum_activation': 5.242250442504883,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.007193689774193454},\n",
      "                     8851: {'ablated_sum_activation': 12.116616249084473,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.003651418961516836},\n",
      "                     9588: {'ablated_sum_activation': 19.953372955322266,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.003535724070379752},\n",
      "                     19105: {'ablated_sum_activation': 11.310168266296387,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.009408371553707584},\n",
      "                     19619: {'ablated_sum_activation': 32.58961868286133,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.003503675699728855},\n",
      "                     22460: {'ablated_sum_activation': 8.640715599060059,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.03945798955520879},\n",
      "                     23011: {'ablated_sum_activation': 30.665321350097656,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.01874676126111735},\n",
      "                     39754: {'ablated_sum_activation': 38.347904205322266,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.01412180026241417}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.2708505392074585,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.0029052377212792635},\n",
      " 'mnt_features': {'ablated_mean_activation': 19.370708465576172,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.020311737433075905},\n",
      " 'other_features': {'ablated_mean_activation': 0.2669638693332672,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'up',\n",
      "                    'relative_act': -0.0026526390574872494},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 25.80868911743164,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'up',\n",
      "                           'relative_act': -0.041211349763165525},\n",
      "                     1617: {'ablated_sum_activation': 7.380819320678711,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.031172574923530226},\n",
      "                     7581: {'ablated_sum_activation': 5.29006290435791,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.0018612980320604235},\n",
      "                     8851: {'ablated_sum_activation': 12.284610748291016,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.010162757988914809},\n",
      "                     9588: {'ablated_sum_activation': 20.53291893005371,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.03268343211370325},\n",
      "                     19105: {'ablated_sum_activation': 11.678451538085938,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.022847410795300473},\n",
      "                     19619: {'ablated_sum_activation': 33.01255416870117,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.016526758870874698},\n",
      "                     22460: {'ablated_sum_activation': 8.93065071105957,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.07433650918265408},\n",
      "                     23011: {'ablated_sum_activation': 30.48281478881836,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.012683626748718415},\n",
      "                     39754: {'ablated_sum_activation': 38.305511474609375,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.01300071181496474}}}\n",
      "Abl Feature ID: 13262\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.2714080512523651,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.00496959313750267},\n",
      " 'mnt_features': {'ablated_mean_activation': 19.02100944519043,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.001892069005407393},\n",
      " 'other_features': {'ablated_mean_activation': 0.2675926387310028,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'up',\n",
      "                    'relative_act': -0.0050141466781497},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 25.025684356689453,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'up',\n",
      "                           'relative_act': -0.009622242695749662},\n",
      "                     1617: {'ablated_sum_activation': 7.635345458984375,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.0022372529657829245},\n",
      "                     7581: {'ablated_sum_activation': 5.257125377655029,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.004376592490697995},\n",
      "                     8851: {'ablated_sum_activation': 12.122289657592773,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.003184894941885728},\n",
      "                     9588: {'ablated_sum_activation': 19.93505859375,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.0026146208492666594},\n",
      "                     19105: {'ablated_sum_activation': 11.300429344177246,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.010261346902445248},\n",
      "                     19619: {'ablated_sum_activation': 32.66297912597656,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.005762599777155623},\n",
      "                     22460: {'ablated_sum_activation': 8.414763450622559,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.012276471629383903},\n",
      "                     23011: {'ablated_sum_activation': 29.989986419677734,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.003688851438808357},\n",
      "                     39754: {'ablated_sum_activation': 37.86640548706055,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.001388422595360976}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.2698323130607605,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0008650495437905192},\n",
      " 'mnt_features': {'ablated_mean_activation': 19.145465850830078,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.008447551168501377},\n",
      " 'other_features': {'ablated_mean_activation': 0.2659912705421448,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'down',\n",
      "                    'relative_act': 0.001000210177153349},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 25.277198791503906,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'up',\n",
      "                           'relative_act': -0.019769200682088598},\n",
      "                     1617: {'ablated_sum_activation': 7.42182731628418,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.02578974829368098},\n",
      "                     7581: {'ablated_sum_activation': 5.243371963500977,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.006981289940541224},\n",
      "                     8851: {'ablated_sum_activation': 12.167755126953125,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.000553727702574425},\n",
      "                     9588: {'ablated_sum_activation': 20.190536499023438,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.015463636663552445},\n",
      "                     19105: {'ablated_sum_activation': 11.639814376831055,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.01946340734066301},\n",
      "                     19619: {'ablated_sum_activation': 32.64484405517578,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.0052041823748319345},\n",
      "                     22460: {'ablated_sum_activation': 8.5806245803833,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.032229179771197505},\n",
      "                     23011: {'ablated_sum_activation': 30.364219665527344,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.008743723547498371},\n",
      "                     39754: {'ablated_sum_activation': 37.92444610595703,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.002923324127615372}}}\n",
      "Abl Feature ID: 5682\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.26829200983047485,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.006568483542650938},\n",
      " 'mnt_features': {'ablated_mean_activation': 19.24493980407715,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.013687134720385075},\n",
      " 'other_features': {'ablated_mean_activation': 0.2644304037094116,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'down',\n",
      "                    'relative_act': 0.006862453650683165},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 25.85400390625,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'up',\n",
      "                           'relative_act': -0.04303950431277983},\n",
      "                     1617: {'ablated_sum_activation': 7.170316696166992,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.05880375065267824},\n",
      "                     7581: {'ablated_sum_activation': 5.217868804931641,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.01181121881088811},\n",
      "                     8851: {'ablated_sum_activation': 12.703468322753906,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.044605389610710276},\n",
      "                     9588: {'ablated_sum_activation': 20.008148193359375,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.006290592046210392},\n",
      "                     19105: {'ablated_sum_activation': 11.901777267456055,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.04240720802554861},\n",
      "                     19619: {'ablated_sum_activation': 32.332313537597656,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.004419296997817444},\n",
      "                     22460: {'ablated_sum_activation': 9.29671859741211,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.1183736244912199},\n",
      "                     23011: {'ablated_sum_activation': 30.210735321044922,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.003644749463382175},\n",
      "                     39754: {'ablated_sum_activation': 37.75404357910156,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.0015830216788784738}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.27115267515182495,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.004023986402899027},\n",
      " 'mnt_features': {'ablated_mean_activation': 19.278573989868164,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.015458744950592518},\n",
      " 'other_features': {'ablated_mean_activation': 0.2672848105430603,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'up',\n",
      "                    'relative_act': -0.0038580175023525953},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 25.5213680267334,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'up',\n",
      "                           'relative_act': -0.029619828035774756},\n",
      "                     1617: {'ablated_sum_activation': 7.474287033081055,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.018903736031942668},\n",
      "                     7581: {'ablated_sum_activation': 5.317038059234619,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.006970001684075863},\n",
      "                     8851: {'ablated_sum_activation': 12.217713356018066,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.004661789690870878},\n",
      "                     9588: {'ablated_sum_activation': 20.33614730834961,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.022786992433831112},\n",
      "                     19105: {'ablated_sum_activation': 11.902941703796387,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.04250919420854985},\n",
      "                     19619: {'ablated_sum_activation': 32.79843521118164,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.009933580748782011},\n",
      "                     22460: {'ablated_sum_activation': 8.936126708984375,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.07499525899664856},\n",
      "                     23011: {'ablated_sum_activation': 30.440214157104492,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.011268371545925632},\n",
      "                     39754: {'ablated_sum_activation': 37.841468811035156,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.0007289647369089888}}}\n",
      "Abl Feature ID: 29628\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.2708558738231659,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.002924990840256214},\n",
      " 'mnt_features': {'ablated_mean_activation': 18.95778465270996,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0014381653163582087},\n",
      " 'other_features': {'ablated_mean_activation': 0.2670532166957855,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'up',\n",
      "                    'relative_act': -0.0029882064554840326},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 24.649675369262695,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'down',\n",
      "                           'relative_act': 0.005547254039973715},\n",
      "                     1617: {'ablated_sum_activation': 7.605839729309082,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.0016357533854009327},\n",
      "                     7581: {'ablated_sum_activation': 5.3005876541137695,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.003854533205687985},\n",
      "                     8851: {'ablated_sum_activation': 12.1763277053833,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.0012586502798922988},\n",
      "                     9588: {'ablated_sum_activation': 19.955646514892578,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.003650070564814954},\n",
      "                     19105: {'ablated_sum_activation': 11.050521850585938,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.03214928572056695},\n",
      "                     19619: {'ablated_sum_activation': 32.62196350097656,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.00449964144526999},\n",
      "                     22460: {'ablated_sum_activation': 8.466880798339844,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.018546067342547413},\n",
      "                     23011: {'ablated_sum_activation': 29.583797454833984,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.017183042080704532},\n",
      "                     39754: {'ablated_sum_activation': 38.166603088378906,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.00932723798022852}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.27031394839286804,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.0009183495421893895},\n",
      " 'mnt_features': {'ablated_mean_activation': 19.248273849487305,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.013862748630344868},\n",
      " 'other_features': {'ablated_mean_activation': 0.2664521038532257,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'up',\n",
      "                    'relative_act': -0.0007305698236450553},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 25.402713775634766,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'up',\n",
      "                           'relative_act': -0.024832907151148248},\n",
      "                     1617: {'ablated_sum_activation': 7.32413387298584,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.038613268692172305},\n",
      "                     7581: {'ablated_sum_activation': 5.254250526428223,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.0049210476765352805},\n",
      "                     8851: {'ablated_sum_activation': 12.21569538116455,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.004495851747428379},\n",
      "                     9588: {'ablated_sum_activation': 20.404821395874023,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.02624088969233735},\n",
      "                     19105: {'ablated_sum_activation': 11.627422332763672,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.018378060525005235},\n",
      "                     19619: {'ablated_sum_activation': 32.84941864013672,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.011503468965453467},\n",
      "                     22460: {'ablated_sum_activation': 8.988175392150879,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.08125659452405666},\n",
      "                     23011: {'ablated_sum_activation': 30.416454315185547,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.010479034896667083},\n",
      "                     39754: {'ablated_sum_activation': 37.99964904785156,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.004912088426456197}}}\n",
      "Abl Feature ID: 33425\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.269595205783844,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0017430102452635765},\n",
      " 'mnt_features': {'ablated_mean_activation': 19.42837905883789,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.023349415510892868},\n",
      " 'other_features': {'ablated_mean_activation': 0.26569655537605286,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'down',\n",
      "                    'relative_act': 0.0021070900838822126},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 25.132240295410156,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'up',\n",
      "                           'relative_act': -0.01392107601794652},\n",
      "                     1617: {'ablated_sum_activation': 7.464171409606934,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.020231541661310366},\n",
      "                     7581: {'ablated_sum_activation': 5.208171844482422,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.01364768267872474},\n",
      "                     8851: {'ablated_sum_activation': 12.175131797790527,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.001160310874846508},\n",
      "                     9588: {'ablated_sum_activation': 20.832571029663086,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.0477541465950354},\n",
      "                     19105: {'ablated_sum_activation': 11.183473587036133,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.020504819076665872},\n",
      "                     19619: {'ablated_sum_activation': 33.13353729248047,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.0202520865769529},\n",
      "                     22460: {'ablated_sum_activation': 9.102688789367676,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.09503228986804871},\n",
      "                     23011: {'ablated_sum_activation': 31.147415161132812,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.03476262175080137},\n",
      "                     39754: {'ablated_sum_activation': 38.904388427734375,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.028838192022107828}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.27069219946861267,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.0023189373314380646},\n",
      " 'mnt_features': {'ablated_mean_activation': 19.147397994995117,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.008549322374165058},\n",
      " 'other_features': {'ablated_mean_activation': 0.26685091853141785,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'up',\n",
      "                    'relative_act': -0.0022284225560724735},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 25.127178192138672,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'up',\n",
      "                           'relative_act': -0.01371685334875398},\n",
      "                     1617: {'ablated_sum_activation': 7.522063255310059,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.012632492644139001},\n",
      "                     7581: {'ablated_sum_activation': 5.264121055603027,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.00305171240587152},\n",
      "                     8851: {'ablated_sum_activation': 12.108927726745605,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.004283645662855884},\n",
      "                     9588: {'ablated_sum_activation': 20.07837677001953,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.009822670842438687},\n",
      "                     19105: {'ablated_sum_activation': 11.67941951751709,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.02293219046421633},\n",
      "                     19619: {'ablated_sum_activation': 32.73512268066406,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.00798405327163303},\n",
      "                     22460: {'ablated_sum_activation': 8.542153358459473,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.027601181252120924},\n",
      "                     23011: {'ablated_sum_activation': 30.48518180847168,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.012762262598580328},\n",
      "                     39754: {'ablated_sum_activation': 37.931427001953125,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.0031079360108901364}}}\n",
      "Abl Feature ID: 13989\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.27126067876815796,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.004423902370035648},\n",
      " 'mnt_features': {'ablated_mean_activation': 18.958755493164062,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0013870283728465438},\n",
      " 'other_features': {'ablated_mean_activation': 0.26745790243148804,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'up',\n",
      "                    'relative_act': -0.004508109297603369},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 24.64474105834961,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'down',\n",
      "                           'relative_act': 0.0057463211255807615},\n",
      "                     1617: {'ablated_sum_activation': 7.588450908660889,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.003918259649478985},\n",
      "                     7581: {'ablated_sum_activation': 5.31713342666626,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.00698806289442044},\n",
      "                     8851: {'ablated_sum_activation': 12.238579750061035,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.006377631941590099},\n",
      "                     9588: {'ablated_sum_activation': 19.77924346923828,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.005221951150230403},\n",
      "                     19105: {'ablated_sum_activation': 11.355283737182617,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.005456970768089417},\n",
      "                     19619: {'ablated_sum_activation': 32.474891662597656,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 2.9013272691197105e-05},\n",
      "                     22460: {'ablated_sum_activation': 8.426057815551758,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.013635157461347809},\n",
      "                     23011: {'ablated_sum_activation': 29.87249755859375,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.007592002993839915},\n",
      "                     39754: {'ablated_sum_activation': 37.89067840576172,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.0020303271927802806}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.2704470753669739,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.001411292003467679},\n",
      " 'mnt_features': {'ablated_mean_activation': 19.140050888061523,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.008162328973412514},\n",
      " 'other_features': {'ablated_mean_activation': 0.26660728454589844,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'up',\n",
      "                    'relative_act': -0.0013133914908394217},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 25.134899139404297,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'up',\n",
      "                           'relative_act': -0.014028342936128285},\n",
      "                     1617: {'ablated_sum_activation': 7.49334716796875,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.016401848287194598},\n",
      "                     7581: {'ablated_sum_activation': 5.288743495941162,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.0016114211869431953},\n",
      "                     8851: {'ablated_sum_activation': 12.156570434570312,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.0003659888383960971},\n",
      "                     9588: {'ablated_sum_activation': 20.173160552978516,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.014589730149874016},\n",
      "                     19105: {'ablated_sum_activation': 11.671856880187988,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.022269823198696383},\n",
      "                     19619: {'ablated_sum_activation': 32.65088653564453,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.005390243200511514},\n",
      "                     22460: {'ablated_sum_activation': 8.473316192626953,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.01932023030751342},\n",
      "                     23011: {'ablated_sum_activation': 30.370227813720703,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.008943323005246178},\n",
      "                     39754: {'ablated_sum_activation': 37.987518310546875,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.004591287448962344}}}\n",
      "Abl Feature ID: 48397\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.2694782018661499,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.002176252193748951},\n",
      " 'mnt_features': {'ablated_mean_activation': 18.9095458984375,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.003979041241109371},\n",
      " 'other_features': {'ablated_mean_activation': 0.26568514108657837,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'down',\n",
      "                    'relative_act': 0.002149959560483694},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 24.561328887939453,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'down',\n",
      "                           'relative_act': 0.009111455175742095},\n",
      "                     1617: {'ablated_sum_activation': 7.610442161560059,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.0010316249827113406},\n",
      "                     7581: {'ablated_sum_activation': 5.282916069030762,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.0005077909288377935},\n",
      "                     8851: {'ablated_sum_activation': 12.175440788269043,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.0011857191421789132},\n",
      "                     9588: {'ablated_sum_activation': 19.70734977722168,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.008837775323675082},\n",
      "                     19105: {'ablated_sum_activation': 11.058427810668945,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.03145684881878129},\n",
      "                     19619: {'ablated_sum_activation': 32.55873107910156,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.0025525806836129727},\n",
      "                     22460: {'ablated_sum_activation': 8.49974250793457,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.022499254575327306},\n",
      "                     23011: {'ablated_sum_activation': 29.590984344482422,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.016944283110293824},\n",
      "                     39754: {'ablated_sum_activation': 38.050106048583984,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.006246438907371668}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.27026450634002686,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.0007352755637839437},\n",
      " 'mnt_features': {'ablated_mean_activation': 19.223615646362305,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.012563928961753845},\n",
      " 'other_features': {'ablated_mean_activation': 0.26640763878822327,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'up',\n",
      "                    'relative_act': -0.0005635696579702199},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 25.386089324951172,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'up',\n",
      "                           'relative_act': -0.02416221959098164},\n",
      "                     1617: {'ablated_sum_activation': 7.377684593200684,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.031584048205677076},\n",
      "                     7581: {'ablated_sum_activation': 5.324773788452148,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.008435036761176259},\n",
      "                     8851: {'ablated_sum_activation': 12.15450382232666,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.000535926231326104},\n",
      "                     9588: {'ablated_sum_activation': 20.284038543701172,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.020166232175467258},\n",
      "                     19105: {'ablated_sum_activation': 11.726447105407715,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.02705106241848109},\n",
      "                     19619: {'ablated_sum_activation': 32.85673904418945,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.011728879776361997},\n",
      "                     22460: {'ablated_sum_activation': 8.768664360046387,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.05484992791201073},\n",
      "                     23011: {'ablated_sum_activation': 30.30054473876953,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.006628349484814948},\n",
      "                     39754: {'ablated_sum_activation': 38.056644439697266,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.006419348616624645}}}\n",
      "Abl Feature ID: 7317\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.2601272761821747,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.03680085390806198},\n",
      " 'mnt_features': {'ablated_mean_activation': 17.0704345703125,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.10085040330886841},\n",
      " 'other_features': {'ablated_mean_activation': 0.25670650601387024,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'down',\n",
      "                    'relative_act': 0.03587157279253006},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 20.90314483642578,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'down',\n",
      "                           'relative_act': 0.15669519089408399},\n",
      "                     1617: {'ablated_sum_activation': 7.882081031799316,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.034624468977439364},\n",
      "                     7581: {'ablated_sum_activation': 5.890655517578125,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.11560484058279516},\n",
      "                     8851: {'ablated_sum_activation': 11.800281524658203,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.029663603166535865},\n",
      "                     9588: {'ablated_sum_activation': 19.03415870666504,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.04269527349894916},\n",
      "                     19105: {'ablated_sum_activation': 4.700462341308594,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.5883139370206762},\n",
      "                     19619: {'ablated_sum_activation': 30.3665828704834,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.06494832524680925},\n",
      "                     22460: {'ablated_sum_activation': 8.892646789550781,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.06976472581663519},\n",
      "                     23011: {'ablated_sum_activation': 21.925926208496094,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.2715887090316259},\n",
      "                     39754: {'ablated_sum_activation': 39.30840301513672,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.03952247866562167}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.2685546875,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.005595840979367495},\n",
      " 'mnt_features': {'ablated_mean_activation': 18.990169525146484,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.0002676403964869678},\n",
      " 'other_features': {'ablated_mean_activation': 0.2647450268268585,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'down',\n",
      "                    'relative_act': 0.00568080460652709},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 25.163400650024414,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'up',\n",
      "                           'relative_act': -0.015178191973710852},\n",
      "                     1617: {'ablated_sum_activation': 7.359257698059082,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.034002815092699104},\n",
      "                     7581: {'ablated_sum_activation': 5.436440467834473,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.029583088565745378},\n",
      "                     8851: {'ablated_sum_activation': 12.125192642211914,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.0029461827018862166},\n",
      "                     9588: {'ablated_sum_activation': 20.121341705322266,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.011983550988115996},\n",
      "                     19105: {'ablated_sum_activation': 12.964078903198242,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.1354480083448083},\n",
      "                     19619: {'ablated_sum_activation': 32.03351974487305,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.013619793394916212},\n",
      "                     22460: {'ablated_sum_activation': 8.70150089263916,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.04677031899519151},\n",
      "                     23011: {'ablated_sum_activation': 27.20360565185547,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.09625648999189014},\n",
      "                     39754: {'ablated_sum_activation': 38.79334259033203,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.025901551627502146}}}\n",
      "Abl Feature ID: 1645\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.2663448452949524,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.013778443448245525},\n",
      " 'mnt_features': {'ablated_mean_activation': 15.538354873657227,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.18154950439929962},\n",
      " 'other_features': {'ablated_mean_activation': 0.2632371187210083,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'down',\n",
      "                    'relative_act': 0.011344147846102715},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 17.506072998046875,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'down',\n",
      "                           'relative_act': 0.29374476121411375},\n",
      "                     1617: {'ablated_sum_activation': 8.502283096313477,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.11603396338123544},\n",
      "                     7581: {'ablated_sum_activation': 6.086284160614014,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.15265407228078193},\n",
      "                     8851: {'ablated_sum_activation': 12.634339332580566,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.038920917159710085},\n",
      "                     9588: {'ablated_sum_activation': 20.570022583007812,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.03454952469444819},\n",
      "                     19105: {'ablated_sum_activation': 3.260133743286133,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.7144639126715188},\n",
      "                     19619: {'ablated_sum_activation': 29.829788208007812,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.08147737463921222},\n",
      "                     22460: {'ablated_sum_activation': 3.7641122341156006,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.5471860529909988},\n",
      "                     23011: {'ablated_sum_activation': 16.183837890625,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.46234926914128766},\n",
      "                     39754: {'ablated_sum_activation': 37.04665756225586,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.02029005654162303}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.2693394422531128,\n",
      "                  'baseline_mean_activation': 0.27006593346595764,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0026900512166321278},\n",
      " 'mnt_features': {'ablated_mean_activation': 18.835433959960938,\n",
      "                  'baseline_mean_activation': 18.985088348388672,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.007882732897996902},\n",
      " 'other_features': {'ablated_mean_activation': 0.26556140184402466,\n",
      "                    'baseline_mean_activation': 0.2662575840950012,\n",
      "                    'direction': 'down',\n",
      "                    'relative_act': 0.0026146946474909782},\n",
      " 'per_mnt_feature': {606: {'ablated_sum_activation': 24.437641143798828,\n",
      "                           'baseline_sum_activation': 24.78717613220215,\n",
      "                           'direction': 'down',\n",
      "                           'relative_act': 0.01410144449443006},\n",
      "                     1617: {'ablated_sum_activation': 7.454782485961914,\n",
      "                            'baseline_sum_activation': 7.6183013916015625,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.021463958595516548},\n",
      "                     7581: {'ablated_sum_activation': 5.1977949142456055,\n",
      "                            'baseline_sum_activation': 5.2802348136901855,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.015612922976318206},\n",
      "                     8851: {'ablated_sum_activation': 12.17469310760498,\n",
      "                            'baseline_sum_activation': 12.16102123260498,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.0011242374088807473},\n",
      "                     9588: {'ablated_sum_activation': 19.571773529052734,\n",
      "                            'baseline_sum_activation': 19.883071899414062,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.015656452480511132},\n",
      "                     19105: {'ablated_sum_activation': 13.643922805786133,\n",
      "                             'baseline_sum_activation': 11.41758918762207,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.1949915679711226},\n",
      "                     19619: {'ablated_sum_activation': 32.06565475463867,\n",
      "                             'baseline_sum_activation': 32.475833892822266,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.012630288094711175},\n",
      "                     22460: {'ablated_sum_activation': 8.056619644165039,\n",
      "                             'baseline_sum_activation': 8.312712669372559,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.030807395298046387},\n",
      "                     23011: {'ablated_sum_activation': 27.300045013427734,\n",
      "                             'baseline_sum_activation': 30.101024627685547,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.09305263355295534},\n",
      "                     39754: {'ablated_sum_activation': 38.451412200927734,\n",
      "                             'baseline_sum_activation': 37.81390380859375,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.01685910017540732}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "mnt_res = runner_L1_5.results['analysis_mnt_mnt_L5_abl1TC1_5_seed42']\n",
    "for abl_fid, data in mnt_res.items():\n",
    "    print(f'Abl Feature ID: {abl_fid}')\n",
    "    print('*' * 10, 'zero abl summary:', '*' * 10)\n",
    "    pprint(data['zero_fea_summary'])\n",
    "    print('*' * 10, 'gn abl summary:', '*' * 10)\n",
    "    pprint(data['gn_fea_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9a2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abl Feature ID: 48184\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.0987367033958435,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.02710266411304474},\n",
      " 'mnt_features': {'ablated_mean_activation': 35.84602737426758,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.2738674581050873},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 0.0,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.9999999999992605},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10150738805532455,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.00019814421830233186},\n",
      " 'mnt_features': {'ablated_mean_activation': 49.4644775390625,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.0020013265311717987},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 136.18450927734375,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.00730781641442766},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "Abl Feature ID: 44755\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10115115344524384,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0033119963482022285},\n",
      " 'mnt_features': {'ablated_mean_activation': 47.713470458984375,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.03346880152821541},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 0.0,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.9999999999939476},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.1014704555273056,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.00016576866619288921},\n",
      " 'mnt_features': {'ablated_mean_activation': 49.28296661376953,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0016755381366237998},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 15.694951057434082,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.050063320072564144},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "Abl Feature ID: 13262\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10124089568853378,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0024277253542095423},\n",
      " 'mnt_features': {'ablated_mean_activation': 48.15457534790039,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.02453334629535675},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 0.0,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.999999999991743},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10151513665914536,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.00027449469780549407},\n",
      " 'mnt_features': {'ablated_mean_activation': 49.502532958984375,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.002772214589640498},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 13.479594230651855,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.11300019292232681},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "Abl Feature ID: 5682\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10120251029729843,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0028059540782123804},\n",
      " 'mnt_features': {'ablated_mean_activation': 47.965972900390625,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.028353864327073097},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 0.0,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.9999999999928556},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10149455070495605,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -7.16520007699728e-05},\n",
      " 'mnt_features': {'ablated_mean_activation': 49.401363372802734,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.000722823606338352},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 14.353900909423828,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'up',\n",
      "                            'relative_act': -0.025493234580854635},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "Abl Feature ID: 29628\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10125377774238586,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0023007928393781185},\n",
      " 'mnt_features': {'ablated_mean_activation': 48.217960357666016,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.02324935793876648},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 0.0,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.9999999999912871},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10144831240177155,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.00038395487354137003},\n",
      " 'mnt_features': {'ablated_mean_activation': 49.174072265625,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0038814095314592123},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 9.561115264892578,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.16694806057422423},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "Abl Feature ID: 33425\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10117170214653015,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0031095207668840885},\n",
      " 'mnt_features': {'ablated_mean_activation': 47.81452560424805,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0314217284321785},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 0.0,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.9999999999935532},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10144937038421631,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0003735301143024117},\n",
      " 'mnt_features': {'ablated_mean_activation': 49.179290771484375,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0037756983656436205},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 13.647666931152344,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.12016222255945905},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "Abl Feature ID: 13989\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.1011609435081482,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0032155304215848446},\n",
      " 'mnt_features': {'ablated_mean_activation': 47.76162338256836,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.03249337151646614},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 0.0,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.9999999999937659},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10148169845342636,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 5.498703831108287e-05},\n",
      " 'mnt_features': {'ablated_mean_activation': 49.33821487426758,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0005563747836276889},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 15.76593017578125,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.01712270127724677},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "Abl Feature ID: 48397\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10127314180135727,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0021099899895489216},\n",
      " 'mnt_features': {'ablated_mean_activation': 48.313148498535156,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0213211327791214},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 0.0,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'down',\n",
      "                             'relative_act': 0.9999999999904992}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10148847103118896,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -1.1746229574782774e-05},\n",
      " 'mnt_features': {'ablated_mean_activation': 49.371456146240234,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'up',\n",
      "                  'relative_act': -0.00011699325841618702},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.583118438720703,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'up',\n",
      "                             'relative_act': -0.005487904104060744}}}\n",
      "Abl Feature ID: 7317\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.09829060733318329,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.031498249620199203},\n",
      " 'mnt_features': {'ablated_mean_activation': 33.65334701538086,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.3182845413684845},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 0.0,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.9999999999993636},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.10145457088947296,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.0003222871746402234},\n",
      " 'mnt_features': {'ablated_mean_activation': 49.20484161376953,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.003258115379139781},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 105.15196990966797,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 155.51495361328125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.010236533376522876},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "Abl Feature ID: 1645\n",
      "********** zero abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.09934797137975693,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.021079564467072487},\n",
      " 'mnt_features': {'ablated_mean_activation': 38.85048294067383,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.21300622820854187},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 0.0,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.999999999999049},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n",
      "********** gn abl summary: **********\n",
      "{'all_features': {'ablated_mean_activation': 0.1014828085899353,\n",
      "                  'baseline_mean_activation': 0.10148727893829346,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 4.40483599959407e-05},\n",
      " 'mnt_features': {'ablated_mean_activation': 49.34365463256836,\n",
      "                  'baseline_mean_activation': 49.36568069458008,\n",
      "                  'direction': 'down',\n",
      "                  'relative_act': 0.000446181686129421},\n",
      " 'other_features': {'ablated_mean_activation': 0.09146242588758469,\n",
      "                    'baseline_mean_activation': 0.09146242588758469,\n",
      "                    'direction': 'same',\n",
      "                    'relative_act': 0.0},\n",
      " 'per_mnt_feature': {1645: {'ablated_sum_activation': 104.93169403076172,\n",
      "                            'baseline_sum_activation': 105.15196990966797,\n",
      "                            'direction': 'down',\n",
      "                            'relative_act': 0.0020948335927065474},\n",
      "                     5682: {'ablated_sum_activation': 13.9970703125,\n",
      "                            'baseline_sum_activation': 13.9970703125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     7317: {'ablated_sum_activation': 157.12335205078125,\n",
      "                            'baseline_sum_activation': 157.12335205078125,\n",
      "                            'direction': 'same',\n",
      "                            'relative_act': 0.0},\n",
      "                     13262: {'ablated_sum_activation': 12.111043930053711,\n",
      "                             'baseline_sum_activation': 12.111043930053711,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     13989: {'ablated_sum_activation': 16.04058837890625,\n",
      "                             'baseline_sum_activation': 16.04058837890625,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     29628: {'ablated_sum_activation': 11.477213859558105,\n",
      "                             'baseline_sum_activation': 11.477213859558105,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     33425: {'ablated_sum_activation': 15.511571884155273,\n",
      "                             'baseline_sum_activation': 15.511571884155273,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     44755: {'ablated_sum_activation': 16.52210235595703,\n",
      "                             'baseline_sum_activation': 16.52210235595703,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48184: {'ablated_sum_activation': 135.19651794433594,\n",
      "                             'baseline_sum_activation': 135.19651794433594,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0},\n",
      "                     48397: {'ablated_sum_activation': 10.52535629272461,\n",
      "                             'baseline_sum_activation': 10.52535629272461,\n",
      "                             'direction': 'same',\n",
      "                             'relative_act': 0.0}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "mnt_res = runner_L1_5.results['analysis_abl_mnt_L5_abl1TC1_5_seed42']\n",
    "for abl_fid, data in mnt_res.items():\n",
    "    print(f'Abl Feature ID: {abl_fid}')\n",
    "    print('*' * 10, 'zero abl summary:', '*' * 10)\n",
    "    pprint(data['zero_fea_summary'])\n",
    "    print('*' * 10, 'gn abl summary:', '*' * 10)\n",
    "    pprint(data['gn_fea_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be378c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chemistry': 0.32059502601623535,\n",
       " 'titration': 0.1483946144580841,\n",
       " 'experimentation': 0.1313956379890442,\n",
       " 'science': 0.07199804484844208,\n",
       " 'science equipment': 0.047188661992549896,\n",
       " 'technology': 0.0370088592171669,\n",
       " 'research tools': 0.02706180326640606,\n",
       " 'graphic': 0.02584078349173069,\n",
       " 'experiment': 0.023069527000188828,\n",
       " 'chemical': 0.019389508292078972,\n",
       " 'test tube': 0.018862372264266014,\n",
       " 'electrostatics': 0.018215486779808998,\n",
       " 'solution': 0.017842872068285942,\n",
       " 'ethicality': 0.007043260149657726,\n",
       " 'reagent': 0.0059230271726846695,\n",
       " 'research': 0.004928987938910723,\n",
       " 'testing': 0.004735670052468777,\n",
       " 'laboratory bottle': 0.00420342106372118,\n",
       " 'laboratory': 0.004067030269652605,\n",
       " 'hardware': 0.003388571087270975}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner_L1_5.results['zero_abl_L5_TC1_5_seed42_all_features']['all'][\n",
    "    'text_probs'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3244c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionModelSAERunnerConfig(model_class_name='HookedViT', model_name='open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K', vit_model_cfg=None, model_path=None, hook_point_layer=0, layer_subtype='ln2.hook_normalized', hook_point_head_index=None, context_size=50, use_cached_activations=False, use_patches_only=False, cached_activations_path='activations/_network_scratch_s_sonia.joseph_datasets_kaggle_datasets/open-clip:laion_CLIP-ViT-B-32-DataComp.XL-s13B-b90K/blocks.9.ln2.hook_normalized', image_size=224, architecture='standard', b_dec_init_method='geometric_median', expansion_factor=64, from_pretrained_path=None, is_transcoder=True, transcoder_with_skip_connection=True, out_hook_point_layer=0, layer_out_subtype='hook_mlp_out', d_out=768, _device='cuda', seed=42, _dtype='float32', d_in=768, activation_fn_str='topk', activation_fn_kwargs={'k': 768}, cls_token_only=False, max_grad_norm=1.0, initialization_method='independent', normalize_activations='layer_norm', n_batches_in_buffer=20, store_batch_size=32, num_workers=16, num_epochs=1, verbose=False, l1_coefficient=0.0002, lp_norm=1, lr=0.0009762258997107048, lr_scheduler_name='cosineannealingwarmup', lr_warm_up_steps=500, train_batch_size=4096, dataset_name='imagenet1k', dataset_path='/network/scratch/s/sonia.joseph/datasets/kaggle_datasets', dataset_train_path='/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/train', dataset_val_path='/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/val', use_ghost_grads=False, feature_sampling_window=1000, dead_feature_window=5000, dead_feature_threshold=1e-08, log_to_wandb=True, wandb_project='openclip-transcoders', wandb_entity=None, wandb_log_frequency=10, n_validation_runs=0, n_checkpoints=10, checkpoint_path='/network/scratch/p/praneet.suresh/openclip-transcoder-checkpoints/496b0e61-openclip-transcoders')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc_list[0].cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202dfb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top activation features number: 2639\n",
      "--- [Seed 42] Running Zero Ablation for L1 ---\n",
      "--- [Seed 42] Running Zero Ablation for L1 ---\n",
      "Zero Ablation Done.\n",
      "--- [Seed 42] Running Gaussian Noise Ablation for L1 (10 samples) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GN Ablation Done.\n",
      "Ablation Suite Done.\n"
     ]
    }
   ],
   "source": [
    "# keep the target features\n",
    "total_fea_num = tc_list[0].cfg.d_sae\n",
    "abl_features = runner_L1_5.get_inverse_ids(shallow_feat_ids, total_fea_num)\n",
    "\n",
    "runner_L1_5.run_ablation_suite(\n",
    "    layer_idx=1,\n",
    "    seed=42,\n",
    "    n_gn_samples=10,\n",
    "    sigma=2.0,\n",
    "    batch_mode=False,\n",
    "    runtime_batch_size=1024,\n",
    "    ablation_feature_ids=abl_features,\n",
    "    output_prefix='keep_shallow_feat_',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d080e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['transcoder_L1_TC1_5_seed42', 'zero_abl_L1_TC1_5_seed42_each_feature', 'gn_abl_L1_TC1_5_seed42_each_feature', 'zero_abl_L1_TC1_5_seed42_all_features', 'gn_abl_L1_TC1_5_seed42_all_features', 'transcoder_L5_TC1_5_seed42', 'zero_abl_L5_TC1_5_seed42_each_feature', 'gn_abl_L5_TC1_5_seed42_each_feature', 'zero_abl_L5_TC1_5_seed42_all_features', 'gn_abl_L5_TC1_5_seed42_all_features', 'transcoder_L1_5_TC1_5_seed42', 'mnt_L5_abl1zero_abl_L1_TC1_5_seed42_each_feature', 'mnt_L5_abl1gn_abl_L1_TC1_5_seed42_each_feature', 'analysis_mnt_L5_abl1TC1_5_seed42', 'keep_shallow_featzero_abl_L1_TC1_5_seed42_all_features', 'keep_shallow_featgn_abl_L1_TC1_5_seed42_all_features', 'keep_shallow_feat_zero_abl_L1_TC1_5_seed42_all_features', 'keep_shallow_feat_gn_abl_L1_TC1_5_seed42_all_features'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner_L1_5.results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9c6767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_probs': {'equivalence': 0.1483469307422638,\n",
       "  'experimentation': 0.057922326028347015,\n",
       "  'graphic': 0.049537450075149536,\n",
       "  'solution': 0.043882980942726135,\n",
       "  'determinable': 0.04143582284450531,\n",
       "  'placidness': 0.037868548184633255,\n",
       "  'candidness': 0.026098575443029404,\n",
       "  'misappropriation': 0.02566319890320301,\n",
       "  'immorality': 0.024013027548789978,\n",
       "  'technology': 0.02248765528202057,\n",
       "  'trustless': 0.022339098155498505,\n",
       "  'funk': 0.02200547605752945,\n",
       "  'ethicality': 0.020759832113981247,\n",
       "  'corrupt': 0.020609714090824127,\n",
       "  'inconstantly': 0.01954696886241436,\n",
       "  'sailed': 0.019056761637330055,\n",
       "  'edged': 0.018303651362657547,\n",
       "  'premonition': 0.01821352355182171,\n",
       "  'secret': 0.016934601590037346,\n",
       "  'electrostatics': 0.01629837229847908},\n",
       " 'logits': tensor([15.6273, 11.5802, 15.2983, 11.1219, 13.4184, 14.4869,  8.6593, 15.2854,\n",
       "         14.2036, 12.9189, 11.7604, 16.1493, 13.4427, 12.9502, 11.7207, 11.4611,\n",
       "         14.4375,  8.7298, 13.9266, 11.5794, 12.1916, 16.5841, 12.3297,  4.9156,\n",
       "         15.4739, 11.4472, 17.2794, 13.3833, 14.9252, 12.7025, 12.0949,  8.0525,\n",
       "         16.2629, 15.1040, 13.0529, 16.5967, 11.0958, 12.6202, 12.2603,  7.8883,\n",
       "         17.2644, 12.4214,  8.0807, 13.6683, 12.3966,  1.9019, 14.7555, 16.7027,\n",
       "         16.2397, 15.6050,  7.7445, 12.7777, 14.8450, 15.3883, 15.3385, 13.1228,\n",
       "          4.8712, 13.2756,  8.9136, 10.0887, 16.8297, 10.7279,  9.8110, 17.9546,\n",
       "         15.2562, 14.7408, 13.1615, 11.7912, 10.4587,  7.9535, 11.9863, 13.2986,\n",
       "          9.4390, 13.4031, 13.0051, 14.2139, 10.4692, 13.3065,  9.3429, 10.7082,\n",
       "          8.6685, 10.2427, 15.2957, 14.3804,  6.4032, 14.5740, 17.1459,  8.3133,\n",
       "         15.0010,  7.5359, 14.1376, 13.2010,  8.2702,  9.8081, 12.5941, 12.5222,\n",
       "         12.2662, 13.8322, 17.8972, 13.1196, 13.8360, 11.0080,  7.7025, 13.6828,\n",
       "         16.5947, 15.9123, 12.9027, 13.7316,  9.2198, 12.1480, 12.0067, 12.6142,\n",
       "         11.4129,  9.9592, 14.6347, 12.3813, 12.7784, 13.8406, 14.7891, 12.2076,\n",
       "          7.8566, 16.7809, 11.8590, 16.6602, 12.6963, 12.4424, 18.0758, 14.0623,\n",
       "         16.9641, 14.3034, 13.6107,  7.8645, 12.5760, 11.6493, 11.3753, 13.0982,\n",
       "          7.2195, 12.7435,  8.0029, 17.8072,  7.2727,  9.1814,  8.8876, 14.8721,\n",
       "         11.7296, 10.1264, 13.6774, 14.9402, 15.9495, 17.1205, 14.2978, 15.3469,\n",
       "         17.0802, 13.9347, 15.5497, 10.3642,  9.8041, 11.9524, 12.4462, 12.8459,\n",
       "         11.7032, 14.7446, 13.1223, 12.6877, 14.9648, 12.6790, 16.9565, 14.5066,\n",
       "         13.0173, 17.3517, 15.4216, 15.3620, 10.0252, 17.1988, 14.3652, 13.4844,\n",
       "         11.6447, 15.9710, 16.6755,  9.3842, 12.6964, 12.1451, 13.0113, 16.3996,\n",
       "         11.6335, 17.2061, 14.8500, 16.2253, 12.2875, 10.1539, 13.7783, 14.8637,\n",
       "         17.0024,  9.7221, 19.1726, 14.7159, 13.5030, 15.0851, 16.2764, 13.6127,\n",
       "         18.2322, 11.5063, 11.0853, 13.0427,  9.8463, 11.9450, 11.3871,  7.8540,\n",
       "         12.4335, 15.2261, 14.2410,  9.0391, 11.4312, 14.4065,  8.0753,  7.3330,\n",
       "         11.8347,  8.3535,  8.5560,  7.8732, 12.6402, 12.7482, 11.4013, 10.6102,\n",
       "          8.0046, 11.6566,  9.6381, 14.4099, 14.0057,  7.7222,  3.5173, 12.4689,\n",
       "         13.8236,  9.9197, 16.2442, 12.9306, 17.4349, 17.2860, 10.8799, 15.8074,\n",
       "         12.0048, 12.5515, 16.1808, 12.5421, 10.1004,  5.4788, 17.0752, 17.4181,\n",
       "         14.8494], device='cuda:0')}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner_L1_5.results['keep_shallow_feat_gn_abl_L1_TC1_5_seed42_all_features'][0][\n",
    "    'all'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600df1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38735eda",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3aa8b75",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# find if 'chemistry' is in the value of the keys\n",
    "for i in range(len(runner.results['gn_ablation_L0_seed42_each_feature'])):\n",
    "    for key, value in runner.results['gn_ablation_L0_seed42_each_feature'][\n",
    "        i\n",
    "    ].items():\n",
    "        if 'chemistry' not in value['text_probs']:\n",
    "            print(key)\n",
    "            print(value['text_probs'])\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1398565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 features with highest impact:\n",
      "Feature 25737 has the largest change in text probabilities: 10.588614463806152\n",
      "Top 20 negative words: {'lapboard': '-1.2820', 'provided': '-1.1584', 'experimentation': '-1.1334', 'cultivatable': '-1.0853', 'associational': '-1.0709', 'graphic': '-1.0633', 'moneyman': '-1.0406', 'regenerate': '-1.0327', 'hockey stick': '-0.9887', 'potbelly': '-0.9720', 'inclusiveness': '-0.9701', 'twit': '-0.9613', 'thyself': '-0.9431', 'damnably': '-0.9401', 'reconnaissance': '-0.9367', 'illuminate': '-0.9363', 'emptiness': '-0.9330', 'relapse': '-0.9258', 'concert': '-0.9236', 'Swiss chard': '-0.9146'}\n",
      "Top 20 positive words: {'guesthouse': '0.4554', 'flask': '-0.0542', 'science equipment': '-0.1873', 'graduated tube': '-0.2040', 'mixture': '-0.2081', 'hippopotamus': '-0.2142', 'apparatus': '-0.2153', 'chemistry': '-0.2169', 'beaker': '-0.2212', 'test tube': '-0.2224', 'titration': '-0.2291', 'glassware': '-0.2424', 'drum kit': '-0.2587', 'trachea': '-0.2633', 'long haul': '-0.2865', 'safety goggles': '-0.2909', 'laboratory bottle': '-0.2940', 'spring fever': '-0.2994', 'applause': '-0.3045', 'chemical': '-0.3134'}\n",
      "Base logit for the concept: 24.789505004882812\n",
      "Top 20 features with largest logit difference for laboratory bottle in zero ablation:\n",
      "Feature 19065 has the largest logit difference: 0.4012298583984375\n",
      "Logit for the concept: 24.388275146484375\n",
      "Feature 18568 has the largest logit difference: 0.3695545196533203\n",
      "Logit for the concept: 24.419950485229492\n",
      "Feature 3759 has the largest logit difference: 0.34330177307128906\n",
      "Logit for the concept: 24.446203231811523\n",
      "Feature 27860 has the largest logit difference: 0.3103828430175781\n",
      "Logit for the concept: 24.479122161865234\n",
      "Feature 25737 has the largest logit difference: 0.29399871826171875\n",
      "Logit for the concept: 24.495506286621094\n",
      "Feature 16252 has the largest logit difference: 0.29347991943359375\n",
      "Logit for the concept: 24.49602508544922\n",
      "Feature 4223 has the largest logit difference: 0.28912925720214844\n",
      "Logit for the concept: 24.500375747680664\n",
      "Feature 46818 has the largest logit difference: 0.279693603515625\n",
      "Logit for the concept: 24.509811401367188\n",
      "Feature 24115 has the largest logit difference: 0.24888038635253906\n",
      "Logit for the concept: 24.540624618530273\n",
      "Feature 36791 has the largest logit difference: 0.23753738403320312\n",
      "Logit for the concept: 24.55196762084961\n",
      "Feature 16576 has the largest logit difference: 0.22533035278320312\n",
      "Logit for the concept: 24.56417465209961\n",
      "Feature 7787 has the largest logit difference: 0.2232666015625\n",
      "Logit for the concept: 24.566238403320312\n",
      "Feature 37423 has the largest logit difference: 0.2203369140625\n",
      "Logit for the concept: 24.569168090820312\n",
      "Feature 31305 has the largest logit difference: 0.2176513671875\n",
      "Logit for the concept: 24.571853637695312\n",
      "Feature 47670 has the largest logit difference: 0.2170391082763672\n",
      "Logit for the concept: 24.572465896606445\n",
      "Feature 14054 has the largest logit difference: 0.21181869506835938\n",
      "Logit for the concept: 24.577686309814453\n",
      "Feature 4330 has the largest logit difference: 0.20575714111328125\n",
      "Logit for the concept: 24.58374786376953\n",
      "Feature 4782 has the largest logit difference: 0.2039337158203125\n",
      "Logit for the concept: 24.5855712890625\n",
      "Feature 44834 has the largest logit difference: 0.201507568359375\n",
      "Logit for the concept: 24.587997436523438\n",
      "Feature 34985 has the largest logit difference: 0.20105552673339844\n",
      "Logit for the concept: 24.588449478149414\n",
      "--- [Seed 42] Running Transcoder Baseline for L0 ---\n",
      "Activations captured.\n",
      "--- [Seed 42] Running Zero Ablation for L0 ---\n",
      "--- [Seed 42] Running Zero Ablation for L0 ---\n",
      "Zero Ablation Done.\n",
      "--- [Seed 42] Running Gaussian Noise Ablation for L0 (10 samples) ---\n",
      "GN Ablation Done.\n",
      "Ablation Suite Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chemistry': 0.22205208241939545,\n",
       " 'research tools': 0.21729044616222382,\n",
       " 'science': 0.10595043003559113,\n",
       " 'science equipment': 0.08035141229629517,\n",
       " 'reaction': 0.039525289088487625,\n",
       " 'chemical': 0.03738505020737648,\n",
       " 'ethicality': 0.03645625337958336,\n",
       " 'reagent': 0.03261404111981392,\n",
       " 'titration': 0.02472684159874916,\n",
       " 'experimentation': 0.017969492822885513,\n",
       " 'test tube': 0.017690295353531837,\n",
       " 'research': 0.013601364567875862,\n",
       " 'distillation': 0.012843393720686436,\n",
       " 'laboratory bottle': 0.012749548070132732,\n",
       " 'experiment': 0.012206331826746464,\n",
       " 'flask': 0.010572710074484348,\n",
       " 'laboratory': 0.007793701719492674,\n",
       " 'equivalence': 0.007493027951568365,\n",
       " 'graphic': 0.007045052479952574,\n",
       " 'solution': 0.006237167399376631}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the top 20 logit difference for 100\n",
    "runner.analyze_feature_impact(0, 42, 1, 20)\n",
    "top_k_logit_diff, top_k_feat_ids = runner.analyze_single_concept(\n",
    "    layer_idx=0,\n",
    "    seed=42,\n",
    "    concept_name='laboratory bottle',\n",
    "    top_k_features=20,\n",
    "    mode='zero',\n",
    ")\n",
    "\n",
    "# ablate the top k features to see if the concept is still there\n",
    "runner.run_ablation_suite(\n",
    "    layer_idx=0,\n",
    "    ablation_feature_ids=top_k_feat_ids,\n",
    "    seed=42,\n",
    "    n_gn_samples=10,\n",
    "    sigma=2.0,\n",
    "    batch_mode=False,\n",
    ")\n",
    "runner.results['zero_ablation_L0_seed42_all_features']['all']['text_probs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d912823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['original_clip_seed42', 'transcoder_L1_TC1_7_seed42', 'zero_abl_L1_TC1_7_seed42_each_feature', 'gn_abl_L1_TC1_7_seed42_each_feature'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212598f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base logit for the concept: 12.93919849395752\n",
      "Top 1 features with largest logit difference for flask in gn ablation:\n",
      "Feature 39603 has the largest logit difference: 0.3448143005371094\n",
      "Logit for the concept: 12.59438419342041\n"
     ]
    }
   ],
   "source": [
    "top_k_logit_diff, top_k_feat_ids = runner.analyze_single_concept(\n",
    "    layer_idx=0,\n",
    "    seed=42,\n",
    "    concept_name='flask',\n",
    "    top_k_features=1,\n",
    "    mode='gn',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb320fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing the transcoder layers for L0_9.\n",
      "--- [Seed 42] Running Transcoder Baseline for L0 ---\n",
      "Activations captured.\n",
      "--- [Seed 42] Running Zero Ablation for L0 ---\n",
      "--- [Seed 42] Running Zero Ablation for L0 ---\n",
      "Zero Ablation Done.\n",
      "--- [Seed 42] Running Gaussian Noise Ablation for L0 (10 samples) ---\n",
      "GN Ablation Done.\n",
      "Ablation Suite Done.\n"
     ]
    }
   ],
   "source": [
    "# ablate the top k features to see if the concept is still there\n",
    "runner.run_ablation_suite(\n",
    "    layer_idx=0,\n",
    "    ablation_feature_ids=top_k_feat_ids,\n",
    "    seed=42,\n",
    "    n_gn_samples=10,\n",
    "    sigma=2.0,\n",
    "    batch_mode=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2334b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_probs': {'funneled': 0.11743934452533722,\n",
       "  'knelt': 0.10808387398719788,\n",
       "  'flask': 0.09735256433486938,\n",
       "  'depersonalization': 0.05512203648686409,\n",
       "  'which': 0.040526580065488815,\n",
       "  'provided': 0.03952052816748619,\n",
       "  'graphic': 0.037601038813591,\n",
       "  'cosigner': 0.03650515526533127,\n",
       "  'handsomely': 0.03599335625767708,\n",
       "  'thyself': 0.03437698632478714,\n",
       "  'engagingness': 0.028975004330277443,\n",
       "  'biologist': 0.025070779025554657,\n",
       "  'hockey stick': 0.020171310752630234,\n",
       "  'isle': 0.0177654717117548,\n",
       "  'proficiently': 0.017592959105968475,\n",
       "  'discovery': 0.014204802922904491,\n",
       "  'commercial': 0.013600829057395458,\n",
       "  'holler': 0.01256959605962038,\n",
       "  'deflower': 0.012242713011801243,\n",
       "  'sample': 0.012179316952824593},\n",
       " 'logits': tensor([ 8.2810, -3.3493,  1.4283, 10.0345,  9.8625,  6.3111,  4.5546,  5.0874,\n",
       "         11.3745,  9.6252,  3.8613,  8.9264,  5.3220,  5.3424,  3.3106,  7.7716,\n",
       "          3.8867,  7.0685,  6.3925,  6.3779,  4.4353,  5.1685, -0.9130,  7.1718,\n",
       "         10.9988,  4.0793,  7.8634,  4.3506, 10.3084,  3.4862,  6.0614,  7.3151,\n",
       "          6.8863,  5.0792,  7.7073, -0.9699,  4.6929,  6.5333,  7.9338,  9.1041,\n",
       "          3.5762,  7.2290,  5.9986,  6.7471,  7.9127,  7.8542,  5.9870,  4.7115,\n",
       "          9.2136,  6.9835,  6.5227,  9.4617,  8.2443, 10.3872,  3.8917,  3.1945,\n",
       "          8.9448,  2.0974, -2.4313,  7.1088,  8.2514,  5.9578,  4.9296,  7.1951,\n",
       "          7.8102,  2.2464,  5.8735,  6.6383,  8.6853,  9.5480,  5.7042,  3.5728,\n",
       "          5.6744,  2.3646, -1.4043,  4.6610,  1.0537,  7.5904, 11.4790,  9.0656,\n",
       "          8.8793,  8.2244,  9.1545,  6.1541,  1.9457,  7.0855,  5.6735,  4.5342,\n",
       "          8.6151,  0.4600,  1.9050,  6.3225,  2.3569, 11.1435,  7.2418,  7.8221,\n",
       "          8.6604,  8.4152,  7.7953,  3.7825,  9.9582,  4.8300, -1.4063,  5.8180,\n",
       "          7.1289,  2.0507,  4.2148,  5.9504,  5.7595,  7.9737,  4.2691, 11.7866,\n",
       "          7.9565,  7.1925, 10.2820,  8.6348,  7.0625,  1.7739,  4.0747,  8.7587,\n",
       "          8.6271,  5.2782,  4.8592,  6.3638,  3.0233,  4.7751,  2.9260,  7.0133,\n",
       "          5.6065,  3.2228,  6.9828,  3.2209,  8.0677,  4.5378,  5.9533,  6.1320,\n",
       "          7.1134,  3.8087,  5.5619,  6.6100,  7.2788,  7.3184,  8.9686,  8.5863,\n",
       "          4.8837,  4.6692,  3.2871,  6.0255,  9.8859,  7.0665,  5.4472,  5.6354,\n",
       "          6.5187, 11.3145,  8.6757,  7.5315, 12.5430, 12.4600,  9.4536,  5.5467,\n",
       "          9.4463,  3.2283,  4.4555,  5.5600,  9.5760,  6.7055,  3.4284,  4.0071,\n",
       "          3.8883,  7.6001,  6.2129,  4.2111,  5.2393, 12.3554,  8.1451,  3.5526,\n",
       "          9.2126,  1.3716, 11.3604,  6.6102,  6.2229,  6.7829,  2.3104,  5.4708,\n",
       "          6.2785, 11.4041,  6.9493,  3.7837,  3.1622,  9.2561,  4.3516, 10.6543,\n",
       "          3.5709, 10.2768,  4.0720,  7.7323,  6.4602, 10.4307,  5.2884,  2.5640,\n",
       "          8.9520, 11.4539,  3.9078,  6.9350,  6.8560,  3.4448,  1.7230,  7.7372,\n",
       "          7.5260,  7.5475,  7.4801,  0.5273, -4.3559,  8.0476,  8.2443,  8.3916,\n",
       "          9.3791,  7.0719,  5.8376,  5.6914,  8.1672,  5.0202,  2.8330,  2.9562,\n",
       "         10.7813,  4.2987, -5.8847,  8.8862,  8.4913,  9.7088,  7.1310,  2.4572,\n",
       "          5.7998, 10.6446,  4.0096,  8.9309,  2.3811,  3.8449, -0.3255, -3.9547,\n",
       "          0.5181,  9.3369,  7.6091,  6.8418,  4.4041,  8.3797,  9.5295,  8.2557,\n",
       "          7.2643], device='cuda:0')}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.results['gn_abl_L0_TC0_9_seed42_all_features'][0]['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad8632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_probs': {'chemistry': 0.19754554331302643,\n",
       "  'laboratory bottle': 0.11423620581626892,\n",
       "  'reagent': 0.11333337426185608,\n",
       "  'chemical': 0.1004834845662117,\n",
       "  'science': 0.08708768337965012,\n",
       "  'research tools': 0.0841355100274086,\n",
       "  'reaction': 0.03261430934071541,\n",
       "  'liquid': 0.028813498094677925,\n",
       "  'titration': 0.027000539004802704,\n",
       "  'test tube': 0.025303339585661888,\n",
       "  'experiment': 0.025279028341174126,\n",
       "  'solvent': 0.023206986486911774,\n",
       "  'science equipment': 0.021823661401867867,\n",
       "  'experimentation': 0.01626974157989025,\n",
       "  'research': 0.01358125638216734,\n",
       "  'solution': 0.01234480645507574,\n",
       "  'flask': 0.010753047652542591,\n",
       "  'ethicality': 0.009960091672837734,\n",
       "  'beaker': 0.007937333546578884,\n",
       "  'vial': 0.005947170313447714},\n",
       " 'logits': tensor([15.8622, 16.0926, 17.0741, 19.1550, 14.5661, 21.7385, 14.0408, 15.9768,\n",
       "         17.7439, 22.8405, 15.5526, 15.5778, 17.3461, 14.6775,  9.9504, 15.2838,\n",
       "         19.6227, 17.4092, 23.2812, 14.7258, 19.0720, 16.1108, 13.6402, 18.2747,\n",
       "         14.3973, 11.6660, 16.9549, 21.3779, 19.6407, 17.4581, 20.9610, 15.6287,\n",
       "         18.2173, 17.2211,  2.2303,  8.0698, 17.2768,  9.4463, 15.3624, 15.2158,\n",
       "         16.5884, 16.3403,  8.6537, 23.2822,  7.6866, 15.5354, 17.8614, 16.7703,\n",
       "         12.5442, 11.9661, 18.3515, 11.5317, 13.4477, 17.5691, 15.5145, 13.7583,\n",
       "         18.3509, 19.3041, 15.6652, 21.7655, 13.2048,  4.7273, 17.9010, 16.6517,\n",
       "         15.5266, 14.3357, 18.2839, 16.5094, 18.0149, 23.5360, 23.1957, 11.4362,\n",
       "         24.4837,  8.4566, 16.3768, 18.6798, 14.5707, 16.0684, 17.7799, 13.4689,\n",
       "         23.1342, 14.8589, 15.9643, 18.4037, 16.8794, 15.6796, 17.7370, 13.4186,\n",
       "         19.1078, 15.2036,  9.6491, 12.6499, 11.9311, 16.9233,  9.6685, 14.6941,\n",
       "         20.3304, 15.6284, 22.3498, 21.8342, 12.7339, 18.2768, 11.4677, 13.7183,\n",
       "         16.9427, 16.5122, 13.2613, 23.4121, 19.1677, 14.9172, 17.6473, 22.6599,\n",
       "         23.3471, 14.3645, 14.7109, 11.6542, 15.8094, 15.6117,  8.6949, 15.2219,\n",
       "         15.9072, 14.1892, 16.3844, 19.9213, 15.2749, 17.1164, 19.0680, 19.2382,\n",
       "         16.7210, 15.1897, 16.7630, 12.7805, 15.1122,  8.3658, 17.6716, 14.6539,\n",
       "         17.6542, 20.8894, 18.2825, 10.7614, 13.3737, 19.4758, 12.5276, 17.7235,\n",
       "         17.5759, 16.2961, 10.3518, 16.8382, 13.9518, 19.4453,  6.8417, 16.2613,\n",
       "         15.8520, 17.7906, 17.3823,  6.1325, 14.1365, 17.5353, 11.6566,  8.8967,\n",
       "         18.0789, 16.0078, 16.2444,  7.6766, 19.0149, 10.7787,  9.7336, 17.4985,\n",
       "         15.4074, 17.6430, 15.5887, 22.4264, 10.4517, 15.4787, 24.5182, 11.9015,\n",
       "         13.6551,  8.9449, 25.3372, 18.7437, 13.7117, 15.9706, 16.8997, 17.4569,\n",
       "         10.1349, 14.5138,  8.7378, 16.6279, 16.3207, 15.4687, 16.7110, 19.5381,\n",
       "          9.4957, 24.6612, 18.8710, 20.1993, 22.5645,  6.4070, 15.6593, 12.6813,\n",
       "         13.5729, 17.1385, 12.6881,  9.4985, 18.4493, 11.9176, 17.7104, 14.5137,\n",
       "         22.1228, 14.9488, 15.3208, 17.9518, 11.0697, 18.6048, 20.0302, 13.0543,\n",
       "         24.7816, 11.6375, 15.4823, 14.2920, 14.4302, 18.5463, 19.1797, 19.0386,\n",
       "         15.5287, 13.8683, 15.9742, 11.7547, 13.1519, 15.8052, 17.3400, 21.0878,\n",
       "         20.0535, 16.2287, 12.2421, 17.6979, 24.7895, 18.0002, 17.7206, 18.0834,\n",
       "         15.2775, 16.8457, 17.6084, 16.5382, 15.6750, 12.0251, 18.0116, 15.3097,\n",
       "         11.1895], device='cuda:0')}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.results['transcoder_L0_seed42']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7674c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find if 'chemistry' is in the value of the keys\n",
    "# zero noise\n",
    "for i in range(len(runner.results['zero_ablation_L0_seed42_all_features'])):\n",
    "    for key, value in runner.results[\n",
    "        'zero_ablation_L0_seed42_all_features'\n",
    "    ].items():\n",
    "        if 'test tube' not in value['text_probs']:\n",
    "            print(value)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceebeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_probs': {'uptempo': 0.24299997091293335, 'chemistry': 0.07799568772315979, 'experimentation': 0.06506412476301193, 'research tools': 0.05123889446258545, 'equivalence': 0.04986099898815155, 'experiment': 0.0488385334610939, 'chemical': 0.048580531030893326, 'reagent': 0.03278970345854759, 'science': 0.02553742378950119, 'reaction': 0.024055713787674904, 'ethicality': 0.020280370488762856, 'graphic': 0.018735643476247787, 'laboratory bottle': 0.016565052792429924, 'handbrake': 0.013377712108194828, 'candidness': 0.013151132501661777, 'solvent': 0.01241375133395195, 'depersonalization': 0.011682567186653614, 'solution': 0.010908620432019234, 'acid': 0.010446690022945404, 'science equipment': 0.00900715496391058}, 'logits': tensor([14.9806, 17.8880, 17.5399, 19.9343, 16.8668, 18.6753, 13.7478, 16.4352,\n",
      "        17.3844, 22.0673, 14.9331, 16.2992, 18.6636, 14.7711,  9.8930, 14.3346,\n",
      "        18.7606, 17.1138, 21.7804, 17.6872, 18.4524, 15.3411, 16.3602, 17.6951,\n",
      "        14.5467,  9.5792, 16.8061, 18.2492, 19.3811, 17.0300, 20.8223, 17.7260,\n",
      "        18.6087, 17.7516,  3.1595,  8.0222, 17.1192, 11.0346, 14.4962, 14.2069,\n",
      "        17.3944, 15.5952, 13.3622, 19.4912,  7.0716, 16.8358, 16.0521, 17.5357,\n",
      "        12.5279, 12.1400, 18.4079, 14.4174, 11.2159, 17.5251, 15.4931, 12.5792,\n",
      "        18.9072, 17.0839, 16.5268, 20.2382, 15.6443,  3.6462, 17.4922, 16.2820,\n",
      "        14.9007, 14.9870, 16.8916, 15.4674, 17.6984, 21.0723, 20.4107, 11.1063,\n",
      "        21.8284,  8.7236, 15.0416, 19.0396, 15.9756, 16.2781, 17.7392, 15.7665,\n",
      "        20.0899, 14.0879, 15.0374, 18.3797, 16.4364, 17.4170, 16.6333, 13.2324,\n",
      "        17.2241, 20.4855, 10.9264, 19.3803, 12.4872, 18.9144,  9.9332, 15.6958,\n",
      "        17.9634, 15.1309, 20.9015, 19.3975, 14.1923, 19.3543, 12.6932, 14.0803,\n",
      "        16.9782, 16.8716, 13.0815, 19.9831, 20.4684, 16.1432, 20.3500, 19.8230,\n",
      "        19.6142, 17.0609, 15.0286, 15.0249, 16.6768, 16.7815, 13.3587, 16.8809,\n",
      "        18.2652, 15.3635, 15.8377, 16.0678, 16.0666, 16.7049, 17.6111, 18.4636,\n",
      "        18.5611, 16.2943, 18.3113, 13.4271, 19.1433,  5.6780, 18.4463, 16.9932,\n",
      "        16.2123, 19.3095, 18.3215, 10.5836, 13.2685, 19.8229, 13.9379, 17.7640,\n",
      "        18.3016, 16.7973, 11.4326, 17.3078, 13.9778, 18.9927, 10.4614, 15.7961,\n",
      "        16.0853, 15.8594, 15.7107,  5.3510, 16.3283, 16.1771, 12.7802, 15.1841,\n",
      "        17.4168, 16.3900, 16.0404,  8.2522, 18.5916, 10.9240, 12.3023, 18.0992,\n",
      "        17.0879, 18.3584, 18.1597, 18.7404, 13.2013, 17.4039, 21.1320, 11.2940,\n",
      "        15.5395, 12.7160, 22.2485, 18.3688, 14.6859, 16.0459, 15.6893, 18.6635,\n",
      "        10.7046, 14.8711,  9.2202, 16.6452, 17.4416, 15.4119, 16.4551, 18.5933,\n",
      "        10.5982, 21.7751, 17.2005, 18.2471, 20.2814,  7.7864, 16.3832, 14.0025,\n",
      "        14.5847, 19.2001, 14.5334, 11.7076, 23.3849, 16.2382, 17.5962, 13.8121,\n",
      "        17.2621, 14.7327, 17.0636, 17.8389, 11.3786, 19.0857, 19.4354, 13.9531,\n",
      "        21.3820, 10.3678, 16.9606, 14.9389, 13.6975, 19.3229, 18.5181, 19.1714,\n",
      "        16.5307, 14.5118, 16.2341, 12.1899, 13.2071, 15.6960, 18.4924, 21.8011,\n",
      "        18.4445, 16.1468, 13.3750, 16.2108, 20.6992, 19.4784, 16.9419, 17.5898,\n",
      "        16.3211, 16.8772, 18.1171, 16.5497, 16.8864, 13.9106, 19.9470, 14.7287,\n",
      "        11.2480], device='cuda:0')}\n",
      "{'text_probs': {'uptempo': 0.4043991267681122, 'experimentation': 0.07936751842498779, 'graphic': 0.04981882497668266, 'equivalence': 0.029225653037428856, 'chemistry': 0.02872857078909874, 'doc': 0.02766670100390911, 'science': 0.0272977352142334, 'ethicality': 0.019224314019083977, 'experiment': 0.017001386731863022, 'research tools': 0.013452897779643536, 'chemical': 0.012301095761358738, 'reaction': 0.011059785261750221, 'sporting': 0.01072249747812748, 'urgency': 0.009847572073340416, 'solution': 0.009495330974459648, 'reagent': 0.009068149141967297, 'garmentless': 0.00858103297650814, 'candidness': 0.008140893653035164, 'depersonalization': 0.007689038757234812, 'electrostatics': 0.0073179816827178}, 'logits': tensor([16.3614, 18.6059, 17.4052, 21.2069, 17.8341, 17.6808, 15.8711, 16.7777,\n",
      "        18.0038, 22.2608, 16.4067, 16.8591, 18.8644, 16.0471,  9.9881, 14.4011,\n",
      "        18.7784, 17.4163, 20.7200, 18.8590, 17.8843, 15.9785, 16.0739, 18.3541,\n",
      "        14.3808, 10.1206, 16.7690, 18.3281, 18.7791, 17.4976, 21.7951, 18.0897,\n",
      "        19.3221, 18.7861,  3.6606,  9.3371, 16.5183,  9.8753, 14.5950, 16.8929,\n",
      "        18.9013, 15.9947, 11.8834, 18.7131,  7.6143, 16.3265, 17.3834, 17.7269,\n",
      "        13.3232, 13.7104, 17.9201, 15.0089, 11.9612, 18.1760, 16.1186, 12.1267,\n",
      "        19.4358, 17.8864, 16.3805, 19.6194, 15.5223,  6.1318, 17.9200, 17.5694,\n",
      "        16.2387, 15.4788, 17.3284, 14.8578, 18.5642, 20.2900, 19.1343, 11.0405,\n",
      "        20.4859,  8.8684, 16.2684, 19.2114, 16.2451, 17.8005, 18.1544, 15.8883,\n",
      "        19.5177, 13.9269, 16.9775, 18.6457, 17.5589, 16.8291, 17.7385, 13.3370,\n",
      "        16.9824, 19.5411, 11.6095, 16.7107, 14.9178, 19.0111,  9.8273, 16.2258,\n",
      "        16.9589, 16.2440, 20.8429, 18.7991, 16.3882, 20.1739, 14.9472, 14.4963,\n",
      "        17.5797, 16.2599, 12.9691, 19.7526, 19.9836, 15.0602, 19.9265, 19.7345,\n",
      "        18.2091, 16.9446, 15.8210, 14.8016, 17.0450, 16.7647, 14.2752, 15.9330,\n",
      "        19.1384, 15.1171, 15.4169, 15.9181, 15.6558, 17.9955, 18.6890, 18.4888,\n",
      "        17.7150, 16.5477, 19.7194, 14.7057, 20.2590,  5.4137, 18.3561, 17.2646,\n",
      "        17.2103, 17.9395, 18.2999, 11.6358, 14.4159, 20.0363, 13.5523, 18.2137,\n",
      "        17.9339, 17.3696, 12.3916, 17.8953, 14.6842, 19.6910, 10.2292, 16.6037,\n",
      "        17.4786, 16.6084, 16.6848,  4.9934, 16.2130, 16.2305, 11.5996, 16.2644,\n",
      "        17.1080, 16.5960, 14.2829, 10.5486, 18.8447, 11.5719, 11.2020, 18.4655,\n",
      "        16.0126, 17.6695, 18.8867, 19.3127, 15.3944, 17.4725, 21.1935, 11.4208,\n",
      "        16.0009, 13.8631, 21.2446, 17.3678, 15.6780, 15.7061, 16.0572, 19.1479,\n",
      "        11.8185, 14.2814, 10.2084, 17.2921, 17.6182, 15.6130, 17.0687, 18.3486,\n",
      "        10.5419, 20.3964, 18.4684, 17.3536, 20.1375,  8.3427, 16.9207, 13.9488,\n",
      "        12.6975, 18.3519, 14.6169, 11.8719, 23.8891, 14.7968, 18.0159, 14.9323,\n",
      "        17.6635, 15.8739, 15.8201, 17.9392, 10.7355, 19.6427, 19.3476, 14.8346,\n",
      "        20.0915, 10.5574, 16.5764, 14.9575, 14.5222, 19.0802, 18.7341, 19.8770,\n",
      "        16.4638, 15.2927, 16.9963, 12.5533, 12.8591, 16.9841, 18.3863, 21.2618,\n",
      "        19.1978, 15.8200, 14.2903, 16.1097, 19.7194, 19.3408, 16.7326, 16.8276,\n",
      "        15.9679, 17.9417, 18.5484, 16.8311, 16.8712, 15.0861, 18.0970, 15.0710,\n",
      "        10.6507], device='cuda:0')}\n",
      "{'text_probs': {'chemistry': 0.1553565114736557, 'chemical': 0.11393887549638748, 'science': 0.09212437272071838, 'reagent': 0.08571767061948776, 'laboratory bottle': 0.07567501068115234, 'experimentation': 0.05695419758558273, 'experiment': 0.055755965411663055, 'reaction': 0.04842701554298401, 'research tools': 0.034952133893966675, 'solvent': 0.034300751984119415, 'liquid': 0.03308936208486557, 'titration': 0.027994178235530853, 'acid': 0.017271863296628, 'research': 0.014940624125301838, 'solution': 0.014608952216804028, 'flask': 0.012206423096358776, 'science equipment': 0.011825320310890675, 'ethicality': 0.010831466875970364, 'beaker': 0.010622432455420494, 'graphic': 0.010010459460318089}, 'logits': tensor([16.9630, 17.3167, 17.5013, 20.1511, 15.9999, 20.7251, 15.1318, 16.5294,\n",
      "        18.4490, 23.5107, 15.4771, 16.9707, 17.7272, 15.3317, 10.6880, 15.4241,\n",
      "        19.4023, 18.0906, 23.4895, 16.4370, 19.5472, 17.3397, 15.1954, 18.5007,\n",
      "        14.6854, 12.5944, 18.0536, 21.7034, 19.4318, 17.3220, 21.7721, 16.2273,\n",
      "        18.6059, 18.5918,  3.3342,  9.2867, 17.5360,  9.9630, 15.0766, 15.7047,\n",
      "        18.7023, 17.3263, 10.0688, 21.2873,  8.4805, 15.5159, 18.2878, 17.5971,\n",
      "        13.6081, 13.0870, 18.4451, 14.7564, 12.3904, 19.4043, 15.4865, 12.9973,\n",
      "        18.8830, 19.3433, 16.4919, 22.3175, 15.2249,  5.9947, 17.9852, 17.1471,\n",
      "        16.4242, 15.2890, 18.7119, 17.4784, 18.2565, 23.3485, 23.0036, 12.3149,\n",
      "        23.0224,  9.0494, 18.0634, 19.3053, 16.3336, 17.1236, 18.8562, 15.1787,\n",
      "        21.9387, 15.0951, 16.8843, 18.6241, 16.8902, 16.1610, 17.8118, 14.5351,\n",
      "        19.5579, 17.1652, 11.4701, 14.4117, 14.2606, 17.4961,  9.8956, 15.8033,\n",
      "        20.0819, 16.7339, 21.8509, 20.6651, 14.6675, 19.2662, 13.3647, 15.0190,\n",
      "        17.2080, 17.4033, 13.2908, 22.9677, 19.7993, 15.8553, 18.4399, 22.1726,\n",
      "        22.8005, 15.7674, 15.4372, 12.8822, 16.3729, 16.9268, 10.8551, 16.4427,\n",
      "        17.4340, 14.6258, 16.4218, 18.2530, 15.9387, 17.5859, 19.2083, 18.9304,\n",
      "        17.7064, 16.2099, 17.8897, 14.7665, 16.0388,  8.9917, 18.3781, 16.6540,\n",
      "        18.2584, 20.3041, 18.7819, 11.6364, 14.6139, 19.9213, 13.2689, 18.1982,\n",
      "        17.5999, 17.2175, 10.8175, 18.8273, 14.8595, 19.6289,  8.9000, 16.4767,\n",
      "        18.0607, 17.7323, 18.2152,  6.0270, 16.0837, 17.5349, 12.1060, 11.7058,\n",
      "        19.3001, 16.4666, 16.6738, 10.4532, 18.9440, 12.6947, 11.1829, 18.1780,\n",
      "        16.1298, 18.9154, 17.6336, 21.9704, 13.0999, 16.7824, 23.9916, 12.7066,\n",
      "        15.2239, 11.6730, 24.5142, 19.1766, 14.8069, 15.9505, 17.3821, 18.9234,\n",
      "        10.6505, 15.3906,  9.1934, 16.9949, 17.0657, 15.8537, 16.8906, 19.5840,\n",
      "        10.8316, 24.2041, 19.2614, 19.4592, 22.1501,  7.6677, 16.2291, 13.8830,\n",
      "        14.1196, 17.7659, 13.8027, 10.6063, 19.9875, 13.0568, 18.5031, 14.7020,\n",
      "        21.8314, 15.6943, 15.6829, 18.3739, 12.4939, 19.4871, 20.1315, 14.2208,\n",
      "        23.9195, 12.3710, 16.0798, 15.5514, 14.5199, 18.6325, 19.3367, 19.6325,\n",
      "        15.9303, 15.3350, 16.3968, 12.8298, 13.4457, 16.6871, 18.2639, 21.5122,\n",
      "        20.1026, 16.5264, 12.1239, 17.5935, 23.7949, 19.1159, 16.8809, 17.9571,\n",
      "        15.7969, 17.4873, 18.4049, 17.1464, 16.1903, 13.5984, 18.3383, 15.4686,\n",
      "        12.2078], device='cuda:0')}\n",
      "{'text_probs': {'chemical': 0.11776410043239594, 'chemistry': 0.10944458842277527, 'laboratory bottle': 0.09891493618488312, 'reagent': 0.09588664025068283, 'experimentation': 0.0664893314242363, 'solvent': 0.041518062353134155, 'science': 0.03615521267056465, 'experiment': 0.03547849506139755, 'research tools': 0.031970154494047165, 'titration': 0.031410932540893555, 'liquid': 0.031162861734628677, 'reaction': 0.03060433454811573, 'acid': 0.025592396035790443, 'graphic': 0.01818927563726902, 'ethicality': 0.01616492122411728, 'flask': 0.01595734991133213, 'uptempo': 0.015471319667994976, 'solution': 0.01431654766201973, 'equivalence': 0.01131670642644167, 'research': 0.009340918622910976}, 'logits': tensor([16.1831, 17.6629, 16.9968, 20.0320, 16.5933, 19.2627, 14.3663, 16.6272,\n",
      "        18.4359, 22.7411, 16.0152, 17.3425, 18.8486, 16.1765, 10.7237, 15.3850,\n",
      "        19.1450, 17.5538, 22.1130, 17.1778, 18.5779, 16.8749, 16.4821, 18.4366,\n",
      "        14.4445, 11.0186, 18.2680, 20.5879, 19.1112, 17.8736, 21.4449, 17.5152,\n",
      "        18.3061, 18.5461,  4.4496,  9.6022, 17.3363, 11.0195, 15.0690, 14.3759,\n",
      "        17.7144, 16.3406, 10.9031, 20.0226,  7.9356, 16.4689, 16.4363, 17.7125,\n",
      "        14.1112, 12.6780, 18.7961, 15.0062, 11.4135, 17.6262, 15.3012, 11.5788,\n",
      "        18.6105, 17.6280, 17.5159, 21.7864, 16.4952,  6.2842, 17.3965, 16.5705,\n",
      "        15.9894, 14.9684, 17.5435, 16.7926, 18.1370, 21.9652, 22.2702, 12.3725,\n",
      "        22.0089,  9.6771, 16.7336, 19.3786, 17.1352, 16.7577, 19.7624, 15.2856,\n",
      "        20.3430, 14.4084, 16.8567, 18.1404, 16.8482, 16.5371, 17.4263, 13.9418,\n",
      "        19.1114, 18.6370, 11.7670, 16.9477, 13.6132, 17.9681,  9.6345, 16.0513,\n",
      "        18.2919, 15.4267, 21.3269, 20.5452, 14.9234, 19.3247, 12.8501, 15.2430,\n",
      "        17.5360, 17.6182, 13.1311, 21.9833, 20.2671, 17.5498, 19.2085, 20.7785,\n",
      "        21.9913, 16.9686, 15.2320, 14.4804, 17.0506, 17.4541, 12.3117, 17.0487,\n",
      "        18.6798, 16.9242, 17.8046, 16.8819, 15.9975, 17.9490, 18.2494, 18.0671,\n",
      "        18.1702, 16.3186, 17.5075, 14.7993, 16.8430,  6.9273, 18.8594, 17.2317,\n",
      "        17.7385, 20.4909, 19.0356, 12.1542, 14.9797, 19.7702, 13.4223, 18.1029,\n",
      "        18.0085, 16.9471, 12.1508, 18.5972, 15.8928, 19.2733,  9.4902, 15.5796,\n",
      "        16.5481, 16.7933, 17.5151,  6.6944, 16.7250, 16.0018, 13.0727, 13.0199,\n",
      "        17.9652, 17.0525, 17.5153, 10.4802, 18.9540, 12.1166, 12.1705, 18.5443,\n",
      "        17.3500, 19.1826, 17.7345, 21.3140, 14.6534, 16.7394, 22.1319, 11.0859,\n",
      "        16.0905, 12.7472, 23.2395, 18.8484, 14.6296, 16.4705, 17.4638, 19.3846,\n",
      "        11.1056, 15.5870, 10.2338, 16.3378, 17.6101, 16.0146, 16.6123, 19.1524,\n",
      "        10.9251, 23.3128, 18.3634, 17.9514, 21.2055,  8.0656, 16.2691, 14.6816,\n",
      "        14.1347, 18.3386, 14.3958, 11.3095, 21.2831, 15.0803, 18.2679, 13.8385,\n",
      "        19.2724, 15.5218, 16.2005, 18.1150, 11.7514, 19.0763, 20.0365, 14.2971,\n",
      "        23.1073, 11.8621, 16.5626, 15.1716, 14.2453, 18.4647, 19.6791, 19.3883,\n",
      "        16.9086, 15.2906, 16.3875, 13.4050, 12.8425, 16.6493, 18.6496, 20.9704,\n",
      "        19.6812, 16.9492, 13.7551, 17.0317, 23.1384, 19.5370, 16.8770, 17.3801,\n",
      "        16.2792, 17.5892, 18.0562, 16.8363, 17.4116, 14.1281, 19.4445, 15.6966,\n",
      "        12.0102], device='cuda:0')}\n",
      "{'text_probs': {'experimentation': 0.1291201114654541, 'uptempo': 0.07011641561985016, 'graphic': 0.06142830848693848, 'chemistry': 0.05546283721923828, 'doc': 0.05282125622034073, 'experiment': 0.03872181102633476, 'reaction': 0.037876516580581665, 'laboratory bottle': 0.03658744692802429, 'science': 0.036325786262750626, 'ethicality': 0.02760469913482666, 'equivalence': 0.02575068548321724, 'chemical': 0.02569323405623436, 'candidness': 0.02568059414625168, 'research tools': 0.023344479501247406, 'solution': 0.018209295347332954, 'reagent': 0.01605132780969143, 'acid': 0.014146806672215462, 'flask': 0.013620402663946152, 'liquid': 0.012444928288459778, 'misappropriation': 0.011062374338507652}, 'logits': tensor([15.6192, 18.0119, 16.6936, 21.5545, 16.8600, 18.0212, 15.0981, 16.6642,\n",
      "        17.8068, 22.4484, 14.9655, 16.5532, 18.5162, 15.5406, 10.4539, 14.3228,\n",
      "        18.1321, 17.0965, 21.2440, 17.8170, 17.9566, 17.1739, 16.2874, 18.5637,\n",
      "        15.0710, 10.5569, 17.9115, 19.0542, 18.2429, 17.2281, 21.7055, 18.1661,\n",
      "        18.2470, 18.5874,  4.0547,  9.1641, 17.3299, 10.4429, 14.3616, 15.8662,\n",
      "        18.1458, 15.4033, 11.6340, 18.7111,  8.2583, 16.2003, 17.2262, 17.9148,\n",
      "        13.5111, 13.2499, 18.4281, 15.6536, 11.1680, 18.5861, 15.7958, 12.2251,\n",
      "        18.4850, 17.8998, 16.4908, 20.2371, 15.6158,  5.3671, 17.8874, 17.2650,\n",
      "        16.6916, 15.7399, 16.8972, 15.6721, 18.8527, 21.2219, 19.7183, 11.5637,\n",
      "        20.7380,  9.4293, 17.4311, 19.1612, 16.4565, 16.3081, 19.3170, 15.9513,\n",
      "        19.2336, 13.7679, 16.5705, 18.4897, 16.8378, 16.5136, 17.5421, 13.4810,\n",
      "        17.3958, 17.8215, 12.0515, 16.4132, 14.5362, 17.7684,  9.9093, 15.8486,\n",
      "        17.2295, 16.6959, 20.9056, 19.6803, 15.8039, 18.7466, 14.7550, 14.8865,\n",
      "        18.0269, 16.9519, 12.4912, 20.1089, 20.8333, 16.6429, 19.6695, 19.8528,\n",
      "        19.1585, 16.4366, 15.7015, 15.0671, 17.2026, 17.9665, 12.1641, 15.3348,\n",
      "        19.1807, 15.7359, 16.5142, 16.4056, 15.5259, 17.3397, 18.0804, 18.8534,\n",
      "        17.3343, 16.6975, 18.2329, 14.0564, 18.0306,  6.0312, 18.2387, 18.0274,\n",
      "        17.0238, 19.6333, 19.0528, 12.2076, 14.1059, 19.7158, 13.0168, 18.0240,\n",
      "        18.3042, 17.4206, 11.9291, 17.1880, 14.7638, 18.9728,  9.9885, 15.4612,\n",
      "        17.8078, 16.2712, 16.7243,  6.1994, 16.5238, 16.1466, 12.7472, 13.9748,\n",
      "        16.9946, 16.4114, 16.0223, 10.7032, 18.7554, 11.6356, 11.6609, 17.9342,\n",
      "        16.3479, 18.0309, 17.8317, 20.1992, 14.3143, 17.3608, 21.1801, 11.8330,\n",
      "        16.4880, 13.3516, 21.6033, 16.9419, 15.2797, 15.5470, 16.1413, 18.8631,\n",
      "        11.9685, 14.8622, 10.5417, 16.8432, 17.8579, 15.9220, 16.9300, 18.5737,\n",
      "        11.1245, 20.8338, 18.3067, 17.6267, 20.4895,  8.2163, 16.1772, 13.7144,\n",
      "        13.4783, 18.9012, 13.8559, 12.1490, 21.8378, 15.0537, 18.3918, 15.8838,\n",
      "        18.3086, 15.7720, 15.8508, 18.0903, 12.2771, 19.4883, 19.9527, 14.9598,\n",
      "        20.3634, 10.9964, 16.3256, 15.7747, 13.8660, 19.1140, 18.6266, 19.5788,\n",
      "        16.6412, 15.0895, 16.8070, 13.3106, 12.3326, 17.1703, 18.5971, 20.8361,\n",
      "        18.9652, 16.2157, 14.2174, 15.7054, 21.1873, 19.9912, 16.6607, 17.1999,\n",
      "        15.5407, 17.8473, 18.5602, 16.9793, 17.1937, 14.8394, 18.2052, 14.6196,\n",
      "        11.6417], device='cuda:0')}\n",
      "{'text_probs': {'uptempo': 0.30298781394958496, 'research tools': 0.13427986204624176, 'chemistry': 0.04954921454191208, 'experimentation': 0.04836108162999153, 'doc': 0.03877519071102142, 'science': 0.036550913006067276, 'ethicality': 0.02825108915567398, 'science equipment': 0.0271152276545763, 'equivalence': 0.023928603157401085, 'chemical': 0.022902879863977432, 'experiment': 0.020448800176382065, 'graphic': 0.017245812341570854, 'reagent': 0.013150863349437714, 'apparatus': 0.01280669029802084, 'laboratory bottle': 0.012094064615666866, 'research': 0.009545430541038513, 'reaction': 0.008042121306061745, 'candidness': 0.007668520323932171, 'hardware': 0.007146551739424467, 'garmentless': 0.007092573679983616}, 'logits': tensor([15.2881, 17.8105, 16.6844, 21.2413, 17.2717, 18.3634, 13.6536, 16.7174,\n",
      "        16.5817, 21.4622, 15.1429, 16.2436, 19.5501, 15.6121,  9.7971, 14.1049,\n",
      "        18.6189, 16.4304, 20.6014, 17.2490, 17.9247, 14.8183, 15.4494, 17.1911,\n",
      "        13.6612,  9.5285, 16.0598, 17.9488, 20.1335, 16.7274, 20.4311, 17.0538,\n",
      "        18.6071, 17.4277,  2.8722,  8.1824, 16.1577,  9.8911, 13.6898, 15.7530,\n",
      "        17.8661, 14.6299, 11.6873, 18.8813,  7.2947, 17.3199, 17.0699, 17.4095,\n",
      "        11.7763, 11.4815, 17.4304, 13.9711, 10.7838, 18.2885, 15.8680, 11.5820,\n",
      "        18.9900, 17.7264, 16.4703, 19.2452, 15.2241,  5.2989, 17.1997, 16.4877,\n",
      "        15.2927, 14.9870, 16.7642, 14.1215, 17.9578, 19.6682, 18.7782, 10.5567,\n",
      "        22.4834,  8.8785, 14.9875, 18.8357, 15.4723, 16.6415, 17.5253, 14.8344,\n",
      "        20.8836, 12.5457, 15.4013, 19.1209, 16.2479, 16.3624, 16.7502, 11.6568,\n",
      "        15.9445, 19.0070, 11.0731, 16.1191, 11.7605, 18.5081,  9.9300, 15.8498,\n",
      "        17.4430, 15.6657, 20.9246, 18.2156, 14.1140, 19.3184, 12.9591, 13.3586,\n",
      "        16.9229, 15.6219, 12.5237, 18.6346, 19.6206, 15.1775, 19.3820, 19.8396,\n",
      "        18.9179, 16.3315, 14.2483, 14.2325, 16.5351, 16.2215, 12.6566, 15.5743,\n",
      "        17.9627, 14.0423, 15.3664, 15.8014, 14.5554, 18.1488, 18.1799, 17.5738,\n",
      "        17.4694, 15.3918, 18.8470, 12.5733, 19.0814,  5.2796, 17.4581, 16.4984,\n",
      "        15.9952, 18.2818, 17.3984, 10.0192, 12.6838, 19.5425, 13.6859, 17.5892,\n",
      "        17.5958, 16.1854, 11.0655, 16.3047, 13.0594, 18.6778, 10.5266, 16.1934,\n",
      "        16.2860, 16.6730, 15.5870,  4.4223, 16.1314, 15.9275, 10.4338, 14.9315,\n",
      "        16.4742, 16.1007, 14.6364,  9.7578, 18.1870, 10.6223, 10.2383, 17.7026,\n",
      "        16.2491, 17.0762, 17.5549, 18.7906, 13.7902, 17.2998, 21.1822, 10.3656,\n",
      "        15.5065, 12.6233, 21.4865, 16.8593, 14.4518, 16.4026, 15.9205, 18.3736,\n",
      "        10.9491, 14.6988, 10.4209, 16.2649, 16.9451, 14.2029, 16.0002, 18.0469,\n",
      "         9.2818, 20.7148, 17.6243, 17.4675, 19.3608,  7.3694, 15.9329, 13.5194,\n",
      "        13.0444, 17.9344, 13.4528, 12.1446, 23.2972, 14.6997, 17.0588, 14.4623,\n",
      "        17.4375, 15.0543, 15.4818, 17.4986, 10.0080, 19.0522, 19.0248, 13.6774,\n",
      "        20.1600, 10.0860, 16.4148, 15.2872, 13.1412, 18.5159, 18.7813, 19.1233,\n",
      "        14.8170, 14.5076, 16.2974, 12.3318, 12.9308, 15.5570, 17.5623, 20.7586,\n",
      "        17.9200, 15.5854, 13.1957, 14.8087, 20.0762, 18.3752, 17.0931, 16.6885,\n",
      "        15.7980, 17.1679, 17.5954, 16.4044, 15.7982, 13.9322, 17.9040, 14.1173,\n",
      "        10.4363], device='cuda:0')}\n",
      "{'text_probs': {'laboratory bottle': 0.14175958931446075, 'chemistry': 0.09432747960090637, 'experimentation': 0.09116099774837494, 'chemical': 0.05607682093977928, 'reagent': 0.05427154153585434, 'candidness': 0.039238832890987396, 'experiment': 0.03624585270881653, 'reaction': 0.033753156661987305, 'science': 0.03300977870821953, 'graphic': 0.030205510556697845, 'research tools': 0.02330426685512066, 'solvent': 0.021794714033603668, 'solution': 0.021584395319223404, 'liquid': 0.01991039514541626, 'acid': 0.016907470300793648, 'ethicality': 0.016285888850688934, 'glassware': 0.01515604555606842, 'uptempo': 0.01454041339457035, 'flask': 0.013724062591791153, 'antisocial': 0.013108187355101109}, 'logits': tensor([15.6571, 17.2162, 16.6745, 20.1292, 17.1048, 18.5207, 14.8714, 16.7442,\n",
      "        17.6874, 22.6385, 15.4940, 16.7716, 18.9110, 15.7805,  9.9923, 15.3573,\n",
      "        18.4392, 17.3685, 21.7162, 17.3048, 18.3855, 16.3991, 16.2663, 18.4734,\n",
      "        14.6097, 10.1254, 17.6859, 19.4526, 18.2269, 18.6916, 21.5339, 17.7607,\n",
      "        17.8121, 19.3903,  4.9044,  9.7760, 17.8042, 11.3611, 14.5241, 14.7339,\n",
      "        17.0018, 15.7010, 10.7891, 19.8106,  7.0261, 16.5430, 16.4381, 17.8333,\n",
      "        13.8956, 12.9475, 18.9094, 14.4955, 11.3394, 17.3678, 16.0341, 12.4532,\n",
      "        18.7651, 17.8100, 17.3001, 20.9536, 16.1239,  5.5236, 17.5941, 17.1662,\n",
      "        16.0067, 15.4509, 17.3529, 15.8626, 17.9597, 21.6450, 21.2075, 12.0975,\n",
      "        21.2745,  9.7586, 16.7349, 19.2009, 16.6022, 16.0449, 19.1970, 15.3826,\n",
      "        19.6280, 14.6009, 16.7741, 19.3236, 16.9827, 17.3719, 17.3968, 12.8894,\n",
      "        18.7144, 18.8625, 12.4877, 17.1895, 14.5482, 18.0761, 10.1111, 16.2092,\n",
      "        17.1550, 16.2790, 20.9162, 20.4730, 16.1642, 19.0669, 13.6171, 15.8769,\n",
      "        17.4857, 16.5111, 12.8985, 21.1171, 21.7955, 17.6502, 19.7699, 20.4121,\n",
      "        20.0936, 17.0560, 16.2087, 14.6568, 17.1906, 17.7948, 11.7314, 15.8764,\n",
      "        18.8980, 17.6098, 18.2242, 16.9820, 16.3568, 17.6307, 18.3063, 18.4087,\n",
      "        17.7205, 16.6419, 17.9199, 15.1455, 17.4153,  6.1038, 18.5531, 17.0503,\n",
      "        17.7128, 20.8443, 19.2829, 12.4403, 14.8008, 19.8734, 13.3156, 18.6078,\n",
      "        18.6756, 17.0990, 12.8812, 17.6110, 15.7402, 18.9325,  9.6245, 15.8373,\n",
      "        16.6435, 16.3946, 16.7838,  6.6776, 17.1621, 15.6044, 12.9224, 13.5802,\n",
      "        16.9101, 16.4669, 17.8893, 10.1200, 19.0572, 11.8840, 12.8110, 18.4970,\n",
      "        17.5631, 18.4692, 18.3025, 20.7450, 14.0816, 17.1388, 21.6227, 11.7016,\n",
      "        15.9798, 12.7352, 22.6727, 17.8396, 15.0744, 16.3871, 16.8862, 19.5846,\n",
      "        12.2734, 14.9848, 10.3297, 16.9908, 18.0077, 16.6771, 16.9597, 18.8330,\n",
      "        10.1362, 22.1526, 18.3356, 18.3974, 21.1979,  8.3165, 16.2468, 13.7288,\n",
      "        13.6643, 19.7656, 13.9933, 11.3394, 20.8028, 15.0218, 18.6903, 15.5430,\n",
      "        19.0156, 15.8298, 16.0194, 18.5396, 11.8634, 18.4302, 20.6991, 14.4683,\n",
      "        22.1199, 12.8152, 17.0121, 14.3347, 14.8033, 18.9802, 19.5269, 19.2367,\n",
      "        17.8347, 14.4778, 16.2496, 12.9797, 13.5008, 17.4871, 18.6957, 20.6565,\n",
      "        19.1304, 17.2724, 13.9868, 16.7634, 23.0800, 20.0892, 16.7748, 17.8826,\n",
      "        16.0240, 17.5287, 18.4233, 16.5658, 17.3944, 14.9327, 19.1904, 15.3184,\n",
      "        11.6404], device='cuda:0')}\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# gaussian noise\n",
    "no_target_found = 0\n",
    "for i in range(len(runner.results['gn_ablation_L0_seed42_all_features'])):\n",
    "    for key, value in runner.results['gn_ablation_L0_seed42_all_features'][\n",
    "        i\n",
    "    ].items():\n",
    "        if 'test tube' not in value['text_probs']:\n",
    "            print(value)\n",
    "            no_target_found += 1\n",
    "print(no_target_found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de4053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layers transcoders replacement\n",
    "transcoder_l1 = tc_list[1]\n",
    "transcoder_l7 = tc_list[7]\n",
    "\n",
    "runner_2_layers = AblationExperimentRunner(\n",
    "    model=hookedsaevit,\n",
    "    transcoders=[transcoder_l1, transcoder_l7],\n",
    "    img_tensor=img_tensor,\n",
    "    text_features=text_features,\n",
    "    labels=final_labels,\n",
    "    device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc82c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prisma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
