{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8de633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHONPATH added: /nfs/turbo/coe-chaijy/janeding/regrounding/clip_tl\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "PROJECT_ROOT = '/nfs/turbo/coe-chaijy/janeding/regrounding/clip_tl'\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "print('PYTHONPATH added:', sys.path[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15134d5b",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576b7349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janeding/miniconda3/envs/prisma/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vit_prisma.utils.constants import DEVICE\n",
    "from vit_prisma.utils.tutorial_utils import (\n",
    "    plot_image,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f59da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# clear cache\n",
    "torch.cuda.empty_cache()\n",
    "# force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "DEVICE = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14989b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcoder list\n",
    "from src.analysis.utils import *\n",
    "\n",
    "tc_list = load_all_tc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721360c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HookedViT\n",
    "from vit_prisma.models.model_loader import load_hooked_model\n",
    "from vit_prisma.utils.enums import ModelType\n",
    "\n",
    "model_name = tc_list[0].cfg.model_name\n",
    "hookedvit = load_hooked_model(model_name, model_type=ModelType.VISION)\n",
    "hookedvit = hookedvit.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa7f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check: predict label to be 'chemistry'\n",
    "# load the labels of a big word list for the check, but not the later ablation\n",
    "import pandas as pd\n",
    "\n",
    "file_path = '/nfs/turbo/coe-chaijy/janeding/regrounding/MyTC_bert/data/concrete_visual_vocabulary.csv'\n",
    "labels = pd.read_csv(file_path)\n",
    "\n",
    "labels = labels.dropna(subset=['word'])\n",
    "labels['word'] = labels['word'].astype(str).str.lower().str.strip()\n",
    "labels = labels[labels['word'].ne('')]\n",
    "labels = labels[labels['word'].ne('nan')]\n",
    "\n",
    "# turn into dict\n",
    "label_dict = {\n",
    "    row['word']: {\n",
    "        'concreteness': row['concreteness'],\n",
    "        'imageability': row['imageability'],\n",
    "    }\n",
    "    for _, row in labels.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CLIP text transformer encoder model\n",
    "\n",
    "import open_clip\n",
    "import torch\n",
    "from open_clip.model import text_global_pool\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def name_from_hf(model_name):\n",
    "    \"\"\"return model name from huggingface\"\"\"\n",
    "    return model_name.replace('open-clip:', 'hf-hub:')\n",
    "\n",
    "\n",
    "class OpenCLIPTextOnly(nn.Module):\n",
    "    def __init__(self, clip):\n",
    "        super().__init__()\n",
    "        # only keep the text part of the model\n",
    "        self.vocab_size = clip.vocab_size\n",
    "        self.token_embedding = clip.token_embedding\n",
    "        self.positional_embedding = clip.positional_embedding\n",
    "        self.transformer = clip.transformer\n",
    "        self.ln_final = clip.ln_final\n",
    "        self.text_projection = clip.text_projection\n",
    "        self.attn_mask = clip.attn_mask\n",
    "        self.text_pool_type = clip.text_pool_type\n",
    "        self.text_eos_id = getattr(clip, 'text_eos_id', None)\n",
    "        # self.register_buffer('attn_mask', self.attn_mask, persistent=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_text(self, text, normalize=False):\n",
    "        cast_dtype = self.transformer.get_cast_dtype()\n",
    "        x = self.token_embedding(text).to(cast_dtype)  # [B, L, D]\n",
    "        x = x + self.positional_embedding.to(cast_dtype)\n",
    "        x = self.transformer(\n",
    "            x, attn_mask=self.attn_mask\n",
    "        )  # transformer(width=D)\n",
    "        x = self.ln_final(x)\n",
    "        x = text_global_pool(x, text, self.text_pool_type)\n",
    "        if self.text_projection is not None:\n",
    "            x = (\n",
    "                self.text_projection(x)\n",
    "                if isinstance(self.text_projection, nn.Linear)\n",
    "                else x @ self.text_projection\n",
    "            )\n",
    "        return nn.functional.normalize(x, dim=-1) if normalize else x\n",
    "\n",
    "    def forward(self, text):\n",
    "        return self.encode_text(text, normalize=True)\n",
    "\n",
    "\n",
    "tmp_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    name_from_hf(tc_list[0].cfg.model_name),\n",
    ")\n",
    "tmp_model.to(DEVICE)\n",
    "tmp_model.eval()\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer(name_from_hf(tc_list[0].cfg.model_name))\n",
    "\n",
    "# free up visual tower memory (optional)\n",
    "if hasattr(tmp_model, 'visual'):\n",
    "    delattr(tmp_model, 'visual')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "text_model = OpenCLIPTextOnly(tmp_model).eval().to(DEVICE)\n",
    "# text_model.forward(input_ids) or text_model.encode_text(input_ids, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa4fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e007a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image\n",
    "\n",
    "from PIL import Image\n",
    "from vit_prisma.transforms import get_clip_val_transforms\n",
    "\n",
    "# img_path = '/nfs/turbo/coe-chaijy/janeding/example_images/smiling-face.png'\n",
    "img_path = '/nfs/turbo/coe-chaijy/janeding/regrounding/clip_tl/example_images/flask.png'\n",
    "gt_label = 'chemistry'\n",
    "img = Image.open(img_path).convert()  # Ensure it's 3 channels\n",
    "transforms = get_clip_val_transforms()\n",
    "img_tensor = transforms(img)\n",
    "plot_image(img_tensor.detach().cpu(), unstandardise=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fda556a6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import open_clip\n",
    "\n",
    "CHUNK_SIZE = 512\n",
    "\n",
    "labels = list(label_dict.keys())\n",
    "prompts = [f'a photo of a {l}' for l in labels]\n",
    "text_tokens = open_clip.tokenize(prompts)  # [N, 77] on CPU\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_input = img_tensor.unsqueeze(0).to(DEVICE)\n",
    "    vis_out, cache = hookedvit.run_with_cache(image_input)\n",
    "    image_features = vis_out.to(DEVICE)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_feats_out = []\n",
    "    for i in range(0, len(text_tokens), CHUNK_SIZE):\n",
    "        chunk = text_tokens[i : i + CHUNK_SIZE].to(DEVICE, non_blocking=True)\n",
    "        feats = text_model.encode_text(chunk)\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        text_feats_out.append(feats.cpu())\n",
    "    text_features = torch.cat(text_feats_out).to(DEVICE)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    top_probs, top_idx = text_probs.squeeze().topk(50)\n",
    "\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_idx)):\n",
    "    print(f'{i + 1}. label: {labels[idx]:<20} | prob: {prob.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a09de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try a smaller label set\n",
    "from custom_labels import final_labels\n",
    "\n",
    "# final_labels.append('cat')\n",
    "smaller_text_tokens = open_clip.tokenize(final_labels)\n",
    "\n",
    "final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 512\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_input = img_tensor.unsqueeze(0).to(DEVICE)\n",
    "    vis_out, cache = hookedvit.run_with_cache(image_input)\n",
    "    image_features = vis_out.to(DEVICE)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_feats_out = []\n",
    "    for i in range(0, len(smaller_text_tokens), CHUNK_SIZE):\n",
    "        chunk = smaller_text_tokens[i : i + CHUNK_SIZE].to(\n",
    "            DEVICE, non_blocking=True\n",
    "        )\n",
    "        feats = text_model.encode_text(chunk)\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        text_feats_out.append(feats.cpu())\n",
    "    text_features = torch.cat(text_feats_out).to(DEVICE)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    top_probs, top_idx = text_probs.squeeze().topk(50)\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_idx)):\n",
    "    print(f'{i + 1}. label: {final_labels[idx]:<20} | prob: {prob.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_features(text_model, labels, chunk_size=1024, device=DEVICE):\n",
    "    prompts = [f'a photo of a {l}' for l in labels]\n",
    "    label_tokens = open_clip.tokenize(prompts)  # [N, 77] on CPU\n",
    "\n",
    "    text_feats_out = []\n",
    "    for i in range(0, len(label_tokens), chunk_size):\n",
    "        chunk = label_tokens[i : i + chunk_size].to(device, non_blocking=True)\n",
    "        feats = text_model.encode_text(chunk)\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        text_feats_out.append(feats.cpu())\n",
    "    text_features = torch.cat(text_feats_out).to(device)\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = get_text_features(\n",
    "    text_model, final_labels, chunk_size=1024, device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadcaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(tc_list[0].cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ea40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(hookedvit.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hookedsaevit\n",
    "from vit_prisma.models.base_vit import HookedTranscoderViT\n",
    "\n",
    "hookedvit.__class__ = HookedTranscoderViT\n",
    "hookedvit.mlp_to_transcoders = {}\n",
    "hookedvit._original_mlps = {}\n",
    "\n",
    "hookedsaevit = hookedvit\n",
    "\n",
    "print(f'Original hookedvit object is now of type: {type(hookedvit)}')\n",
    "print(f'hookedsaevit object is of type: {type(hookedsaevit)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ed450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(tc_list[10].cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64883f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_img_input(\n",
    "    hooked_vis_model: HookedTranscoderViT,\n",
    "    transcoders: list[SparseAutoencoder],\n",
    "    fwd_hooks,\n",
    "    img_tensor: torch.Tensor,\n",
    "    text_features: torch.Tensor,\n",
    "    labels: list[str],\n",
    "    device=DEVICE,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        img_tensor = img_tensor.unsqueeze(0).to(DEVICE)\n",
    "        vis_out = hooked_vis_model.run_with_hooks_with_transcoders(\n",
    "            img_tensor,\n",
    "            transcoders=transcoders,\n",
    "            fwd_hooks=fwd_hooks,\n",
    "            bwd_hooks=[],\n",
    "        )\n",
    "        image_features = vis_out.to(device)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        top_probs, top_idx = text_probs.squeeze().topk(20)\n",
    "\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_idx)):\n",
    "        print(f'{i + 1}. label: {labels[idx]:<20} | prob: {prob.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_activations(\n",
    "    hooked_vis_model: HookedTranscoderViT,\n",
    "    transcoder: SparseAutoencoder,\n",
    "    img_tensor: torch.Tensor,\n",
    "    text_features: torch.Tensor,\n",
    "    labels: list[str],\n",
    "):\n",
    "    top_act_ids: dict[int, list[int]] = {}\n",
    "    hook_layer = transcoder.cfg.hook_point_layer\n",
    "    hook_point = f'blocks.{hook_layer}.mlp.hook_hidden_post'\n",
    "\n",
    "    def get_top_activations_hook(\n",
    "        feature_activations: torch.Tensor,\n",
    "        hook,\n",
    "    ):\n",
    "        nonzero_indices = torch.nonzero(feature_activations, as_tuple=False)\n",
    "        if nonzero_indices.numel() > 0:\n",
    "            feature_ids = nonzero_indices[:, -1]\n",
    "            sorted_feature_ids = sorted(feature_ids.detach().unique().tolist())\n",
    "            top_act_ids[hook_layer] = sorted_feature_ids\n",
    "        else:\n",
    "            top_act_ids[hook_layer] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_tensor = img_tensor.unsqueeze(0).to(DEVICE)\n",
    "        vis_out = hooked_vis_model.run_with_hooks_with_transcoders(\n",
    "            img_tensor,\n",
    "            transcoders=[transcoder],\n",
    "            fwd_hooks=[(hook_point, get_top_activations_hook)],\n",
    "            bwd_hooks=[],\n",
    "            reset_hooks_end=True,\n",
    "        )\n",
    "        image_features = vis_out.to(img_tensor.device)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        top_probs, top_idx = text_probs.squeeze().topk(20)\n",
    "\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_idx)):\n",
    "        print(f'{i + 1}. label: {labels[idx]:<20} | prob: {prob.item():.4f}')\n",
    "\n",
    "    return top_act_ids[hook_layer], top_probs, top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff92736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_noise_level(\n",
    "    model: nn.Module,\n",
    "    embedding_layer: str,\n",
    "    std_multiplier: float = 3,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Pick a noise level to corrupt the input text with, such that the\n",
    "    noise is a multiplier of the stdev of the token embeddings.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        embedding_weights = cast(\n",
    "            torch.Tensor, get_module(model, embedding_layer).weight\n",
    "        )\n",
    "        noise_level_std = embedding_weights.std().item()\n",
    "    return std_multiplier * noise_level_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f89aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def _get_consistent_noise(\n",
    "    self, sample_num: int, subj_len: int, layer_size: int\n",
    ") -> np.typing.NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Returns a numpy array of noise for a given sample number and shape, which is consistent for each combination of inputs\n",
    "    \"\"\"\n",
    "    cache_key = (sample_num, subj_len, layer_size)\n",
    "    if cache_key not in self._noise_cache:\n",
    "        prng = np.random.RandomState(self.random_seed)\n",
    "        self._noise_cache[cache_key] = prng.randn(\n",
    "            self.samples_per_patch, subj_len, layer_size\n",
    "        )[sample_num]\n",
    "    return self._noise_cache[cache_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def zero_ablate_feature_hook(\n",
    "    feature_activations, hook, feature_ids, position=None\n",
    "):\n",
    "    if position is None:\n",
    "        feature_activations[:, :, feature_ids] = 0\n",
    "    else:\n",
    "        feature_activations[:, position, feature_ids] = 0\n",
    "\n",
    "    return feature_activations\n",
    "\n",
    "\n",
    "def gaussian_ablate_feature_hook(\n",
    "    feature_activations,\n",
    "    hook,\n",
    "    feature_ids,\n",
    "    position=None,\n",
    "    sigma=3.0,\n",
    "    # n_repeats=10,\n",
    "    mode='add',\n",
    "    match_scale=True,\n",
    "    eps=1e-6,\n",
    "    seed=42,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Add gaussian noise to the feature activations.\n",
    "\n",
    "    Args:\n",
    "        - feature_activations: The feature activations after the activation\n",
    "        function in transcoder.\n",
    "        - hook: The hook to ablate, should be after the activation function in\n",
    "        transcoder.\n",
    "        - feature_ids: The feature ids to ablate. If None, all features are\n",
    "        ablated.\n",
    "        - position: The token position to ablate. If None, all positions are\n",
    "        ablated.\n",
    "        - sigma: The standard deviation of the gaussian noise. In the ROME\n",
    "        paper, they use 3.0, and point out that the noise level should be large\n",
    "        enough to make an effect.\n",
    "        - mode: The mode to add the gaussian noise. Can be 'add' or 'replace'.\n",
    "        - match_scale: Whether to match the scale of the feature activations.\n",
    "            Reccomended to be True since the feature activations can vary.\n",
    "        - eps: The epsilon to avoid division by zero.\n",
    "        - seed: The seed to generate the gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "        The feature activations after the gaussian noise is added.\n",
    "    \"\"\"\n",
    "\n",
    "    if position is None:\n",
    "        target_slice = (slice(None), slice(None), feature_ids)\n",
    "    else:\n",
    "        target_slice = (slice(None), position, feature_ids)\n",
    "\n",
    "    target = feature_activations[target_slice]\n",
    "\n",
    "    # print(f'target shape: {target.shape}')\n",
    "    # print(f'target sum: {target.sum().item()}')\n",
    "    # print(f'target max: {target.max().item()}')\n",
    "    # print(f'target min: {target.min().item()}')\n",
    "    # print(f'feature_activations shape: {feature_activations.shape}')\n",
    "    # print(f'feature_activations sum: {feature_activations.sum().item()}')\n",
    "    # print(f'feature_activations max: {feature_activations.max().item()}')\n",
    "    # print(f'feature_ids range: {min(feature_ids)} to {max(feature_ids)}')\n",
    "\n",
    "    if match_scale:\n",
    "        dims = (0, 1) if target.dim() == 3 else (0,)\n",
    "        # dims = tuple(range(target.dim()))\n",
    "        std = (\n",
    "            target.detach()\n",
    "            # feature_activations.detach()\n",
    "            .float()\n",
    "            .std(dim=dims, keepdim=True, unbiased=False)\n",
    "            .clamp_min(eps)\n",
    "            .to(target.device)\n",
    "        )\n",
    "        noise_std = sigma * std\n",
    "    else:\n",
    "        noise_std = sigma\n",
    "\n",
    "    # print(f'noise_std: {noise_std}')\n",
    "\n",
    "    # shape_full = (n_repeats, *target.shape)\n",
    "\n",
    "    # set seed for reproducibility\n",
    "    current_rng_state = torch.get_rng_state()\n",
    "    torch.manual_seed(seed)\n",
    "    noise = torch.randn_like(target).mul_(noise_std)  # type: ignore\n",
    "    torch.set_rng_state(current_rng_state)\n",
    "\n",
    "    # noise = (\n",
    "    #     torch.randn(shape_full, device=target.device, dtype=target.dtype)\n",
    "    #     .mul_(noise_std)\n",
    "    #     .sum(dim=0)\n",
    "    # )\n",
    "\n",
    "    # see how much element in target are nonzero\n",
    "    print(f'{target.nonzero().numel()} / {target.numel()} elements are nonzero')\n",
    "\n",
    "    if mode == 'add':\n",
    "        target.add_(noise)\n",
    "\n",
    "    elif mode == 'replace':\n",
    "        target.copy_(noise)\n",
    "\n",
    "    feature_activations[target_slice] = target\n",
    "\n",
    "    return feature_activations\n",
    "\n",
    "\n",
    "def _test_with_ablation(\n",
    "    model,\n",
    "    transcoder,\n",
    "    type,\n",
    "    img_tensor,\n",
    "    text_features,\n",
    "    ablation_features,\n",
    "    labels,\n",
    "    **ablation_kwargs,\n",
    "):\n",
    "    hook_layer = transcoder.cfg.hook_point_layer\n",
    "    hook_point = f'blocks.{hook_layer}.mlp.hook_hidden_post'\n",
    "    if type == 'zero':\n",
    "        ablation_hook = partial(\n",
    "            zero_ablate_feature_hook,\n",
    "            feature_ids=ablation_features,\n",
    "            **ablation_kwargs,\n",
    "        )\n",
    "    elif type == 'gaussian':\n",
    "        ablation_hook = partial(\n",
    "            gaussian_ablate_feature_hook,\n",
    "            feature_ids=ablation_features,\n",
    "            **ablation_kwargs,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Invalid ablation type: {type}')\n",
    "\n",
    "    test_img_input(\n",
    "        model,\n",
    "        transcoder,\n",
    "        [(hook_point, ablation_hook)],\n",
    "        img_tensor,\n",
    "        text_features,\n",
    "        labels,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcoder = tc_list[0]\n",
    "\n",
    "transcoder.eval()\n",
    "hookedsaevit.eval()\n",
    "top_act_ids, top_probs, top_idx = get_top_activations(\n",
    "    hookedsaevit,\n",
    "    transcoder,\n",
    "    img_tensor,  # type: ignore\n",
    "    text_features,\n",
    "    final_labels,\n",
    ")\n",
    "\n",
    "top_act_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablation_features = list(range(20000, 40000))\n",
    "ablation_features = top_act_ids\n",
    "_test_with_ablation(\n",
    "    hookedsaevit,\n",
    "    transcoder,\n",
    "    'zero',\n",
    "    img_tensor,\n",
    "    text_features,\n",
    "    ablation_features,\n",
    "    final_labels,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0859d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_with_ablation(\n",
    "    hookedsaevit,\n",
    "    transcoder,\n",
    "    'gaussian',\n",
    "    img_tensor,\n",
    "    text_features,\n",
    "    ablation_features,\n",
    "    labels=final_labels,\n",
    "    sigma=2.0,\n",
    "    match_scale=True,\n",
    "    seed=42,  # Change seed for different noise\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6907d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class AblationExperimentRunner:\n",
    "    \"\"\"\n",
    "    Encapsulates the logic for running a suite of ablation experiments\n",
    "    on a HookedTranscoderViT model, ensuring comparability between different\n",
    "    ablation methods by using a feature injection technique. Will run 4 types of\n",
    "    forward passes:\n",
    "    - Original CLIP baseline\n",
    "    - Transcoder baseline\n",
    "    - Zero Ablation\n",
    "    - Gaussian Noise Ablation (with n_gn_samples samples)\n",
    "\n",
    "    The feature activations from the transcoder baseline are used to inject\n",
    "    zero and Gaussian noise into the model for apple-to-apple comparison.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: 'HookedTranscoderViT',\n",
    "        transcoders: list[SparseAutoencoder],\n",
    "        img_tensor: torch.Tensor,\n",
    "        labels: list[str],\n",
    "        text_features: torch.Tensor,\n",
    "        device='cuda',\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.transcoders = {tc.cfg.hook_point_layer: tc for tc in transcoders}\n",
    "        self.img_tensor = img_tensor.to(device)\n",
    "        self.labels = labels\n",
    "        self.text_features = text_features.to(device)\n",
    "        self.device = device\n",
    "        self.gn_generator = torch.Generator(device=device)\n",
    "        self.results: dict[str, Any] = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def set_seeds(seed: int):\n",
    "        \"\"\"Sets random seeds for reproducibility.\"\"\"\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def _format_results(self, probs, idxs) -> dict[str, float]:\n",
    "        \"\"\"Formats model output into a dictionary.\"\"\"\n",
    "        return {\n",
    "            self.labels[idx.item()]: prob.item()\n",
    "            for prob, idx in zip(probs, idxs)\n",
    "        }\n",
    "\n",
    "    def _run_forward_pass(\n",
    "        self, transcoders_to_use, fwd_hooks\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Generic helper to run the model and get top predictions.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            img_input = self.img_tensor.unsqueeze(0).to(self.device)\n",
    "            vis_out = self.model.run_with_hooks_with_transcoders(\n",
    "                img_input,\n",
    "                transcoders=transcoders_to_use,\n",
    "                fwd_hooks=fwd_hooks,\n",
    "                bwd_hooks=[],\n",
    "                reset_hooks_end=True,\n",
    "            )\n",
    "            image_features = vis_out.to(self.device)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (\n",
    "                100.0 * image_features @ self.text_features.T\n",
    "            ).softmax(dim=-1)\n",
    "            top_probs, top_idx = text_probs.squeeze().topk(20)\n",
    "        return top_probs, top_idx\n",
    "\n",
    "    def run_original_clip_baseline(self, seed: int):\n",
    "        \"\"\"Runs the baseline CLIP model without any transcoders.\"\"\"\n",
    "        print(f'--- [Seed {seed}] Running Original CLIP Baseline ---')\n",
    "        self.set_seeds(seed)\n",
    "        self.model.eval()\n",
    "        # This resets all transcoders, restoring the original MLP layers\n",
    "        self.model.reset_transcoders()\n",
    "        top_probs, top_idx = self._run_forward_pass(\n",
    "            transcoders_to_use=[], fwd_hooks=[]\n",
    "        )\n",
    "        self.results[f'original_clip_seed{seed}'] = self._format_results(\n",
    "            top_probs, top_idx\n",
    "        )\n",
    "        print('Baseline Done.')\n",
    "\n",
    "    def run_ablation_suite(\n",
    "        self,\n",
    "        layer_idx: int,\n",
    "        ablation_feature_ids: list[int],\n",
    "        seed: int,\n",
    "        n_gn_samples: int = 10,\n",
    "        sigma: float = 1.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Runs the full suite of experiments for a single layer and seed.\n",
    "        1. Captures baseline activations with the transcoder.\n",
    "        2. Runs zero and Gaussian noise ablations by injecting modified versions\n",
    "           of the captured activations.\n",
    "\n",
    "        Args:\n",
    "            layer_idx: The layer index to run the experiments on.\n",
    "            n_gn_samples: The number of Gaussian noise samples to run.\n",
    "            sigma: The standard deviation of the Gaussian noise.\n",
    "        \"\"\"\n",
    "        self.set_seeds(seed)\n",
    "        self.model.eval()\n",
    "\n",
    "        if layer_idx not in self.transcoders:\n",
    "            raise ValueError(f'No transcoder found for layer {layer_idx}')\n",
    "        transcoder = self.transcoders[layer_idx]\n",
    "        hook_point = f'blocks.{layer_idx}.mlp.hook_hidden_post'\n",
    "\n",
    "        # --- Step 1: Capture baseline activations & get baseline transcoder results ---\n",
    "        print(\n",
    "            f'\\n--- [Seed {seed}] Running Transcoder Baseline & Capturing Activations for L{layer_idx} ---'\n",
    "        )\n",
    "        captured_activations = None\n",
    "\n",
    "        def capture_hook(activations, hook):\n",
    "            nonlocal captured_activations\n",
    "            captured_activations = activations.clone().detach()\n",
    "\n",
    "        top_probs, top_idx = self._run_forward_pass(\n",
    "            transcoders_to_use=[transcoder],\n",
    "            fwd_hooks=[(hook_point, capture_hook)],\n",
    "        )\n",
    "        if captured_activations is None:\n",
    "            raise RuntimeError('Failed to capture activations.')\n",
    "        self.results[f'transcoder_L{layer_idx}_seed{seed}'] = (\n",
    "            self._format_results(top_probs, top_idx)\n",
    "        )\n",
    "        print('Activations captured.')\n",
    "\n",
    "        # --- Step 2: Run ablation experiments using injection ---\n",
    "\n",
    "        # 2a. Zero Ablation\n",
    "        print(f'--- [Seed {seed}] Running Zero Ablation for L{layer_idx} ---')\n",
    "\n",
    "        def inject_hook_zero_ablation(activations, hook):\n",
    "            modified_activations = captured_activations.clone()\n",
    "            return zero_ablate_feature_hook(\n",
    "                modified_activations, hook, feature_ids=ablation_feature_ids\n",
    "            )\n",
    "\n",
    "        top_probs, top_idx = self._run_forward_pass(\n",
    "            transcoders_to_use=[transcoder],\n",
    "            fwd_hooks=[(hook_point, inject_hook_zero_ablation)],\n",
    "        )\n",
    "        self.results[f'zero_ablation_L{layer_idx}_seed{seed}'] = (\n",
    "            self._format_results(top_probs, top_idx)\n",
    "        )\n",
    "        print('Zero Ablation Done.')\n",
    "\n",
    "        # 2b. Gaussian Noise Ablation\n",
    "        print(\n",
    "            f'--- [Seed {seed}] Running Gaussian Noise Ablation for L{layer_idx} ({n_gn_samples} samples) ---'\n",
    "        )\n",
    "        gn_results_list = []\n",
    "        for i in range(n_gn_samples):\n",
    "            gn_seed = seed * 100 + i\n",
    "\n",
    "            def inject_hook_gn_ablation(activations, hook):\n",
    "                modified_activations = captured_activations.clone()\n",
    "                return gaussian_ablate_feature_hook(\n",
    "                    modified_activations,\n",
    "                    hook,\n",
    "                    feature_ids=ablation_feature_ids,\n",
    "                    sigma=sigma,\n",
    "                    match_scale=True,\n",
    "                    seed=gn_seed,\n",
    "                )\n",
    "\n",
    "            top_probs, top_idx = self._run_forward_pass(\n",
    "                transcoders_to_use=[transcoder],\n",
    "                fwd_hooks=[(hook_point, inject_hook_gn_ablation)],\n",
    "            )\n",
    "            gn_results_list.append(self._format_results(top_probs, top_idx))\n",
    "\n",
    "        self.results[f'gn_ablation_L{layer_idx}_seed{seed}'] = gn_results_list\n",
    "        print('GN Ablation Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = AblationExperimentRunner(\n",
    "    model=hookedsaevit,\n",
    "    transcoders=[transcoder],\n",
    "    img_tensor=img_tensor,\n",
    "    text_features=text_features,\n",
    "    labels=final_labels,\n",
    "    device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d8dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcoder = tc_list[0]\n",
    "\n",
    "top_feature_ids, _, _ = get_top_activations(\n",
    "    hookedsaevit,\n",
    "    transcoder,\n",
    "    img_tensor,\n",
    "    text_features,\n",
    "    final_labels,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4830dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "LAYER_TO_TEST = 0  # Example: test the first transcoder\n",
    "N_GN_SAMPLES = 10\n",
    "SIGMA = 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ceadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.run_original_clip_baseline(seed=SEED)\n",
    "runner.run_ablation_suite(\n",
    "    layer_idx=LAYER_TO_TEST,\n",
    "    ablation_feature_ids=top_feature_ids,\n",
    "    seed=SEED,\n",
    "    n_gn_samples=N_GN_SAMPLES,\n",
    "    sigma=SIGMA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "34ea8c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gn_ablation_L0_seed42': [{'candidness': 0.016439052298665047,\n",
      "                            'disorganize': 0.012279752641916275,\n",
      "                            'electrostatics': 0.021551677957177162,\n",
      "                            'engagingness': 0.010660474188625813,\n",
      "                            'equivalence': 0.010657505132257938,\n",
      "                            'ethicality': 0.04339324310421944,\n",
      "                            'experimentation': 0.125112846493721,\n",
      "                            'funneled': 0.010829261504113674,\n",
      "                            'graphic': 0.13300321996212006,\n",
      "                            'inconstantly': 0.015027370303869247,\n",
      "                            'misappropriation': 0.029279856011271477,\n",
      "                            'much': 0.010729238390922546,\n",
      "                            'reaction': 0.011629119515419006,\n",
      "                            'regenerate': 0.019415581598877907,\n",
      "                            'science': 0.012716084718704224,\n",
      "                            'sound': 0.010766691528260708,\n",
      "                            'technology': 0.10563768446445465,\n",
      "                            'trustless': 0.010851467028260231,\n",
      "                            'twit': 0.012955811806023121,\n",
      "                            'which': 0.015446241945028305},\n",
      "                           {'corrupt': 0.012260936200618744,\n",
      "                            'ethicality': 0.030850812792778015,\n",
      "                            'experimentation': 0.046909622848033905,\n",
      "                            'funneled': 0.02071375586092472,\n",
      "                            'garmentless': 0.02406066283583641,\n",
      "                            'graphic': 0.2120000571012497,\n",
      "                            'homogenous': 0.022557808086276054,\n",
      "                            'immorality': 0.014508824795484543,\n",
      "                            'inconstantly': 0.012448199093341827,\n",
      "                            'misappropriation': 0.025881504639983177,\n",
      "                            'pintsize': 0.012059847824275494,\n",
      "                            'premonition': 0.015187234617769718,\n",
      "                            'regenerate': 0.04404507204890251,\n",
      "                            'sailed': 0.01786942407488823,\n",
      "                            'tearfulness': 0.01775228977203369,\n",
      "                            'technology': 0.01321466825902462,\n",
      "                            'thyself': 0.024427538737654686,\n",
      "                            'trustless': 0.015077096410095692,\n",
      "                            'uptempo': 0.016601042822003365,\n",
      "                            'which': 0.014939550310373306},\n",
      "                           {'antisocial': 0.03170621395111084,\n",
      "                            'autocratic': 0.02538711205124855,\n",
      "                            'blockheaded': 0.03989841789007187,\n",
      "                            'candidness': 0.026506492868065834,\n",
      "                            'corrupt': 0.010225714184343815,\n",
      "                            'disreputability': 0.016730353236198425,\n",
      "                            'engagingness': 0.012483533471822739,\n",
      "                            'ethicality': 0.014495772309601307,\n",
      "                            'experimentation': 0.06661952286958694,\n",
      "                            'garmentless': 0.01932377740740776,\n",
      "                            'graphic': 0.1151319295167923,\n",
      "                            'immorality': 0.01690240018069744,\n",
      "                            'misappropriation': 0.06191834434866905,\n",
      "                            'regenerate': 0.009935669600963593,\n",
      "                            'sailed': 0.011476024985313416,\n",
      "                            'technology': 0.028877699747681618,\n",
      "                            'trustless': 0.024304339662194252,\n",
      "                            'untraded': 0.029025694355368614,\n",
      "                            'uptempo': 0.04273712635040283,\n",
      "                            'which': 0.016701500862836838},\n",
      "                           {'antisocial': 0.015684522688388824,\n",
      "                            'candidness': 0.015832191333174706,\n",
      "                            'engagingness': 0.019648613408207893,\n",
      "                            'ethicality': 0.030314646661281586,\n",
      "                            'experimentation': 0.0727650448679924,\n",
      "                            'expletive': 0.01647612825036049,\n",
      "                            'garmentless': 0.01830425299704075,\n",
      "                            'graphic': 0.11877598613500595,\n",
      "                            'homogenous': 0.017630716785788536,\n",
      "                            'immorality': 0.014556866139173508,\n",
      "                            'inconstantly': 0.033680159598588943,\n",
      "                            'knowledge': 0.012154994532465935,\n",
      "                            'misappropriation': 0.03855974227190018,\n",
      "                            'regenerate': 0.04559899494051933,\n",
      "                            'solution': 0.012663010507822037,\n",
      "                            'technology': 0.026004180312156677,\n",
      "                            'thyself': 0.01976332627236843,\n",
      "                            'trustless': 0.015097647905349731,\n",
      "                            'uptempo': 0.01629582978785038,\n",
      "                            'which': 0.0221701692789793},\n",
      "                           {'acid': 0.026677411049604416,\n",
      "                            'antisocial': 0.01342591643333435,\n",
      "                            'candidness': 0.029136506840586662,\n",
      "                            'engagingness': 0.024638192728161812,\n",
      "                            'ethicality': 0.04737618938088417,\n",
      "                            'experimentation': 0.050313904881477356,\n",
      "                            'expletive': 0.032983697950839996,\n",
      "                            'funneled': 0.015665262937545776,\n",
      "                            'garmentless': 0.015369771979749203,\n",
      "                            'graphic': 0.1174604520201683,\n",
      "                            'homogenous': 0.01757078245282173,\n",
      "                            'immorality': 0.014726907014846802,\n",
      "                            'inconstantly': 0.024210315197706223,\n",
      "                            'misappropriation': 0.03815171867609024,\n",
      "                            'reaction': 0.01325101125985384,\n",
      "                            'regenerate': 0.031418923288583755,\n",
      "                            'satisfied': 0.014131506904959679,\n",
      "                            'secret': 0.014628389850258827,\n",
      "                            'technology': 0.014717301353812218,\n",
      "                            'uptempo': 0.018435917794704437},\n",
      "                           {'analysis': 0.026479359716176987,\n",
      "                            'antisocial': 0.018445877358317375,\n",
      "                            'candidness': 0.01992497779428959,\n",
      "                            'chemistry': 0.012999203987419605,\n",
      "                            'electrostatics': 0.048210565000772476,\n",
      "                            'engagingness': 0.014915520325303078,\n",
      "                            'ethicality': 0.024598151445388794,\n",
      "                            'experimentation': 0.0902809128165245,\n",
      "                            'funk': 0.015452520921826363,\n",
      "                            'garmentless': 0.010825545527040958,\n",
      "                            'graphic': 0.14882677793502808,\n",
      "                            'hardware': 0.016663381829857826,\n",
      "                            'inconstantly': 0.011829591356217861,\n",
      "                            'misappropriation': 0.038291122764348984,\n",
      "                            'pintsize': 0.011067288927733898,\n",
      "                            'research': 0.015077932737767696,\n",
      "                            'research tools': 0.022997330874204636,\n",
      "                            'science': 0.018234942108392715,\n",
      "                            'technology': 0.023819666355848312,\n",
      "                            'which': 0.01833188720047474},\n",
      "                           {'analysis': 0.01156503427773714,\n",
      "                            'antisocial': 0.01836729608476162,\n",
      "                            'candidness': 0.03930681198835373,\n",
      "                            'compound': 0.010128865018486977,\n",
      "                            'corrupt': 0.009808791801333427,\n",
      "                            'doc': 0.018836965784430504,\n",
      "                            'edged': 0.012977546080946922,\n",
      "                            'engagingness': 0.04475505277514458,\n",
      "                            'ethicality': 0.10336349904537201,\n",
      "                            'experimentation': 0.07399521768093109,\n",
      "                            'expletive': 0.05702420324087143,\n",
      "                            'funneled': 0.019008837640285492,\n",
      "                            'graphic': 0.09203796833753586,\n",
      "                            'knowledge': 0.01529860869050026,\n",
      "                            'misappropriation': 0.013908594846725464,\n",
      "                            'reaction': 0.016069112345576286,\n",
      "                            'research': 0.01063764002174139,\n",
      "                            'science': 0.011504466645419598,\n",
      "                            'secret': 0.012342584319412708,\n",
      "                            'technology': 0.06481821835041046},\n",
      "                           {'acid': 0.010462121106684208,\n",
      "                            'blockheaded': 0.01091015711426735,\n",
      "                            'candidness': 0.010536015033721924,\n",
      "                            'corrupt': 0.013425100594758987,\n",
      "                            'electrostatics': 0.011988522484898567,\n",
      "                            'ethicality': 0.010163974948227406,\n",
      "                            'experiment': 0.010169870220124722,\n",
      "                            'experimentation': 0.20254087448120117,\n",
      "                            'expletive': 0.012304064817726612,\n",
      "                            'garmentless': 0.010453923605382442,\n",
      "                            'graphic': 0.13740038871765137,\n",
      "                            'immorality': 0.01105673797428608,\n",
      "                            'inconstantly': 0.01645263470709324,\n",
      "                            'misappropriation': 0.03981843590736389,\n",
      "                            'reaction': 0.020154498517513275,\n",
      "                            'regenerate': 0.045083899050951004,\n",
      "                            'secret': 0.025967294350266457,\n",
      "                            'technology': 0.018073201179504395,\n",
      "                            'uptempo': 0.029360106214880943,\n",
      "                            'which': 0.015103201381862164},\n",
      "                           {'compound': 0.013139219954609871,\n",
      "                            'electrostatics': 0.023763619363307953,\n",
      "                            'ethicality': 0.02218027226626873,\n",
      "                            'experimentation': 0.13757269084453583,\n",
      "                            'funk': 0.017529215663671494,\n",
      "                            'funneled': 0.01991908624768257,\n",
      "                            'garmentless': 0.010479097254574299,\n",
      "                            'graphic': 0.13981087505817413,\n",
      "                            'hardware': 0.014575310982763767,\n",
      "                            'homogenous': 0.01831996813416481,\n",
      "                            'inconstantly': 0.020383326336741447,\n",
      "                            'misappropriation': 0.020044246688485146,\n",
      "                            'much': 0.012905081734061241,\n",
      "                            'pintsize': 0.010111111216247082,\n",
      "                            'regenerate': 0.025597499683499336,\n",
      "                            'technology': 0.06692773103713989,\n",
      "                            'trustless': 0.010752962902188301,\n",
      "                            'twit': 0.011505327187478542,\n",
      "                            'uptempo': 0.03916732966899872,\n",
      "                            'which': 0.03444881737232208},\n",
      "                           {'antisocial': 0.01684378832578659,\n",
      "                            'blockheaded': 0.026389295235276222,\n",
      "                            'candidness': 0.02827715128660202,\n",
      "                            'depersonalization': 0.018596498295664787,\n",
      "                            'disorganize': 0.03228187933564186,\n",
      "                            'engagingness': 0.02756747603416443,\n",
      "                            'ethicality': 0.02369603142142296,\n",
      "                            'experimentation': 0.041021790355443954,\n",
      "                            'funk': 0.019748954102396965,\n",
      "                            'graphic': 0.0887526124715805,\n",
      "                            'inclusiveness': 0.021335067227482796,\n",
      "                            'inconstantly': 0.02514585107564926,\n",
      "                            'knowledge': 0.02074878290295601,\n",
      "                            'misappropriation': 0.049288060516119,\n",
      "                            'pintsize': 0.029034649953246117,\n",
      "                            'regenerate': 0.01636889949440956,\n",
      "                            'solution': 0.013667129911482334,\n",
      "                            'technology': 0.015056370757520199,\n",
      "                            'trustless': 0.0337861105799675,\n",
      "                            'uptempo': 0.02001902088522911}],\n",
      " 'original_clip_seed42': {'beaker': 0.014817215502262115,\n",
      "                          'chemical': 0.026879629120230675,\n",
      "                          'chemistry': 0.2883979380130768,\n",
      "                          'experiment': 0.005474903620779514,\n",
      "                          'experimentation': 0.0016794033581390977,\n",
      "                          'flask': 0.011675550602376461,\n",
      "                          'graduated tube': 0.006809170823544264,\n",
      "                          'lab bench': 0.0013898651814088225,\n",
      "                          'laboratory': 0.011186669580638409,\n",
      "                          'laboratory bottle': 0.050220634788274765,\n",
      "                          'liquid': 0.006443819031119347,\n",
      "                          'reagent': 0.033832304179668427,\n",
      "                          'research': 0.002508324570953846,\n",
      "                          'research tools': 0.0566556341946125,\n",
      "                          'science': 0.11973288655281067,\n",
      "                          'science equipment': 0.197359099984169,\n",
      "                          'solution': 0.002339880680665374,\n",
      "                          'solvent': 0.0017457738285884261,\n",
      "                          'test tube': 0.1367606520652771,\n",
      "                          'titration': 0.009259015321731567},\n",
      " 'transcoder_L0_seed42': {'beaker': 0.007937334477901459,\n",
      "                          'chemical': 0.1004834920167923,\n",
      "                          'chemistry': 0.19754555821418762,\n",
      "                          'ethicality': 0.009960092604160309,\n",
      "                          'experiment': 0.025279030203819275,\n",
      "                          'experimentation': 0.0162697434425354,\n",
      "                          'flask': 0.010753048583865166,\n",
      "                          'laboratory bottle': 0.11423622071743011,\n",
      "                          'liquid': 0.028813501819968224,\n",
      "                          'reaction': 0.03261431306600571,\n",
      "                          'reagent': 0.11333338916301727,\n",
      "                          'research': 0.013581258244812489,\n",
      "                          'research tools': 0.0841355174779892,\n",
      "                          'science': 0.08708769828081131,\n",
      "                          'science equipment': 0.021823665127158165,\n",
      "                          'solution': 0.01234480831772089,\n",
      "                          'solvent': 0.023206990212202072,\n",
      "                          'test tube': 0.025303341448307037,\n",
      "                          'titration': 0.027000540867447853,\n",
      "                          'vial': 0.0059471712447702885},\n",
      " 'zero_ablation_L0_seed42': {'antisocial': 0.014310234226286411,\n",
      "                             'autocratic': 0.018137127161026,\n",
      "                             'chemistry': 0.026498058810830116,\n",
      "                             'compound': 0.015866050496697426,\n",
      "                             'disreputability': 0.024706915020942688,\n",
      "                             'equivalence': 0.013359330594539642,\n",
      "                             'ethicality': 0.03280710056424141,\n",
      "                             'experimentation': 0.025254352018237114,\n",
      "                             'graphic': 0.08900509774684906,\n",
      "                             'immorality': 0.027452323585748672,\n",
      "                             'inclusiveness': 0.018770448863506317,\n",
      "                             'misappropriation': 0.026786332949995995,\n",
      "                             'reaction': 0.018846001476049423,\n",
      "                             'research': 0.019373899325728416,\n",
      "                             'research tools': 0.015347516164183617,\n",
      "                             'science': 0.016537461429834366,\n",
      "                             'solution': 0.02702862210571766,\n",
      "                             'unpremeditatedly': 0.014771221205592155,\n",
      "                             'uptempo': 0.04342057928442955,\n",
      "                             'which': 0.021947935223579407}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(runner.results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prisma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
